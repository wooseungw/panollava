#!/usr/bin/env python3
"""
ìˆ˜ì •ëœ Generate ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
"""
import torch
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from panovlm.model import PanoramaVLM

def test_fixed_generate():
    """ìˆ˜ì •ëœ generate ë©”ì„œë“œ í…ŒìŠ¤íŠ¸"""
    print("=== ìˆ˜ì •ëœ Generate ë©”ì„œë“œ í…ŒìŠ¤íŠ¸ ===")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = PanoramaVLM(
        vision_model_name="google/siglip-base-patch16-224",
        language_model_name="Qwen/Qwen2.5-0.5B",
        resampler_type="mlp"
    )
    
    print("ğŸ”§ ì£¼ìš” ê°œì„ ì‚¬í•­:")
    improvements = [
        "âœ… ê°•ì œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ë¡œ ë¹ˆ ë¬¸ìì—´ ë°©ì§€",
        "âœ… ë°°ì¹˜ ì°¨ì› ì¼ê´€ì„± í™•ë³´",
        "âœ… ì•ˆì „í•œ ì˜ˆì™¸ ì²˜ë¦¬ ë° fallback",
        "âœ… ìµœì†Œ ê¸¸ì´ ë³´ì¥ (min_length)",
        "âœ… ì¡°ê¸° ì¢…ë£Œ ë°©ì§€ (early_stopping=False)",
        "âœ… ê°œì„ ëœ ìƒì„± íŒŒë¼ë¯¸í„°"
    ]
    
    for improvement in improvements:
        print(f"  {improvement}")
    
    # ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸
    batch_sizes = [1, 2, 4]
    
    for batch_size in batch_sizes:
        print(f"\n--- ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ---")
        
        # ë”ë¯¸ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€
        dummy_pixel_values = torch.randn(batch_size, 6, 3, 224, 224)
        
        model.eval()
        with torch.no_grad():
            try:
                # 1. ê¸°ë³¸ ìº¡ì…”ë‹ í…ŒìŠ¤íŠ¸
                print("1. ê¸°ë³¸ ìº¡ì…”ë‹:")
                result1 = model.generate(
                    pixel_values=dummy_pixel_values,
                    max_new_tokens=20,
                    temperature=0.7
                )
                
                for i, text in enumerate(result1['text']):
                    print(f"   ìƒ˜í”Œ {i}: '{text}' (ê¸¸ì´: {len(text)})")
                
                # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                empty_count = sum(1 for text in result1['text'] if len(text) == 0)
                if empty_count == 0:
                    print("   âœ… ë¹ˆ ë¬¸ìì—´ ì—†ìŒ")
                else:
                    print(f"   âš ï¸  ë¹ˆ ë¬¸ìì—´ {empty_count}ê°œ ë°œê²¬")
                
                # 2. ì§ˆë¬¸-ë‹µë³€ í…ŒìŠ¤íŠ¸
                print("\n2. ì§ˆë¬¸-ë‹µë³€:")
                question = "What can you see?"
                question_tokens = model.tokenizer(question, return_tensors="pt")
                
                # ë°°ì¹˜ í¬ê¸°ì— ë§ê²Œ í™•ì¥
                if question_tokens["input_ids"].size(0) != batch_size:
                    question_tokens["input_ids"] = question_tokens["input_ids"].repeat(batch_size, 1)
                
                result2 = model.generate(
                    pixel_values=dummy_pixel_values,
                    input_ids=question_tokens["input_ids"],
                    max_new_tokens=25,
                    temperature=0.8
                )
                
                for i, text in enumerate(result2['text']):
                    print(f"   ìƒ˜í”Œ {i}: '{text}' (ê¸¸ì´: {len(text)})")
                
                print(f"   âœ… ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
                
            except Exception as e:
                print(f"   âŒ ë°°ì¹˜ í¬ê¸° {batch_size} ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()

def test_edge_cases():
    """ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print("\n=== ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ===")
    
    model = PanoramaVLM(
        vision_model_name="google/siglip-base-patch16-224",
        language_model_name="Qwen/Qwen2.5-0.5B",
        resampler_type="mlp"
    )
    
    model.eval()
    
    # 1. 4D ì´ë¯¸ì§€ ì…ë ¥ í…ŒìŠ¤íŠ¸
    print("1. 4D ì´ë¯¸ì§€ ì…ë ¥ í…ŒìŠ¤íŠ¸:")
    dummy_4d = torch.randn(2, 3, 224, 224)  # ë°°ì¹˜, ì±„ë„, ë†’ì´, ë„ˆë¹„
    
    with torch.no_grad():
        try:
            result = model.generate(
                pixel_values=dummy_4d,
                max_new_tokens=15,
                temperature=0.6
            )
            print(f"   âœ… 4D ì…ë ¥ ì„±ê³µ: {result['text'][0]}")
        except Exception as e:
            print(f"   âŒ 4D ì…ë ¥ ì‹¤íŒ¨: {e}")
    
    # 2. ë§¤ìš° ì‘ì€ max_new_tokens í…ŒìŠ¤íŠ¸
    print("\n2. ë§¤ìš° ì‘ì€ max_new_tokens í…ŒìŠ¤íŠ¸:")
    dummy_5d = torch.randn(1, 6, 3, 224, 224)
    
    with torch.no_grad():
        try:
            result = model.generate(
                pixel_values=dummy_5d,
                max_new_tokens=3,
                temperature=0.5
            )
            print(f"   âœ… ì‘ì€ í† í° ìˆ˜ ì„±ê³µ: '{result['text'][0]}'")
        except Exception as e:
            print(f"   âŒ ì‘ì€ í† í° ìˆ˜ ì‹¤íŒ¨: {e}")
    
    # 3. ë§¤ìš° ë‚®ì€/ë†’ì€ ì˜¨ë„ í…ŒìŠ¤íŠ¸
    print("\n3. ê·¹ë‹¨ì  ì˜¨ë„ í…ŒìŠ¤íŠ¸:")
    temperatures = [0.1, 1.5, 2.0]
    
    for temp in temperatures:
        with torch.no_grad():
            try:
                result = model.generate(
                    pixel_values=dummy_5d,
                    max_new_tokens=10,
                    temperature=temp
                )
                print(f"   âœ… ì˜¨ë„ {temp}: '{result['text'][0]}'")
            except Exception as e:
                print(f"   âŒ ì˜¨ë„ {temp} ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    test_fixed_generate()
    test_edge_cases()