# coding: utf-8
"""
PanoLLaVA Comprehensive Model Evaluation System
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ë‹¨ê³„ë³„ í‰ê°€ ì‹œìŠ¤í…œ:
1. ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ (ChatPanoTestDataset, VLMDataModule)
3. ë°°ì¹˜ë³„ í…ìŠ¤íŠ¸ ìƒì„± (generate)
4. ì˜ˆì¸¡/ì •ë‹µ í…ìŠ¤íŠ¸ ì €ì¥ ë° ë¡œê¹…
5. í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (BLEU, ROUGE, METEOR, SPICE, CIDEr, CLIP-S, RefCLIP-S)

ì‚¬ìš©ë²•:
    python eval.py --model-dir runs/<run_name>/hf_model --lora-weights-path runs/<run_name>/lora_weights --csv-input data/quic360/test.csv
"""

import argparse
import torch
import json
import logging
import time
import traceback
import os
from pathlib import Path
from tqdm import tqdm
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple

# ë‚´ë¶€ ëª¨ë“ˆ
# Silence HF tokenizers fork/parallelism warnings and avoid deadlocks
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

from panovlm.dataset import VLMDataModule
from panovlm.processors.universal_text_formatter import UniversalTextFormatter

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def resolve_model_dir(config_or_path, stage: str = None, crop_strategy: str = None) -> str:
    """
    HF-style ëª¨ë¸ ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰ (PyTorch bin ê¸°ë°˜)
    - config_or_path: dict ë˜ëŠ” JSON íŒŒì¼ ê²½ë¡œ(str)
    - stage/crop_strategy: runs/<prefix>_<crop>_<stage>_<resampler>/hf_model íŒíŠ¸ êµ¬ì„±ì— ì‚¬ìš©
    """
    try:
        # config ë¡œë”© (dict ë˜ëŠ” íŒŒì¼ ê²½ë¡œ)
        if isinstance(config_or_path, (str, Path)):
            with open(config_or_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
        elif isinstance(config_or_path, dict):
            config = config_or_path
        else:
            raise TypeError(f"Unsupported config type: {type(config_or_path)}")

        prefix = config.get('training', {}).get('prefix')
        if not prefix:
            raise KeyError("training.prefix is required in config.json")

        resampler = config.get('models', {}).get('resampler_type') or config.get('models', {}).get('resampler', 'mlp')
        if stage is None:
            stage = config.get('training', {}).get('default_stage', 'finetune')

        if crop_strategy is None:
            crop_strategy = config.get('image_processing', {}).get('crop_strategy', 'e2p')

        # ì¶”ê°€: pretrained_dir ì§€ì› ë° HF ë””ë ‰í† ë¦¬/ì²´í¬í¬ì¸íŠ¸ ìë™ íƒìƒ‰
        paths_cfg = config.get('paths', {}) if isinstance(config, dict) else {}
        pretrained_dir = paths_cfg.get('pretrained_dir')
        if pretrained_dir and Path(pretrained_dir).exists():
            p = Path(pretrained_dir)
            if p.is_file() and p.suffix == '.ckpt':
                logger.info(f"âœ… Using checkpoint from config: {pretrained_dir}")
            else:
                logger.info(f"âœ… Using pretrained_dir from config: {pretrained_dir}")
            return str(p)

        # runs ë””ë ‰í† ë¦¬ ë‚´ hf_model í´ë” ìë™ íƒìƒ‰
        candidate_run_dir = Path(paths_cfg.get('runs_dir', 'runs')) / f"{prefix}_{crop_strategy}_{stage}_{resampler}"
        # 1) HF í´ë” ìš°ì„ 
        hf_dir = candidate_run_dir / 'hf_model'
        if hf_dir.exists() and hf_dir.is_dir():
            logger.info(f"âœ… Using HF model dir: {str(hf_dir)}")
            return str(hf_dir)

        # 2) panorama_model í´ë”ë„ í—ˆìš© (ë™ì¼í•œ ê·œì•½ìœ¼ë¡œ ì €ì¥ëœ ê²½ìš°)
        pano_dir = candidate_run_dir / 'panorama_model'
        if pano_dir.exists() and pano_dir.is_dir():
            logger.info(f"âœ… Using panorama_model dir: {str(pano_dir)}")
            return str(pano_dir)

        # 3) ì²´í¬í¬ì¸íŠ¸(.ckpt) ìë™ ì„ íƒ (best > last > any)
        best_ckpt = candidate_run_dir / 'best.ckpt'
        last_ckpt = candidate_run_dir / 'last.ckpt'
        if best_ckpt.exists():
            logger.info(f"âœ… Using best checkpoint: {str(best_ckpt)}")
            return str(best_ckpt)
        if last_ckpt.exists():
            logger.info(f"âœ… Using last checkpoint: {str(last_ckpt)}")
            return str(last_ckpt)
        # any *.ckpt as fallback
        try:
            any_ckpts = sorted(candidate_run_dir.glob('*.ckpt'))
            if any_ckpts:
                logger.info(f"âœ… Using checkpoint: {str(any_ckpts[0])}")
                return str(any_ckpts[0])
        except Exception:
            pass

        raise FileNotFoundError("No pretrained model dir found. Set paths.pretrained_dir or pass --model-dir")

    except Exception as e:
        logger.error(f"Failed to resolve model dir: {e}")
        raise



def load_model_and_lora(
    model_dir: str,
    lora_weights_path: Optional[str],
    device: torch.device,
    config_path: Optional[str] = None,
    **model_kwargs
):
    """
    1ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ì™€ LoRA ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ì—¬ ìƒì„±ìš© ëª¨ë¸ ì¤€ë¹„ (ì„¤ì • ì‹œìŠ¤í…œ í†µí•©)
    - ìƒˆë¡œìš´ PanoramaVLM ì¸í„°í˜ì´ìŠ¤ ìš°ì„  ì‹œë„
    - ì‹¤íŒ¨ ì‹œ VLMModule í´ë°± (ì´ë•Œ model_configë¥¼ ë°˜ë“œì‹œ ì „ë‹¬)
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ 1ë‹¨ê³„: ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ (ì„¤ì • ì‹œìŠ¤í…œ í†µí•©)")
    logger.info("=" * 60)

    # ë””ë°”ì´ìŠ¤ ë¬¸ìì—´
    device_str = str(device) if device != "auto" else "auto"

    # config ê°ì²´ ì¤€ë¹„ (ModelConfig ë˜ëŠ” dict)
    config_obj = None
    if config_path:
        try:
            from panovlm.config import ModelConfig
            try:
                config_obj = ModelConfig.load(config_path)
                logger.info(f"ğŸ“‹ ModelConfig ë¡œë“œ ì™„ë£Œ(from {config_path})")
            except Exception as e:
                logger.warning(f"ModelConfig.load ì‹¤íŒ¨, JSON dictë¡œ ëŒ€ì²´: {e}")
                with open(config_path, "r", encoding="utf-8") as f:
                    config_obj = json.load(f)
        except Exception as e:
            logger.warning(f"panovlm.config.ModelConfig ì‚¬ìš© ë¶ˆê°€, JSON dictë¡œ ëŒ€ì²´: {e}")
            with open(config_path, "r", encoding="utf-8") as f:
                config_obj = json.load(f)

    # â”€â”€ PanoramaVLM (HF ë””ë ‰í† ë¦¬ ë˜ëŠ” .ckpt) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        from panovlm.model import PanoramaVLM

        # from_checkpoint/from_pretrained_dirì— config/model_config ì–´ëŠ ìª½ ì´ë¦„ì„ ì“°ëŠ”ì§€ ëª¨ë“ˆë³„ë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆì–´
        # ëª¨ë‘ ì•ˆì „í•˜ê²Œ ì „ë‹¬(ë°›ëŠ” ìª½ì—ì„œ ë¬´ì‹œí•´ë„ ë¬´í•´)
        extra_cfg = {}
        if config_obj is not None:
            extra_cfg["config"] = config_obj
            extra_cfg["model_config"] = config_obj

        mpath = Path(model_dir)
        if mpath.is_file() and mpath.suffix == ".ckpt":
            logger.info(f"ğŸ“¦ Loading from checkpoint: {str(mpath)}")
            model = PanoramaVLM.from_checkpoint(
                str(mpath),
                device=device_str,
                **extra_cfg,
                **{k: v for k, v in model_kwargs.items() if v is not None}
            )
        else:
            model = PanoramaVLM.from_pretrained_dir(
                str(mpath),
                device=device_str,
                **extra_cfg,
                **{k: v for k, v in model_kwargs.items() if v is not None}
            )

        # ì„¤ì • ì •ë³´ ë¡œê·¸
        if hasattr(model, "config") and model.config:
            logger.info("ğŸ“‹ Model Configuration ìš”ì•½:")
            for k in [
                "vision_name", "language_model_name", "latent_dimension",
                "image_size", "crop_strategy", "use_lora", "lora_r", "lora_alpha"
            ]:
                try:
                    val = getattr(model.config, k, None)
                except Exception:
                    val = None
                if val is not None:
                    logger.info(f"   - {k}: {val}")

        # ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„ ìœ„í•œ ë˜í¼
        class ModelWrapper:
            def __init__(self, panorama_model):
                self.model = panorama_model
                self._stage_key = "finetune"
            def eval(self):
                self.model.eval(); return self
            def to(self, dev):
                self.model = self.model.to(dev); return self

        wrapped_model = ModelWrapper(model).eval()
        logger.info(f"âœ“ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ - Device: {device}")
        return wrapped_model

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise

def prepare_test_dataset(
    csv_input: str,
    batch_size: int,
    max_text_length: str | int,
    crop_strategy: str,
    lm_name: str,
    num_workers: int = 0,
    overlap_ratio: float = 0.5,
    *,
    vision_name: Optional[str] = None,
    system_msg: Optional[str] = None,
    use_vision_processor: bool = True,
    auto_max_text_length_cap: Optional[int] = None,
    auto_max_text_length_floor: Optional[int] = None,
    auto_max_text_length_scan_limit: Optional[int] = None
) -> Tuple[VLMDataModule, Any]:
    """
    2ë‹¨ê³„: ChatPanoTestDatasetê³¼ VLMDataModuleì„ í™œìš©í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    - config.jsonì˜ image_processing/ training ë‚´ìš©ì„ ì¸ìí™”í•˜ì—¬ ë°˜ì˜
    """
    logger.info("=" * 60)
    logger.info("ğŸ“Š 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„")
    logger.info("=" * 60)

    logger.info(f"ğŸ“‚ CSV ì…ë ¥: {csv_input}")
    system_msg = system_msg or "You are an expert assistant specialized in analyzing panoramic images. Please provide detailed, accurate, and helpful responses about what you observe in the panoramic view shortly."

    datamodule = VLMDataModule(
        csv_train=csv_input,
        csv_val=csv_input,  # í‰ê°€ìš©ìœ¼ë¡œ ë™ì¼í•œ íŒŒì¼ ì‚¬ìš©
        batch_size=batch_size,
        num_workers=num_workers,
        tokenizer_name=lm_name,
        max_text_length=max_text_length,
        crop_strategy=crop_strategy,
        eval_mode=True,
        system_msg=system_msg,
        overlap_ratio=overlap_ratio,
        vision_model_name=vision_name,
        use_vision_processor=use_vision_processor,
        auto_max_text_length_cap=int(auto_max_text_length_cap) if auto_max_text_length_cap is not None else 8192,
        auto_max_text_length_floor=int(auto_max_text_length_floor) if auto_max_text_length_floor is not None else None,
        auto_max_text_length_scan_limit=int(auto_max_text_length_scan_limit) if auto_max_text_length_scan_limit is not None else None
    )

    datamodule.setup()
    test_dataloader = datamodule.val_dataloader()

    logger.info(f"âœ“ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
    logger.info(f"   - ì´ ë°°ì¹˜ ìˆ˜: {len(test_dataloader)}")
    logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    logger.info(f"   - í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´ (requested): {max_text_length}")
    logger.info(f"   - í¬ë¡­ ì „ëµ: {crop_strategy}")
    logger.info(f"   - ê²¹ì¹¨ ë¹„ìœ¨: {overlap_ratio}")
    logger.info(f"   - ì›Œì»¤ ìˆ˜: {num_workers}")
    logger.info(f"   - Vision ëª¨ë¸: {vision_name}")
    logger.info(f"   - use_vision_processor: {use_vision_processor}")

    return datamodule, test_dataloader

def generate_predictions(
    model: Any,
    test_dataloader,
    datamodule: VLMDataModule,
    device: torch.device,
    *,
    max_new_tokens: int = 32,
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: int = 50,
    repetition_penalty: float = 1.1,
    length_penalty: float = 1.0,
    min_new_tokens: int = 5,
    system_msg: Optional[str] = None
) -> Tuple[List[str], List[str], List[str], List[str]]:
    """
    3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë°°ì¹˜ë³„ í…ìŠ¤íŠ¸ ìƒì„±
    - config.training.system_msg(ë˜ëŠ” system_messages.default) ë¥¼ UniversalTextFormatterì— ë°˜ì˜
    """
    logger.info("=" * 60)
    logger.info("ğŸ¤– 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ìƒì„± (UniversalTextFormatter í™œìš©)")
    logger.info("=" * 60)

    predictions, references, image_paths, input_texts = [], [], [], []

    tokenizer = datamodule.tokenizer
    tokenizer_name = getattr(tokenizer, 'name_or_path', 'Qwen/Qwen2.5-0.5B-Instruct')
    sys_msg = system_msg or "You are an expert assistant specialized in analyzing panoramic images. Please provide detailed, accurate, and helpful responses about what you observe in the panoramic view shortly."
    text_formatter = UniversalTextFormatter(
        tokenizer_name_or_path=tokenizer_name,
        system_msg=sys_msg
    )

    logger.info(f"ğŸ¯ ìƒì„± íŒŒë¼ë¯¸í„° - Max tokens: {max_new_tokens}, Min tokens: {min_new_tokens}, Temperature: {temperature}")
    logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ í¬ë§·í„° - ëª¨ë¸: {text_formatter.model_family} ({'Instruct' if text_formatter.is_instruct else 'Base'})")

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(test_dataloader, desc="ìƒì„± ì¤‘")):
            try:
                pixel_values = batch["pixel_values"].to(device)
                input_ids = batch.get("input_ids")
                if input_ids is not None:
                    input_ids = input_ids.to(device)
                attention_mask = batch.get("attention_mask")
                if attention_mask is not None:
                    attention_mask = attention_mask.to(device)

                batch_size = pixel_values.shape[0]

                # ê°„ì†Œí™”ëœ ì •ë‹µÂ·ë©”íƒ€ ì¶”ì¶œ
                batch_references = []
                if "reference" in batch:
                    refs = batch["reference"]
                    batch_references = [str(r).strip() for r in (refs if isinstance(refs, list) else [refs]*batch_size)]
                else:
                    batch_references = [f"no_reference_{i}" for i in range(batch_size)]

                batch_image_paths = batch.get("image_path", [f"batch_{batch_idx}_sample_{i}" for i in range(batch_size)])
                batch_input_texts = batch.get("original_query", batch.get("input_text", [f"no_query_{i}" for i in range(batch_size)]))
                if not isinstance(batch_input_texts, list):
                    batch_input_texts = [batch_input_texts] * batch_size

                generation_config = text_formatter.get_generation_config()
                gen_kwargs = {
                    "pixel_values": pixel_values,
                    "input_ids": input_ids,
                    "attention_mask": attention_mask,
                    "max_new_tokens": max_new_tokens,
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": top_k,
                    "repetition_penalty": repetition_penalty,
                    "length_penalty": length_penalty,
                    "min_new_tokens": min_new_tokens,
                    "do_sample": True,
                    "pad_token_id": tokenizer.pad_token_id,
                    "eos_token_id": tokenizer.eos_token_id,
                }
                if hasattr(model, 'model') and hasattr(model.model, 'generation_config'):
                    if hasattr(model.model.generation_config, 'stop_strings'):
                        gen_kwargs["stop_strings"] = generation_config["stop_strings"][:3]

                if hasattr(model, 'model') and hasattr(model.model, 'generate'):
                    output = model.model.generate(**gen_kwargs)
                elif hasattr(model, 'generate'):
                    output = model.generate(**gen_kwargs)
                else:
                    raise AttributeError("ëª¨ë¸ì— generate ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")

                batch_predictions = []
                if isinstance(output, torch.Tensor):
                    for i in range(batch_size):
                        input_length = input_ids[i].shape[0] if input_ids is not None else 0
                        generated_tokens = output[i][input_length:]
                        raw_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                        clean_prediction = text_formatter.extract_assistant_response(raw_text)
                        batch_predictions.append(clean_prediction)
                elif isinstance(output, dict) and "text" in output:
                    for raw_text in output["text"]:
                        clean_prediction = text_formatter.extract_assistant_response(raw_text)
                        batch_predictions.append(clean_prediction)
                else:
                    logger.warning(f"Unexpected output format: {type(output)}")
                    batch_predictions = ["[ìƒì„± ì‹¤íŒ¨]"] * batch_size

                # í¬ê¸° ì •í•©
                if len(batch_predictions) != batch_size:
                    if len(batch_predictions) < batch_size:
                        batch_predictions.extend(["[í¬ê¸° ë¶€ì¡±]"] * (batch_size - len(batch_predictions)))
                    else:
                        batch_predictions = batch_predictions[:batch_size]

                # ì •ë¦¬
                cleaned_predictions = []
                for pred in batch_predictions:
                    cleaned_predictions.append(pred.strip().replace('\n\n', '\n') if pred and pred.strip() else "[ë¹ˆ ì‘ë‹µ]")

                # ë¡œê·¸ & ì¶•ì 
                logger.info(f"=== ë°°ì¹˜ {batch_idx} ê²°ê³¼ ë¡œê·¸ ===")
                for i, (pred, ref) in enumerate(zip(cleaned_predictions, batch_references)):
                    logger.info(f"  ìƒ˜í”Œ {len(predictions) + i}")
                    logger.info(f"    ì˜ˆì¸¡: '{pred}'")
                    logger.info(f"    ì •ë‹µ: '{ref}'")
                logger.info(f"==========================")

                predictions.extend(cleaned_predictions)
                references.extend(batch_references)
                image_paths.extend(batch_image_paths)
                input_texts.extend(batch_input_texts)

                if batch_idx % 10 == 0:
                    logger.info(f"ì§„í–‰: {batch_idx + 1}/{len(test_dataloader)} ë°°ì¹˜ ì™„ë£Œ ({len(predictions)} ìƒ˜í”Œ)")

            except Exception as e:
                logger.error(f"ë°°ì¹˜ {batch_idx} ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                bs = pixel_values.shape[0] if 'pixel_values' in locals() else 1
                predictions.extend([f"[ë°°ì¹˜ ì˜¤ë¥˜_{i}]" for i in range(bs)])
                references.extend(batch_references if 'batch_references' in locals() else [f"[ì •ë‹µ ì—†ìŒ_{i}]" for i in range(bs)])
                image_paths.extend(batch_image_paths if 'batch_image_paths' in locals() else [f"error_batch_{batch_idx}_sample_{i}" for i in range(bs)])
                input_texts.extend(batch_input_texts if 'batch_input_texts' in locals() else [f"error_input_{i}" for i in range(bs)])
                continue

    logger.info(f"âœ“ í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ! ì´ ìƒ˜í”Œ ìˆ˜: {len(predictions)}")
    return predictions, references, image_paths, input_texts



def save_and_log_results(predictions: List[str], references: List[str], image_paths: List[str], input_texts: List[str], output_dir: Path, timestamp: str) -> pd.DataFrame:
    """
    4ë‹¨ê³„: ìƒì„±ëœ ë‹µë³€ê³¼ ì •ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ê³  ë¡œê¹… (ê°œì„ ëœ ë¶„ì„ í¬í•¨)
    """
    logger.info("=" * 60)
    logger.info("ğŸ’¾ 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ë¶„ì„")
    logger.info("=" * 60)
    
    # ê°œì„ ëœ CSV ë°ì´í„° ì¤€ë¹„
    results_data = []
    for i, (pred, ref, img_path) in enumerate(zip(predictions, references, image_paths)):
        # ë¹ˆ ê°’ ì²˜ë¦¬ ë° ê¸°ë³¸ ì •ë¦¬
        pred_str = str(pred).strip() if pred is not None else ""
        ref_str = str(ref).strip() if ref is not None else ""
        img_path_str = str(img_path) if img_path is not None else ""
        
        # ì˜ˆì¸¡ê°’ í’ˆì§ˆ ë¶„ì„
        is_error = pred_str.startswith('[') and pred_str.endswith(']')
        is_empty = not pred_str or pred_str in ["", "[ë¹ˆ ì‘ë‹µ]"]
        
        # input_text ì¶”ì¶œ (ì¸ë±ìŠ¤ í™•ì¸ í›„ ì•ˆì „í•˜ê²Œ)
        input_text_str = ""
        if i < len(input_texts):
            input_text_str = str(input_texts[i]).strip() if input_texts[i] is not None else ""
        
        results_data.append({
            'sample_id': i,
            'image_path': img_path_str,
            'original_query': input_text_str,
            'prediction': pred_str,
            'reference': ref_str,
            'pred_length': len(pred_str.split()),
            'ref_length': len(ref_str.split()),
            'is_error': is_error,
            'is_empty': is_empty
        })
    
    # DataFrame ìƒì„± ë° ì €ì¥
    df = pd.DataFrame(results_data)
    csv_path = output_dir / f"predictions_{timestamp}.csv"
    df.to_csv(csv_path, index=False, encoding='utf-8')
    
    # ê°œì„ ëœ ê²°ê³¼ í†µê³„ ë¶„ì„
    total_samples = len(df)
    error_count = df['is_error'].sum()
    empty_count = df['is_empty'].sum()
    valid_count = total_samples - error_count - empty_count
    
    # ê¸¸ì´ í†µê³„ (ìœ íš¨í•œ ì˜ˆì¸¡ê°’ë§Œ)
    valid_df = df[~df['is_error'] & ~df['is_empty']]
    if len(valid_df) > 0:
        avg_pred_length = valid_df['pred_length'].mean()
        avg_ref_length = valid_df['ref_length'].mean()
        pred_length_std = valid_df['pred_length'].std()
    else:
        avg_pred_length = avg_ref_length = pred_length_std = 0.0
    
    logger.info(f"ğŸ“Š ìƒì„± í’ˆì§ˆ ë¶„ì„:")
    logger.info(f"   - ì´ ìƒ˜í”Œ: {total_samples}")
    logger.info(f"   - ì„±ê³µì  ìƒì„±: {valid_count}ê°œ ({valid_count/total_samples*100:.1f}%)")
    logger.info(f"   - ìƒì„± ì˜¤ë¥˜: {error_count}ê°œ ({error_count/total_samples*100:.1f}%)")
    logger.info(f"   - ë¹ˆ ì‘ë‹µ: {empty_count}ê°œ ({empty_count/total_samples*100:.1f}%)")
    
    if valid_count > 0:
        logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„:")
        logger.info(f"   - í‰ê·  ì˜ˆì¸¡ ê¸¸ì´: {avg_pred_length:.1f} Â± {pred_length_std:.1f} ë‹¨ì–´")
        logger.info(f"   - í‰ê·  ì •ë‹µ ê¸¸ì´: {avg_ref_length:.1f} ë‹¨ì–´")
        logger.info(f"   - ê¸¸ì´ ë¹„ìœ¨ (ì˜ˆì¸¡/ì •ë‹µ): {avg_pred_length/avg_ref_length:.2f}")
    
    logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
    return df
    


def calculate_evaluation_metrics(data_input, output_dir: Path, timestamp: str) -> Dict[str, float]:
    """
    5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (BLEU-4, METEOR, ROUGE-L, SPICE, CIDEr, CLIP-S, RefCLIP-S)
    
    Args:
        data_input: pandas DataFrame ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œ (str/Path)
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´
    """
    logger.info("=" * 60)
    logger.info("ğŸ“ˆ 5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°")
    logger.info("=" * 60)
    
    # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬: CSV íŒŒì¼ì´ë©´ DataFrameìœ¼ë¡œ ë³€í™˜
    if isinstance(data_input, (str, Path)):
        csv_path = Path(data_input)
        if not csv_path.exists():
            raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        
        logger.info(f"ğŸ“‚ CSV íŒŒì¼ ë¡œë“œ: {csv_path}")
        df = pd.read_csv(csv_path, encoding='utf-8')
        logger.info(f"âœ“ DataFrame ë³€í™˜ ì™„ë£Œ - ì´ {len(df)}ê°œ ìƒ˜í”Œ")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['prediction', 'reference']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"CSV íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}. í•„ìš”í•œ ì»¬ëŸ¼: {required_columns}")
        
        # ì˜µì…˜ ì»¬ëŸ¼ í™•ì¸ ë° ë¡œê·¸
        optional_columns = ['image_path']
        available_optional = [col for col in optional_columns if col in df.columns]
        logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: í•„ìˆ˜ {required_columns} + ì„ íƒ {available_optional}")
    
    elif isinstance(data_input, pd.DataFrame):
        df = data_input
        logger.info(f"âœ“ DataFrame ì…ë ¥ - ì´ {len(df)}ê°œ ìƒ˜í”Œ")
    else:
        raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {type(data_input)}. pandas DataFrame ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    # ìœ íš¨í•œ ìƒ˜í”Œë§Œ ì„ íƒ (ì˜ˆì¸¡ê³¼ ì •ë‹µê°€ ëª¨ë‘ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°)
    valid_df = df[(df['prediction'].str.strip() != '') & (df['reference'].str.strip() != '')]
    
    if len(valid_df) == 0:
        logger.error("âŒ ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    logger.info(f"ğŸ“Š í‰ê°€ ëŒ€ìƒ: {len(valid_df)}/{len(df)} ìƒ˜í”Œ")
    
    # ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (NaN ê°’ ì²˜ë¦¬)
    predictions = [str(pred) if pred is not None and not pd.isna(pred) else "" for pred in valid_df['prediction'].tolist()]
    references = [str(ref) if ref is not None and not pd.isna(ref) else "" for ref in valid_df['reference'].tolist()]
    
    # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
    valid_pairs = [(pred, ref) for pred, ref in zip(predictions, references) if pred.strip() and ref.strip()]
    
    if not valid_pairs:
        logger.error("âŒ ìœ íš¨í•œ ì˜ˆì¸¡-ì •ë‹µ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    predictions, references = zip(*valid_pairs)
    predictions = list(predictions)
    references = list(references)
    
    logger.info(f"ğŸ“Š ìµœì¢… í‰ê°€ ëŒ€ìƒ: {len(valid_pairs)} ìƒ˜í”Œ")
    
    metrics = {}
    
    # Assistant ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì •ë‹µìš©) - NaN ì²˜ë¦¬ ì¶”ê°€
    ref_texts_for_bleu = []
    for ref in references:
        if "Assistant:" in ref:
            assistant_part = ref.split("Assistant:")[-1].strip()
            ref_texts_for_bleu.append(assistant_part)
        else:
            ref_texts_for_bleu.append(ref)
    
    # 1. BLEU-4 ê³„ì‚°
    try:
        from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
        
        ref_tokens = [[ref.split()] for ref in ref_texts_for_bleu if ref.strip()]
        pred_tokens = [pred.split() for pred in predictions if pred.strip()]
        
        
        if len(ref_tokens) == 0 or len(pred_tokens) == 0:
            logger.warning("âš ï¸ BLEU-4: ìœ íš¨í•œ í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
            metrics['bleu4'] = 0.0
        else:
            smoothing = SmoothingFunction().method1
            metrics['bleu4'] = corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)
            logger.info(f"âœ“ BLEU-4: {metrics['bleu4']:.4f}")
    except Exception as e:
        logger.error(f"âŒ BLEU-4 ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['bleu4'] = 0.0
    
    # 2. METEOR ê³„ì‚°
    try:
        import nltk
        try:
            nltk.data.find('corpora/wordnet')
        except LookupError:
            logger.info("NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
            nltk.download('wordnet', quiet=True)
            nltk.download('punkt', quiet=True)
        
        from nltk.translate.meteor_score import meteor_score
        
        meteor_scores = []
        for ref, pred in zip(ref_texts_for_bleu, predictions):
            if ref.strip() and pred.strip():  # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                ref_tokens = ref.split()
                pred_tokens = pred.split()
                if len(ref_tokens) > 0 and len(pred_tokens) > 0:
                    score = meteor_score([ref_tokens], pred_tokens)
                    meteor_scores.append(score)
        
        if meteor_scores:
            metrics['meteor'] = float(np.mean(meteor_scores))
            logger.info(f"âœ“ METEOR: {metrics['meteor']:.4f}")
        else:
            logger.warning("âš ï¸ METEOR: ìœ íš¨í•œ ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['meteor'] = 0.0
    except Exception as e:
        logger.error(f"âŒ METEOR ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['meteor'] = 0.0
    
    # 3. ROUGE-L ê³„ì‚°
    try:
        from rouge_score import rouge_scorer
        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        
        rouge_scores = []
        for ref, pred in zip(ref_texts_for_bleu, predictions):
            if ref.strip() and pred.strip():  # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                scores = scorer.score(ref, pred)
                rouge_scores.append(scores['rougeL'].fmeasure)
        
        if rouge_scores:
            metrics['rougeL'] = float(np.mean(rouge_scores))
            logger.info(f"âœ“ ROUGE-L: {metrics['rougeL']:.4f}")
        else:
            logger.warning("âš ï¸ ROUGE-L: ìœ íš¨í•œ ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['rougeL'] = 0.0
        logger.info(f"âœ“ ROUGE-L: {metrics['rougeL']:.4f}")
    except Exception as e:
        logger.error(f"âŒ ROUGE-L ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['rougeL'] = 0.0
    
    # 4. SPICE ê³„ì‚° (ë” ì•ˆì „í•œ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬)
    try:
        from pycocoevalcap.spice.spice import Spice
        
        # SPICE ê³„ì‚°ì„ ë” ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        spice_scorer = Spice()
        
        # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
        valid_refs_for_spice = [ref for ref in ref_texts_for_bleu if ref.strip()]
        valid_preds_for_spice = [pred for pred in predictions if pred.strip()]
        
        if len(valid_refs_for_spice) == 0 or len(valid_preds_for_spice) == 0:
            logger.warning("âš ï¸ SPICE: ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['spice'] = 0.0
        else:
            gts = {str(i): [ref] for i, ref in enumerate(valid_refs_for_spice)}
            res = {str(i): [pred] for i, pred in enumerate(valid_preds_for_spice)}
            
            # ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ì´ìš©í•œ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ (ë” ì•ˆì „í•¨)
            import multiprocessing
            import queue
            
            def spice_calculate(gts, res, result_queue):
                try:
                    spice_score, _ = spice_scorer.compute_score(gts, res)
                    result_queue.put(('success', spice_score))
                except Exception as e:
                    result_queue.put(('error', str(e)))
            
            # í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•œ íƒ€ì„ì•„ì›ƒ
            result_queue = multiprocessing.Queue()
            process = multiprocessing.Process(target=spice_calculate, args=(gts, res, result_queue))
            process.start()
            process.join(timeout=60)  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
            
            if process.is_alive():
                process.terminate()
                process.join()
                raise TimeoutError("SPICE calculation timeout (60s)")
            
            # ê²°ê³¼ í™•ì¸
            try:
                result_type, result_value = result_queue.get_nowait()
                if result_type == 'success':
                    metrics['spice'] = float(result_value)
                    logger.info(f"âœ“ SPICE: {metrics['spice']:.4f}")
                else:
                    raise Exception(f"SPICE calculation failed: {result_value}")
            except queue.Empty:
                raise Exception("SPICE calculation returned no result")
            
    except (Exception, TimeoutError) as e:
        logger.warning(f"âš ï¸ SPICE ê³„ì‚° ì˜¤ë¥˜: {e}")
        # SPICE ëŒ€ì•ˆ: ê°„ë‹¨í•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
        try:
            logger.info("SPICE ëŒ€ì•ˆìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì‹œë„...")
            from sentence_transformers import SentenceTransformer
            model_st = SentenceTransformer('all-MiniLM-L6-v2')
            
            pred_embeddings = model_st.encode(predictions)
            ref_embeddings = model_st.encode(ref_texts_for_bleu)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = []
            for pred_emb, ref_emb in zip(pred_embeddings, ref_embeddings):
                sim = cosine_similarity([pred_emb], [ref_emb])[0][0]
                similarities.append(sim)
            
            metrics['spice'] = float(np.mean(similarities))
            logger.info(f"âœ“ SPICE (ëŒ€ì•ˆ-ì˜ë¯¸ìœ ì‚¬ë„): {metrics['spice']:.4f}")
        except Exception as fallback_e:
            logger.warning(f"âš ï¸ SPICE ëŒ€ì•ˆ ê³„ì‚°ë„ ì‹¤íŒ¨: {fallback_e}")
            metrics['spice'] = 0.0
    
    # 5. CIDEr ê³„ì‚°
    try:
        from pycocoevalcap.cider.cider import Cider
        cider_scorer = Cider()
        
        # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
        valid_refs_for_cider = [ref for ref in ref_texts_for_bleu if ref.strip()]
        valid_preds_for_cider = [pred for pred in predictions if pred.strip()]
        
        if len(valid_refs_for_cider) == 0 or len(valid_preds_for_cider) == 0:
            logger.warning("âš ï¸ CIDEr: ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['cider'] = 0.0
        else:
            gts = {str(i): [ref] for i, ref in enumerate(valid_refs_for_cider)}
            res = {str(i): [pred] for i, pred in enumerate(valid_preds_for_cider)}
            
            cider_score, _ = cider_scorer.compute_score(gts, res)
            metrics['cider'] = float(cider_score)
            logger.info(f"âœ“ CIDEr: {metrics['cider']:.4f}")
    except Exception as e:
        logger.warning(f"âš ï¸ CIDEr ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['cider'] = 0.0
    
    # 6. CLIP Score ì¸¡ì • ì œê±°ë¨ (ì‚¬ìš©ì ìš”ì²­)
    logger.info("â„¹ï¸ CLIP Score ë° RefCLIP-S ì¸¡ì •ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ë©”íŠ¸ë¦­ ì €ì¥
    metrics_path = output_dir / f"metrics_{timestamp}.json"
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ“ ë©”íŠ¸ë¦­ ì €ì¥: {metrics_path}")
    return metrics


def print_final_results(metrics: Dict[str, float]):
    """
    ìµœì¢… ê²°ê³¼ ì¶œë ¥
    """
    print("\n" + "=" * 80)
    print("ğŸ‰ PanoLLaVA ëª¨ë¸ í‰ê°€ ì™„ë£Œ")
    print("=" * 80)
    
    print("\nğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­ ê²°ê³¼:")
    print("-" * 40)
    
    if 'bleu4' in metrics:
        print(f"BLEU-4     (â†‘): {metrics['bleu4']:.4f}")
    if 'meteor' in metrics:
        print(f"METEOR     (â†‘): {metrics['meteor']:.4f}")
    if 'rougeL' in metrics:
        print(f"ROUGE-L    (â†‘): {metrics['rougeL']:.4f}")
    if 'spice' in metrics:
        print(f"SPICE      (â†‘): {metrics['spice']:.4f}")
    if 'cider' in metrics:
        print(f"CIDEr      (â†‘): {metrics['cider']:.4f}")
    # CLIP Score ì¶œë ¥ ì œê±°ë¨
    
    print("-" * 40)
    print("ğŸ’¡ (â†‘) í‘œì‹œëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤.")
    print("=" * 80)


def load_global_config():
    """Load global configuration from config.json"""
    config_path = Path("config.json")
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load config.json: {e}")
    return {}

def main():
    # Load global configuration
    global_config = load_global_config()

    env_config = global_config.get("environment", {})
    model_config = global_config.get("models", {})
    data_config = global_config.get("data", {})
    training_config = global_config.get("training", {})
    image_cfg = global_config.get("image_processing", {})
    system_msgs = global_config.get("system_messages", {})

    # ë””ë°”ì´ìŠ¤ ì„¤ì •: í™˜ê²½ë³€ìˆ˜ ëŒ€ì‹  config ê¸°ë°˜ìœ¼ë¡œ GPU indexë¥¼ ì„ íƒ
    cuda_vis = env_config.get("cuda_visible_devices")
    if cuda_vis and torch.cuda.is_available():
        try:
            # ì²« ë²ˆì§¸ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš© (ì˜ˆ: "1" ë˜ëŠ” "0,1" â†’ 1 ë˜ëŠ” 0)
            first_idx = int(str(cuda_vis).split(",")[0].strip())
            torch.cuda.set_device(first_idx)
            logger.info(f"Device: using GPU index {first_idx} (from config)")
        except Exception as e:
            logger.warning(f"Invalid cuda_visible_devices in config: {cuda_vis} ({e})")

    parser = argparse.ArgumentParser(description="PanoLLaVA ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ")
    parser.add_argument('--model-dir', default=None, help='ëª¨ë¸ ë””ë ‰í† ë¦¬(hf_model ë˜ëŠ” panorama_model). ì§€ì • ì—†ìœ¼ë©´ config.paths.pretrained_dir ìë™ ì‚¬ìš©')
    parser.add_argument('--stage', default=None, help='í•™ìŠµ ë‹¨ê³„ (e.g., finetune, resampler, vision)')
    parser.add_argument('--lora-weights-path', default=None, help='LoRA ê°€ì¤‘ì¹˜ ê²½ë¡œ (ìë™ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œì—ì„œ ì°¾ìŒ)')
    parser.add_argument('--csv-input', default=data_config.get("csv_test", "data/quic360/test.csv"), help='í…ŒìŠ¤íŠ¸ CSV íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output-dir', default='eval_results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')

    # âœ” config í‚¤ ì •ì •: vision_name / lm_model / resampler
    parser.add_argument('--vision-name', default=model_config.get("vision_name", "google/siglip-base-patch16-224"))
    parser.add_argument('--lm-name', default=model_config.get("lm_model", "Qwen/Qwen2.5-0.5B-Instruct"))
    parser.add_argument('--resampler', default=model_config.get("resampler", "mlp"))

    # âœ” image_processingì—ì„œ crop_strategy/overlap_ratio/use_vision_processor ê°€ì ¸ì˜¤ê¸°
    parser.add_argument('--crop-strategy', default=image_cfg.get("crop_strategy", "e2p"),
                        choices=['sliding_window', 'e2p', 'cubemap', 'resize', 'anyres', 'anyres_max'])
    parser.add_argument('--overlap-ratio', type=float, default=image_cfg.get("overlap_ratio", 0.5))
    parser.add_argument('--use-vision-processor', action='store_true' if image_cfg.get("use_vision_processor", True) else 'store_false')
    # Allow "auto" or an integer for max text length
    parser.add_argument(
        '--max-text-length',
        type=str,
        default=str(training_config.get("max_text_length", data_config.get("max_text_length", 256))),
        help='Max text length for tokenization. Use an integer, "auto" (model-based), or "auto:dataset" (measure from CSV).'
    )

    # ìƒì„± ê´€ë ¨
    # generation ì„¹ì…˜ì—ì„œ max_new_tokens ê°€ì ¸ì˜¤ê¸°(í‚¤ ìœ ì—° ì²˜ë¦¬)
    gen_cfg = global_config.get("generation", {}) if isinstance(global_config, dict) else {}
    def _get_gen_default(*keys, fallback=None):
        for k in keys:
            if isinstance(gen_cfg, dict) and k in gen_cfg:
                return gen_cfg.get(k)
        return fallback

    parser.add_argument('--max-new-tokens', type=int, default=_get_gen_default('max_new_tokens', 'max_new_token', 'max_tokens', fallback=128))
    parser.add_argument('--temperature', type=float, default=0.7)
    parser.add_argument('--min-new-tokens', type=int, default=5)
    parser.add_argument('--top-p', type=float, default=0.9)
    parser.add_argument('--top-k', type=int, default=50)
    parser.add_argument('--repetition-penalty', type=float, default=1.1)
    parser.add_argument('--length-penalty', type=float, default=1.0)

    parser.add_argument('--batch-size', type=int, default=16)
    parser.add_argument('--num-workers', type=int, default=training_config.get("num_workers", 16), help='ë°ì´í„°ë¡œë” ì›Œì»¤ ìˆ˜')

    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ (training.system_msg ìš°ì„ , ì—†ìœ¼ë©´ system_messages.default)
    default_sys_msg = training_config.get("system_msg", system_msgs.get("default", "You are a helpful assistant."))
    parser.add_argument('--system-msg', type=str, default=default_sys_msg)

    # ì„¤ì • ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°ë“¤
    parser.add_argument('--config', help='ModelConfig JSON íŒŒì¼ ê²½ë¡œ (ë¯¸ì§€ì • ì‹œ config.json ë¡œë“œê°’ ì‚¬ìš©)')

    args = parser.parse_args()

    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìë™ í•´ê²° (args.configê°€ ì—†ìœ¼ë©´ ë¡œë“œëœ global_config ì‚¬ìš©)
    model_dir = args.model_dir
    if model_dir is None:
        cfg_source = args.config if args.config else global_config
        model_dir = resolve_model_dir(cfg_source, args.stage, crop_strategy=args.crop_strategy)

    # LoRA ê°€ì¤‘ì¹˜ ìë™ ì„¤ì •
    lora_weights_path = args.lora_weights_path
    if lora_weights_path is None and model_dir:
        checkpoint_dir = Path(model_dir)
        checkpoint_dir = checkpoint_dir if checkpoint_dir.is_dir() else checkpoint_dir.parent
        potential_lora_path = checkpoint_dir / "lora_weights"
        if potential_lora_path.exists():
            lora_weights_path = str(potential_lora_path)
            logger.info(f"âœ… Auto-found LoRA weights: {lora_weights_path}")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    timestamp = time.strftime('%Y%m%d_%H%M%S')

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    try:
        # 1ë‹¨ê³„: ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
        # Convert max_text_length for model only if numeric; otherwise omit (DataModule handles "auto")
        _mtl_val = None
        try:
            _mtl_val = int(args.max_text_length)
        except Exception:
            _mtl_val = None
        model_kwargs = {
            "vision_name": args.vision_name,
            "lm_name": args.lm_name,
            "resampler": args.resampler,
            "lr": 1e-5,
            **({"max_text_length": _mtl_val} if _mtl_val is not None else {})
        }
        model = load_model_and_lora(
            model_dir,
            lora_weights_path,
            device,
            config_path=args.config,  # ModelConfigë¥¼ ë³„ë„ë¡œ ì“°ëŠ” ê²½ìš°
            **model_kwargs
        )

        # 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ (config ë°˜ì˜ ì¸ì ì¶”ê°€)
        datamodule, test_dataloader = prepare_test_dataset(
            csv_input=args.csv_input,
            batch_size=args.batch_size,
            max_text_length=args.max_text_length,
            crop_strategy=args.crop_strategy,
            lm_name=args.lm_name,
            num_workers=args.num_workers,
            overlap_ratio=args.overlap_ratio,
            vision_name=args.vision_name,
            system_msg=args.system_msg,
            use_vision_processor=args.use_vision_processor,
            auto_max_text_length_cap=int(global_config.get("data", {}).get("auto_max_text_length_cap", 8192)) if isinstance(global_config, dict) else 8192,
            auto_max_text_length_floor=int(global_config.get("data", {}).get("auto_max_text_length_floor", 512)) if isinstance(global_config, dict) else None,
            auto_max_text_length_scan_limit=int(global_config.get("data", {}).get("auto_max_text_length_scan_limit", 1000)) if isinstance(global_config, dict) else None
        )

        # 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ìƒì„± (system_msg ì „ë‹¬)
        predictions, references, image_paths, input_texts = generate_predictions(
            model, test_dataloader, datamodule, device,
            max_new_tokens=args.max_new_tokens, temperature=args.temperature,
            top_p=args.top_p, top_k=args.top_k,
            repetition_penalty=args.repetition_penalty, length_penalty=args.length_penalty,
            min_new_tokens=args.min_new_tokens,
            system_msg=args.system_msg
        )

        # 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ë¡œê¹…
        df = save_and_log_results(predictions, references, image_paths, input_texts, output_dir, timestamp)

        # 5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = calculate_evaluation_metrics(df, output_dir, timestamp)

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print_final_results(metrics)

    except Exception as e:
        logger.error(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise

if __name__ == '__main__':
    main()
