# coding: utf-8
"""
PanoLLaVA Comprehensive Model Evaluation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ìµœì¢… ëª¨ë¸ ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ:
â€¢ vlm_finetune ë° vlm_resampler ëª¨ë¸ ì§€ì›
â€¢ ëª¨ë“  í‰ê°€ ë©”íŠ¸ë¦­ í¬í•¨ (BLEU, ROUGE, METEOR, CIDEr, CLIP Score)
â€¢ generate ê¸°ë°˜ì˜ ì •í™•í•œ í…ìŠ¤íŠ¸ ìƒì„± í‰ê°€
â€¢ GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ ë° OOM ë°©ì§€
â€¢ ìƒì„¸í•œ ê²°ê³¼ ë¶„ì„ ë° ë¹„êµ

ì‚¬ìš© ì˜ˆì‹œ:
    # Finetune ëª¨ë¸ í‰ê°€
    python eval.py --ckpt runs/e2p_finetune_mlp/best-v1.ckpt --lora-weights-path runs/e2p_finetune_mlp/lora_weights --csv-input data/quic360/test.csv
"""

import argparse
import torch
import json
import logging
import sys
import time
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
from pathlib import Path
from tqdm import tqdm
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
from collections import defaultdict

# ë‚´ë¶€ ëª¨ë“ˆ
from train import VLMModule, VLMDataModule, get_gpu_memory_info, safe_load_checkpoint

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_model(checkpoint_path: str, stage: str, device: torch.device, 
               lora_weights_path: Optional[str] = None, **kwargs) -> VLMModule:
    """ëª¨ë¸ ë¡œë“œ (LoRA ì§€ì›)"""
    logger.info(f"Loading {stage} model from: {checkpoint_path}")
    
    # ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
    checkpoint = safe_load_checkpoint(checkpoint_path)
    if not checkpoint:
        raise ValueError(f"Failed to load checkpoint: {checkpoint_path}")
    
    # LoRA ìë™ ê°ì§€
    auto_detected_lora_path = None
    if lora_weights_path is None:
        # ì²´í¬í¬ì¸íŠ¸ì™€ ê°™ì€ ë””ë ‰í† ë¦¬ì—ì„œ lora_weights í´ë” ì°¾ê¸°
        checkpoint_dir = Path(checkpoint_path).parent
        potential_lora_path = checkpoint_dir / "lora_weights"
        if potential_lora_path.exists():
            auto_detected_lora_path = str(potential_lora_path)
            logger.info(f"ğŸ” Auto-detected LoRA weights: {auto_detected_lora_path}")
    
    final_lora_path = lora_weights_path or auto_detected_lora_path
    
    # ëª¨ë¸ ë¡œë“œ - evaluationì„ ìœ„í•´ì„œëŠ” ì›ë˜ stageë¡œ ë¡œë“œí•˜ë˜ ë‚˜ì¤‘ì— generate ëª¨ë“œë¡œ ì„¤ì •
    original_stage = stage
    model = VLMModule.load_from_checkpoint(
        checkpoint_path,
        stage=original_stage,
        map_location=device,
        **kwargs
    )
    
    # LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ (finetune ë‹¨ê³„ì—ì„œë§Œ)
    if final_lora_path and stage == "finetune":
        try:
            logger.info(f"ğŸ”§ Loading LoRA weights from: {final_lora_path}")
            
            # LoRA íŒŒì¼ ì¡´ì¬ í™•ì¸
            lora_path = Path(final_lora_path)
            if not lora_path.exists():
                logger.warning(f"âš ï¸ LoRA weights path does not exist: {final_lora_path}")
                logger.info("Continuing with base model...")
            else:
                # LoRA íŒŒì¼ êµ¬ì¡° í™•ì¸
                adapter_config = lora_path / "adapter_config.json"
                adapter_model = lora_path / "adapter_model.safetensors"
                
                if not adapter_config.exists():
                    logger.warning(f"âš ï¸ adapter_config.json not found in {final_lora_path}")
                if not adapter_model.exists():
                    logger.warning(f"âš ï¸ adapter_model.safetensors not found in {final_lora_path}")
                
                success = model.model.load_lora_weights(final_lora_path)
                if success:
                    logger.info("âœ… LoRA weights loaded successfully!")
                    
                    # LoRA ì •ë³´ ì¶œë ¥
                lora_info = model.model.get_lora_info()
                if lora_info.get("is_lora_enabled", False):
                    logger.info(f"ğŸ“Š LoRA Configuration:")
                    logger.info(f"   - Rank: {lora_info.get('lora_r', 'N/A')}")
                    logger.info(f"   - Alpha: {lora_info.get('lora_alpha', 'N/A')}")
                    logger.info(f"   - Dropout: {lora_info.get('lora_dropout', 'N/A')}")
                    logger.info(f"   - Target modules: {lora_info.get('target_modules', 'N/A')}")
                else:
                    logger.warning("âš ï¸ LoRA weights loading failed, using base model")
        except Exception as e:
            logger.warning(f"âš ï¸ Error loading LoRA weights: {e}")
            logger.info("Continuing with base model...")
    elif final_lora_path and stage != "finetune":
        logger.warning(f"âš ï¸ LoRA weights found but stage is '{stage}'. LoRA only supported for finetune stage.")
    
    # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    model.eval()
    model = model.to(device)
    
    # ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ unfrozeí•˜ì—¬ generateí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
    model.model.requires_grad_(False)  # gradientëŠ” í•„ìš”ì—†ì§€ë§Œ forwardëŠ” ê°€ëŠ¥í•˜ë„ë¡
    
    logger.info(f"âœ“ {stage.capitalize()} model loaded successfully on {device}")
    logger.info(f"Model stage: {model._stage_key}")
    
    return model

def evaluate_model(model: VLMModule, dataloader, stage: str, device: torch.device, 
                  generation_params: Dict[str, Any], datamodule) -> Tuple[List[str], List[str], List[Dict], int]:
    """ëª¨ë¸ í‰ê°€ ìˆ˜í–‰"""
    logger.info(f"Starting {stage} model evaluation...")
    
    predictions = []
    references = []
    metadata = []
    error_count = 0
    start_time = time.time()
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f"Evaluating {stage}")):
            try:
                # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
                pixel_values = batch["pixel_values"].to(device)
                batch_size = pixel_values.shape[0]
                
                # ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
                if batch_idx == 0:
                    logger.info(f"First batch debug info:")
                    logger.info(f"  Pixel values shape: {pixel_values.shape}")
                    logger.info(f"  Batch keys: {list(batch.keys())}")
                    logger.info(f"  Device: {pixel_values.device}")
                    logger.info(f"  Model device: {next(model.parameters()).device}")
                
                # 5ë²ˆì§¸ ë°°ì¹˜ë§ˆë‹¤ ì§„í–‰ ìƒí™© ë¡œê·¸
                if batch_idx % 5 == 0:
                    progress_pct = (batch_idx / len(dataloader)) * 100
                    elapsed = time.time() - start_time
                    estimated_total = elapsed / (batch_idx + 1) * len(dataloader)
                    remaining = estimated_total - elapsed
                    logger.info(f"Processing batch {batch_idx}/{len(dataloader)} ({progress_pct:.1f}%) - "
                              f"Elapsed: {elapsed/60:.1f}min, Remaining: {remaining/60:.1f}min")
                
                # Ground truth ì¶”ì¶œ
                gt_texts = []
                if "text" in batch:
                    gt_texts = batch["text"] if isinstance(batch["text"], list) else [batch["text"]]
                elif "reference" in batch:
                    # labelsê°€ í…ì„œì¸ ê²½ìš° ë””ì½”ë”©
                    labels = batch["reference"]
                    if torch.is_tensor(labels):
                        # datamoduleì˜ tokenizer ì‚¬ìš©í•´ì„œ ë””ì½”ë”©
                        try:
                            # -100 í† í° ì œê±° (loss ë§ˆìŠ¤í‚¹ëœ ë¶€ë¶„)
                            labels_for_decode = labels.clone()
                            labels_for_decode[labels_for_decode == -100] = datamodule.tokenizer.pad_token_id
                            gt_texts = datamodule.tokenizer.batch_decode(labels_for_decode, skip_special_tokens=True)
                            
                            # ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš© (ìë¥´ì§€ ì•ŠìŒ)
                            processed_gt_texts = []
                            for gt_text in gt_texts:
                                # Assistant ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì¶”ì¶œí•˜ë˜, ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
                                if "Assistant:" in gt_text:
                                    # Assistant: ì´í›„ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ê¸°ì¡´ ë™ì‘ ìœ ì§€í•˜ë˜ ë¡œê·¸ ì¶”ê°€)
                                    assistant_part = gt_text.split("Assistant:")[-1].strip()
                                    full_text = gt_text.strip()
                                    
                                    # ë””ë²„ê¹…ì„ ìœ„í•´ ì˜ë¦° í…ìŠ¤íŠ¸ì™€ ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„êµ
                                    if len(processed_gt_texts) < 2:  # ì²˜ìŒ 2ê°œë§Œ ë¡œê·¸
                                        logger.debug(f"Reference processing - Full: {len(full_text)} chars, Assistant only: {len(assistant_part)} chars")
                                    
                                    # ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš© (Assistant ë¶€ë¶„ë§Œ ìë¥´ì§€ ì•ŠìŒ)
                                    processed_gt_texts.append(full_text)
                                else:
                                    processed_gt_texts.append(gt_text.strip())
                            gt_texts = processed_gt_texts
                            
                            logger.debug(f"Decoded GT texts (ìƒ˜í”Œ ê¸¸ì´): {[len(gt) for gt in gt_texts[:2]]}")  # ê¸¸ì´ ì •ë³´ í¬í•¨
                            
                        except Exception as e:
                            logger.error(f"Failed to decode labels: {e}")
                            gt_texts = [f"decode_error_{i}" for i in range(batch_size)]
                    else:
                        gt_texts = labels if isinstance(labels, list) else [labels]
                elif "input_text" in batch:
                    # input_textì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš© (Assistant ë¶€ë¶„ë§Œ ìë¥´ì§€ ì•ŠìŒ)
                    input_texts = batch["input_text"] if isinstance(batch["input_text"], list) else [batch["input_text"]]
                    gt_texts = []
                    for text in input_texts:
                        if "Assistant:" in text:
                            # ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©í•˜ë˜ Assistant ë¶€ë¶„ ì •ë³´ëŠ” ë¡œê·¸ì— ê¸°ë¡
                            assistant_part = text.split("Assistant:")[-1].strip()
                            full_text = text.strip()
                            
                            if len(gt_texts) < 2:  # ì²˜ìŒ 2ê°œë§Œ ë¡œê·¸
                                logger.debug(f"Input text processing - Full: {len(full_text)} chars, Assistant only: {len(assistant_part)} chars")
                            
                            # ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
                            gt_texts.append(full_text)
                        else:
                            gt_texts.append(text.strip() if text else "")
                else:
                    gt_texts = [f"no_gt_{i}" for i in range(batch_size)]
                
                # ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ
                image_paths = batch.get("image_path", [f"batch_{batch_idx}_sample_{i}" for i in range(batch_size)])
                
                # ëª¨ë¸ ì¶”ë¡  (generate ëª¨ë“œ)
                # PanoramaVLMì˜ generate ë©”ì„œë“œ ì§ì ‘ í˜¸ì¶œ
                try:
                    # ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ ëª¨ë¸ ìƒíƒœ í™•ì¸
                    if batch_idx == 0:
                        logger.info(f"Model generate method available: {hasattr(model.model, 'generate')}")
                        logger.info(f"Model training mode: {model.training}")
                        logger.info(f"Generation params: {generation_params}")
                        
                        # í† í¬ë‚˜ì´ì € ìƒíƒœ í™•ì¸
                        if hasattr(datamodule, 'tokenizer') and datamodule.tokenizer:
                            logger.info(f"Tokenizer EOS token ID: {datamodule.tokenizer.eos_token_id}")
                            logger.info(f"Tokenizer PAD token ID: {datamodule.tokenizer.pad_token_id}")
                        else:
                            logger.warning("Tokenizer not available for generation parameters")
                    
                    # input_idsëŠ” Noneìœ¼ë¡œ ì„¤ì • (ì´ë¯¸ì§€ë§Œìœ¼ë¡œ ìƒì„±)
                    # ìœ íš¨í•œ generation íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©
                    gen_kwargs = {
                        "pixel_values": pixel_values,
                        "input_ids": None,
                        "max_new_tokens": generation_params.get("max_new_tokens", 128),
                        "temperature": generation_params.get("temperature", 0.7),
                        "do_sample": True,  # temperature ì‚¬ìš© ì‹œ í•„ìš”
                    }
                    
                    # í† í¬ë‚˜ì´ì € ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ íŒ¨ë”© í† í° ì„¤ì •
                    if hasattr(datamodule, 'tokenizer') and datamodule.tokenizer:
                        if datamodule.tokenizer.eos_token_id is not None:
                            gen_kwargs["pad_token_id"] = datamodule.tokenizer.eos_token_id
                        elif datamodule.tokenizer.pad_token_id is not None:
                            gen_kwargs["pad_token_id"] = datamodule.tokenizer.pad_token_id
                    
                    # early_stopping ë“± ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œê±°
                    # (transformers ê²½ê³  ë°©ì§€)
                    
                    output = model.model.generate(**gen_kwargs)
                    
                    # ì¶œë ¥ ê²€ì¦ ë° ì²˜ë¦¬
                    if isinstance(output, dict) and "text" in output:
                        batch_predictions = output["text"]
                        if batch_idx == 0:
                            logger.info(f"Successfully generated text output: {len(batch_predictions)} predictions")
                    elif isinstance(output, torch.Tensor):
                        logger.warning(f"Received tensor output instead of text dict: {output.shape}")
                        batch_predictions = []
                    else:
                        logger.error(f"Unexpected output format: {type(output)}")
                        if hasattr(output, '__dict__'):
                            logger.error(f"Output attributes: {list(output.__dict__.keys())}")
                        batch_predictions = []
                        
                    
                    if batch_predictions and batch_idx % 10 == 0:  # ë§¤ 10ë²ˆì§¸ ë°°ì¹˜ë§ˆë‹¤ ìƒ˜í”Œ ì¶œë ¥
                        for i, pred in enumerate(batch_predictions[:2]):  # ì²˜ìŒ 2ê°œë§Œ
                            logger.info(f"  Sample {i}: '{pred}' (length: {len(pred)})")
                    
                    logger.debug(f"Generated {len(batch_predictions)} predictions for batch {batch_idx}")
                    
                except Exception as e:
                    logger.error(f"Generation failed for batch {batch_idx}: {e}")
                    logger.error(f"Error details: {str(e)}")
                    import traceback
                    logger.error(f"Traceback: {traceback.format_exc()}")
                    # ë¹ˆ ì˜ˆì¸¡ìœ¼ë¡œ í´ë°±
                    batch_predictions = [""] * batch_size
                
                # ë°°ì¹˜ í¬ê¸° ê²€ì¦
                if not isinstance(batch_predictions, list):
                    batch_predictions = [batch_predictions]
                
                # ë°°ì¹˜ í¬ê¸°ê°€ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ë¡œê·¸ ì¶œë ¥
                if len(batch_predictions) != batch_size:
                    logger.warning(f"Batch size mismatch: expected {batch_size}, got {len(batch_predictions)}")
                
                # ê²°ê³¼ ì €ì¥
                for i, pred in enumerate(batch_predictions):
                    predictions.append(pred)
                    
                    if i < len(gt_texts):
                        references.append(gt_texts[i])
                    else:
                        references.append("")
                    
                    metadata.append({
                        'stage': stage,
                        'batch_idx': batch_idx,
                        'sample_idx': i,
                        'image_path': image_paths[i] if i < len(image_paths) else "",
                        'prediction_length': len(pred.split()),
                        'reference_length': len(gt_texts[i].split()) if i < len(gt_texts) else 0
                    })
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if batch_idx % 10 == 0:
                    torch.cuda.empty_cache()
                    
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    error_count += 1
                    logger.error(f"CUDA OOM in batch {batch_idx} for {stage} model. Batch size: {batch_size}")
                    torch.cuda.empty_cache()
                    
                    # ë°°ì¹˜ í¬ê¸°ê°€ 1ë³´ë‹¤ í¬ë©´ ê°œë³„ ì²˜ë¦¬ ì‹œë„
                    if batch_size > 1:
                        logger.info(f"Attempting individual processing for batch {batch_idx}")
                        try:
                            for sample_idx in range(batch_size):
                                sample_pixel_values = pixel_values[sample_idx:sample_idx+1]
                                sample_params = {
                                    "pixel_values": sample_pixel_values,
                                    "input_ids": None,
                                    "max_new_tokens": generation_params.get("max_new_tokens", 128),
                                    "temperature": generation_params.get("temperature", 0.7),
                                    "do_sample": True,
                                }
                                
                                # í† í¬ë‚˜ì´ì € ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ íŒ¨ë”© í† í° ì„¤ì •
                                if hasattr(datamodule, 'tokenizer') and datamodule.tokenizer:
                                    if datamodule.tokenizer.eos_token_id is not None:
                                        sample_params["pad_token_id"] = datamodule.tokenizer.eos_token_id
                                    elif datamodule.tokenizer.pad_token_id is not None:
                                        sample_params["pad_token_id"] = datamodule.tokenizer.pad_token_id
                                
                                sample_output = model.model.generate(**sample_params)
                                sample_pred = sample_output["text"][0] if isinstance(sample_output, dict) else sample_output[0]
                                
                                predictions.append(sample_pred)
                                
                                if sample_idx < len(gt_texts):
                                    references.append(gt_texts[sample_idx])
                                else:
                                    references.append("")
                                
                                metadata.append({
                                    'stage': stage,
                                    'batch_idx': batch_idx,
                                    'sample_idx': sample_idx,
                                    'image_path': image_paths[sample_idx] if sample_idx < len(image_paths) else "",
                                    'prediction_length': len(sample_pred.split()),
                                    'reference_length': len(gt_texts[sample_idx].split()) if sample_idx < len(gt_texts) else 0,
                                    'individual_processing': True
                                })
                            
                            logger.info(f"âœ“ Individual processing successful for batch {batch_idx}")
                        except Exception as individual_e:
                            logger.error(f"Individual processing also failed for batch {batch_idx}: {individual_e}")
                    continue
                else:
                    raise
            except Exception as e:
                error_count += 1
                logger.error(f"Error in batch {batch_idx} for {stage} model: {e}")
                continue
    
    elapsed_time = time.time() - start_time
    logger.info(f"{stage.capitalize()} evaluation completed in {elapsed_time/60:.1f} minutes")
    logger.info(f"Total samples: {len(predictions)}, Errors: {error_count}")
    
    return predictions, references, metadata, error_count

def main():
    parser = argparse.ArgumentParser(description="PanoLLaVA Simple Evaluation")
    parser.add_argument('--ckpt', required=True, help='Model checkpoint')
    parser.add_argument('--lora-weights-path', help='LoRA weights directory (optional)')
    parser.add_argument('--csv-input', required=True, help='ì…ë ¥ CSV íŒŒì¼ (ì˜ˆ: test.csv)')
    parser.add_argument('--vision-name', default='google/siglip-base-patch16-224')
    parser.add_argument('--lm-name', default='Qwen/Qwen2.5-0.5B')
    parser.add_argument('--resampler', default='mlp')
    parser.add_argument('--crop-strategy', default='e2p', choices=['sliding_window', 'e2p', 'cubemap', 'resize', 'anyres', 'anyres_max'])
    parser.add_argument('--max-text-length', type=int, default=1024, 
                        help='Maximum text sequence length for tokenization (default: 256)')
    parser.add_argument('--max-new-tokens', type=int, default=128, 
                        help='Maximum new tokens to generate (default: 128)')
    parser.add_argument('--temperature', type=float, default=0.7)
    parser.add_argument('--num-workers', type=int, default=0)
    parser.add_argument('--batch-size', type=int, default=1)
    args = parser.parse_args()

    # 1. ê²½ë¡œ/íŒŒì¼ëª… ì„¸íŒ…
    eval_dir = Path('eval_results')
    eval_dir.mkdir(exist_ok=True)
    timestamp = time.strftime('%y%m%d_%H%M')
    csv_out = eval_dir / f"eval_{timestamp}.csv"
    json_out = eval_dir / f"result_{timestamp}.json"

    # 2. ëª¨ë¸ ë¡œë“œ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_kwargs = {
        "vision_name": args.vision_name,
        "lm_name": args.lm_name,
        "resampler": args.resampler,
        "lr": 1e-5,
        "max_text_length": args.max_text_length
    }
    model = load_model(args.ckpt, 'finetune', device, lora_weights_path=args.lora_weights_path, **model_kwargs)
    model.eval()
    model = model.to(device)

    # 3. ë°ì´í„°ì…‹ ì¤€ë¹„ (ì…ë ¥ CSV ì‚¬ìš©)
    datamodule = VLMDataModule(
        csv_train=args.csv_input,
        csv_val=args.csv_input,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        tokenizer_name=args.lm_name,
        max_text_length=args.max_text_length,
        crop_strategy=args.crop_strategy,
        eval_mode=True
    )
    datamodule.setup()
    dataloader = datamodule.val_dataloader()

    # 4. ì˜ˆì¸¡ ìˆ˜í–‰
    predictions, references, metadata, error_count = evaluate_model(
        model, dataloader, 'finetune', device,
        {"max_new_tokens": args.max_new_tokens, "temperature": args.temperature},
        datamodule
    )

    # 5. CSV ì €ì¥
    sample_data = []
    for i, (pred, ref, meta) in enumerate(zip(predictions, references, metadata)):
        sample_data.append({
            'sample_id': i,
            'stage': 'finetune',
            'prediction': pred,
            'reference': ref,
            'prediction_length': len(pred.split()),
            'reference_length': len(ref.split()),
            'is_empty_prediction': not pred.strip(),
            'image_path': meta.get('image_path', ''),
            'batch_idx': meta.get('batch_idx', ''),
            'individual_processing': meta.get('individual_processing', False)
        })
    df = pd.DataFrame(sample_data)
    df.to_csv(csv_out, index=False, encoding="utf-8")
    print(f"âœ“ ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥: {csv_out}")

    # 6. ì§€í‘œ ê³„ì‚° ë° JSON ì €ì¥
    valid = df[(df["prediction"].str.strip() != "") & (df["reference"].str.strip() != "")]
    if len(valid) == 0:
        print("âš ï¸ predictionê³¼ referenceê°€ ëª¨ë‘ ìˆëŠ” ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ì§€í‘œ ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    refs = valid["reference"].tolist()
    preds = valid["prediction"].tolist()
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = {}
    try:
        from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
        ref_tokens = [[ref.split()] for ref in refs]
        pred_tokens = [pred.split() for pred in preds]
        smoothing = SmoothingFunction().method1
        metrics['bleu1'] = corpus_bleu(ref_tokens, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing)
        metrics['bleu2'] = corpus_bleu(ref_tokens, pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)
        metrics['bleu3'] = corpus_bleu(ref_tokens, pred_tokens, weights=(1/3, 1/3, 1/3, 0), smoothing_function=smoothing)
        metrics['bleu4'] = corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)
    except Exception as e:
        print(f"BLEU ê³„ì‚° ì˜¤ë¥˜: {e}")
    try:
        from rouge_score import rouge_scorer
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        rouge1_scores, rouge2_scores, rougeL_scores = [], [], []
        for ref, pred in zip(refs, preds):
            scores = scorer.score(ref, pred)
            rouge1_scores.append(scores['rouge1'].fmeasure)
            rouge2_scores.append(scores['rouge2'].fmeasure)
            rougeL_scores.append(scores['rougeL'].fmeasure)
        metrics['rouge1'] = float(np.mean(rouge1_scores))
        metrics['rouge2'] = float(np.mean(rouge2_scores))
        metrics['rougeL'] = float(np.mean(rougeL_scores))
    except Exception as e:
        print(f"ROUGE ê³„ì‚° ì˜¤ë¥˜: {e}")
    # METEOR ì ìˆ˜ ê³„ì‚° (ìˆ˜ì •ë¨)
    try:
        import nltk
        # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
        try:
            nltk.data.find('corpora/wordnet')
        except LookupError:
            print("ë‹¤ìš´ë¡œë”© NLTK WordNet...")
            nltk.download('wordnet', quiet=True)
            nltk.download('punkt', quiet=True)
            nltk.download('omw-1.4', quiet=True)
        
        from nltk.translate.meteor_score import meteor_score
        meteor_scores = []
        for ref, pred in zip(refs, preds):
            try:
                # METEORëŠ” ë‹¨ì–´ ë‹¨ìœ„ë¡œ ê³„ì‚°ë¨
                ref_tokens = ref.split()
                pred_tokens = pred.split()
                if len(ref_tokens) > 0 and len(pred_tokens) > 0:
                    score = meteor_score([ref_tokens], pred_tokens)
                    meteor_scores.append(score)
                else:
                    meteor_scores.append(0.0)
            except Exception as e:
                print(f"METEOR ê°œë³„ ê³„ì‚° ì˜¤ë¥˜: {e}")
                meteor_scores.append(0.0)
        
        if meteor_scores:
            metrics['meteor'] = float(np.mean(meteor_scores))
            print(f"METEOR ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {metrics['meteor']:.4f} (ì´ {len(meteor_scores)}ê°œ ìƒ˜í”Œ)")
        else:
            metrics['meteor'] = 0.0
            print("METEOR ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: ìœ íš¨í•œ ì ìˆ˜ê°€ ì—†ìŒ")
            
    except Exception as e:
        print(f"METEOR ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['meteor'] = 0.0
    
    # ê¸°ë³¸ í†µê³„ (í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¶„ì„ í¬í•¨)
    ref_lengths = [len(ref.split()) for ref in refs]
    pred_lengths = [len(pred.split()) for pred in preds]
    
    # ê¸´ í…ìŠ¤íŠ¸ ë¶„ì„
    long_refs = [ref for ref in refs if len(ref.split()) > 50]
    long_preds = [pred for pred in preds if len(pred.split()) > 50]
    
    metrics['avg_ref_length'] = float(np.mean(ref_lengths))
    metrics['avg_pred_length'] = float(np.mean(pred_lengths))
    metrics['max_ref_length'] = float(max(ref_lengths)) if ref_lengths else 0
    metrics['max_pred_length'] = float(max(pred_lengths)) if pred_lengths else 0
    metrics['long_refs_count'] = len(long_refs)
    metrics['long_preds_count'] = len(long_preds)
    metrics['long_refs_ratio'] = len(long_refs) / len(refs) if refs else 0
    metrics['long_preds_ratio'] = len(long_preds) / len(preds) if preds else 0
    metrics['length_ratio'] = float(np.mean(pred_lengths) / np.mean(ref_lengths)) if np.mean(ref_lengths) > 0 else 0
    metrics['empty_predictions'] = float(sum(1 for pred in preds if not pred.strip()) / len(preds))
    
    # SPICE ì ìˆ˜ ê³„ì‚° (pycocoevalcap ì‚¬ìš©)
    try:
        from pycocoevalcap.spice.spice import Spice
        spice_scorer = Spice()
        
        # SPICEëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì…ë ¥ì„ ìš”êµ¬í•¨
        gts = {str(i): [ref] for i, ref in enumerate(refs)}
        res = {str(i): [pred] for i, pred in enumerate(preds)}
        
        spice_score, _ = spice_scorer.compute_score(gts, res)
        metrics['spice'] = float(spice_score)
        print(f"SPICE ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {metrics['spice']:.4f}")
        
    except ImportError:
        print("SPICE ê³„ì‚°ì„ ìœ„í•´ pycocoevalcapì„ ì„¤ì¹˜í•˜ì„¸ìš”: pip install pycocoevalcap")
        metrics['spice'] = 0.0
    except Exception as e:
        print(f"SPICE ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['spice'] = 0.0
    
    # CLIP-based ë©”íŠ¸ë¦­ë“¤ ê³„ì‚° (CLIP Score, CLIP-S, RefCLIP-S)
    try:
        import clip
        from PIL import Image
        device_clip = "cuda" if torch.cuda.is_available() else "cpu"
        model_clip, preprocess = clip.load("ViT-B/32", device=device_clip)
        
        clip_scores = []
        clip_s_scores = []  # CLIP-S (Image-Text Cosine Similarity)
        ref_clip_s_scores = []  # RefCLIP-S (Reference-Prediction Similarity)
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
        pred_texts = []
        ref_texts = []
        valid_indices = []
        
        for idx, row in valid.iterrows():
            img_path = row['image_path']
            if img_path and os.path.exists(img_path):
                pred_texts.append(row['prediction'])
                ref_texts.append(row['reference'])
                valid_indices.append(idx)
        
        if pred_texts:
            print(f"CLIP ê¸°ë°˜ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘... ({len(pred_texts)}ê°œ ìƒ˜í”Œ)")
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ í…ìŠ¤íŠ¸ë“¤ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
            chunk_size = 32  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
            
            for chunk_start in range(0, len(pred_texts), chunk_size):
                chunk_end = min(chunk_start + chunk_size, len(pred_texts))
                chunk_pred_texts = pred_texts[chunk_start:chunk_end]
                chunk_ref_texts = ref_texts[chunk_start:chunk_end]
                chunk_indices = valid_indices[chunk_start:chunk_end]
                
                # í…ìŠ¤íŠ¸ ì„ë² ë”© ê³„ì‚° (ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°œì„ )
                try:
                    # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜
                    def preprocess_text_for_clip(text, max_length=75):
                        """CLIP í† í° ì œí•œì„ ê³ ë ¤í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
                        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ìœ ì§€
                        words = text.split()
                        if len(words) > max_length:
                            # ì•ìª½ ì ˆë°˜ê³¼ ë’¤ìª½ ì ˆë°˜ì„ ìœ ì§€ (ì¤‘ê°„ ìƒëµ)
                            half_length = max_length // 2
                            truncated = words[:half_length] + ["..."] + words[-half_length:]
                            return " ".join(truncated)
                        return text
                    
                    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
                    processed_pred_texts = [preprocess_text_for_clip(text) for text in chunk_pred_texts]
                    processed_ref_texts = [preprocess_text_for_clip(text) for text in chunk_ref_texts]
                    
                    # í† í°í™” ì‹œ ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°œì„ 
                    pred_tokens = clip.tokenize(processed_pred_texts, truncate=True).to(device_clip)
                    ref_tokens = clip.tokenize(processed_ref_texts, truncate=True).to(device_clip)
                    
                    # ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¡œê¹… (ì²« ë²ˆì§¸ ì²­í¬ì—ì„œë§Œ)
                    if chunk_start == 0:
                        for i in range(min(2, len(chunk_pred_texts))):
                            orig_pred_len = len(chunk_pred_texts[i])
                            orig_ref_len = len(chunk_ref_texts[i])
                            proc_pred_len = len(processed_pred_texts[i])
                            proc_ref_len = len(processed_ref_texts[i])
                            logger.debug(f"CLIP text processing {i}: Pred {orig_pred_len}â†’{proc_pred_len} chars, Ref {orig_ref_len}â†’{proc_ref_len} chars")
                    
                    with torch.no_grad():
                        pred_text_features = model_clip.encode_text(pred_tokens)
                        ref_text_features = model_clip.encode_text(ref_tokens)
                        
                        # ì •ê·œí™”
                        pred_text_features = pred_text_features / pred_text_features.norm(dim=-1, keepdim=True)
                        ref_text_features = ref_text_features / ref_text_features.norm(dim=-1, keepdim=True)
                        
                        # RefCLIP-S: Referenceì™€ Prediction ê°„ì˜ ìœ ì‚¬ë„
                        ref_clip_s_batch = torch.diagonal(pred_text_features @ ref_text_features.T).cpu().numpy()
                        ref_clip_s_scores.extend(ref_clip_s_batch.tolist())
                        
                        # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ CLIP Scoreì™€ CLIP-S ê³„ì‚°
                        for i, (pred_feat, idx) in enumerate(zip(pred_text_features, chunk_indices)):
                            try:
                                row = valid.iloc[valid_indices.index(idx)]
                                img_path = row['image_path']
                                
                                if os.path.exists(img_path):
                                    image = preprocess(Image.open(img_path).convert("RGB")).unsqueeze(0).to(device_clip)
                                    image_features = model_clip.encode_image(image)
                                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                                    
                                    # CLIP Score (Image-Prediction similarity)
                                    clip_score = (image_features @ pred_feat.unsqueeze(0).T).item()
                                    clip_scores.append(clip_score)
                                    
                                    # CLIP-S (ë™ì¼í•¨ - Image-Text Cosine Similarity)
                                    clip_s_scores.append(clip_score)
                                    
                            except Exception as e:
                                print(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {idx}): {e}")
                                continue
                                
                except Exception as e:
                    print(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            # ë©”íŠ¸ë¦­ ì €ì¥
            if clip_scores:
                metrics['clip_score'] = float(np.mean(clip_scores))
                metrics['clip_score_std'] = float(np.std(clip_scores))
                print(f"CLIP Score ê³„ì‚° ì™„ë£Œ: {metrics['clip_score']:.4f} Â± {metrics['clip_score_std']:.4f}")
            
            if clip_s_scores:
                metrics['clip_s'] = float(np.mean(clip_s_scores))
                metrics['clip_s_std'] = float(np.std(clip_s_scores))
                print(f"CLIP-S ê³„ì‚° ì™„ë£Œ: {metrics['clip_s']:.4f} Â± {metrics['clip_s_std']:.4f}")
            
            if ref_clip_s_scores:
                metrics['ref_clip_s'] = float(np.mean(ref_clip_s_scores))
                metrics['ref_clip_s_std'] = float(np.std(ref_clip_s_scores))
                print(f"RefCLIP-S ê³„ì‚° ì™„ë£Œ: {metrics['ref_clip_s']:.4f} Â± {metrics['ref_clip_s_std']:.4f}")
        
        else:
            print("ìœ íš¨í•œ ì´ë¯¸ì§€ ê²½ë¡œê°€ ì—†ì–´ CLIP ê¸°ë°˜ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except ImportError:
        print("CLIP ë©”íŠ¸ë¦­ ê³„ì‚°ì„ ìœ„í•´ CLIPì„ ì„¤ì¹˜í•˜ì„¸ìš”: pip install git+https://github.com/openai/CLIP.git")
    except Exception as e:
        print(f"CLIP ê¸°ë°˜ ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

    with open(json_out, "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    print(f"âœ“ í‰ê°€ ë©”íŠ¸ë¦­ JSON ì €ì¥: {json_out}")
    
    # ë©”íŠ¸ë¦­ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­ ìš”ì•½")
    print("="*60)
    
    # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­
    print("ğŸ”¤ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­:")
    if 'bleu4' in metrics:
        print(f"  BLEU-4:     {metrics['bleu4']:.4f}")
    if 'rouge1' in metrics:
        print(f"  ROUGE-1:    {metrics['rouge1']:.4f}")
    if 'rougeL' in metrics:
        print(f"  ROUGE-L:    {metrics['rougeL']:.4f}")
    if 'meteor' in metrics and metrics['meteor'] > 0:
        print(f"  METEOR:     {metrics['meteor']:.4f}")
    if 'spice' in metrics and metrics['spice'] > 0:
        print(f"  SPICE:      {metrics['spice']:.4f}")
    
    # ë©€í‹°ëª¨ë‹¬ ë©”íŠ¸ë¦­
    print("\nğŸ–¼ï¸  ë©€í‹°ëª¨ë‹¬ ë©”íŠ¸ë¦­:")
    if 'clip_score' in metrics:
        print(f"  CLIP Score: {metrics['clip_score']:.4f} Â± {metrics.get('clip_score_std', 0):.4f}")
    if 'clip_s' in metrics:
        print(f"  CLIP-S:     {metrics['clip_s']:.4f} Â± {metrics.get('clip_s_std', 0):.4f}")
    if 'ref_clip_s' in metrics:
        print(f"  RefCLIP-S:  {metrics['ref_clip_s']:.4f} Â± {metrics.get('ref_clip_s_std', 0):.4f}")
    
    # ê¸°ë³¸ í†µê³„
    print("\nğŸ“ˆ ê¸°ë³¸ í†µê³„:")
    print(f"  í‰ê·  ì˜ˆì¸¡ ê¸¸ì´:     {metrics['avg_pred_length']:.1f} ë‹¨ì–´")
    print(f"  í‰ê·  ì°¸ì¡° ê¸¸ì´:     {metrics['avg_ref_length']:.1f} ë‹¨ì–´")
    print(f"  ìµœëŒ€ ì˜ˆì¸¡ ê¸¸ì´:     {metrics['max_pred_length']:.0f} ë‹¨ì–´")
    print(f"  ìµœëŒ€ ì°¸ì¡° ê¸¸ì´:     {metrics['max_ref_length']:.0f} ë‹¨ì–´")
    print(f"  ê¸´ í…ìŠ¤íŠ¸ ë¹„ìœ¨:     ì˜ˆì¸¡ {metrics['long_preds_ratio']:.1%}, ì°¸ì¡° {metrics['long_refs_ratio']:.1%}")
    print(f"  ê¸¸ì´ ë¹„ìœ¨:         {metrics['length_ratio']:.2f}")
    print(f"  ë¹ˆ ì˜ˆì¸¡ ë¹„ìœ¨:      {metrics['empty_predictions']:.2%}")
    print(f"  ì´ í‰ê°€ ìƒ˜í”Œ:      {len(valid)} / {len(df)}")
    
    # í…ìŠ¤íŠ¸ ì²˜ë¦¬ í’ˆì§ˆ ì•ˆë‚´
    if metrics['long_refs_ratio'] > 0.1 or metrics['long_preds_ratio'] > 0.1:
        print(f"\nğŸ’¡ ê¸´ í…ìŠ¤íŠ¸ ê°ì§€ë¨ (50ë‹¨ì–´ ì´ìƒ): ì „ì²´ í…ìŠ¤íŠ¸ê°€ í‰ê°€ì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("="*60)


if __name__ == '__main__':
    main()