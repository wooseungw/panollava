# coding: utf-8
"""
PanoLLaVA Comprehensive Model Evaluation System
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ë‹¨ê³„ë³„ í‰ê°€ ì‹œìŠ¤í…œ:
1. ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ (ChatPanoTestDataset, VLMDataModule)
3. ë°°ì¹˜ë³„ í…ìŠ¤íŠ¸ ìƒì„± (generate)
4. ì˜ˆì¸¡/ì •ë‹µ í…ìŠ¤íŠ¸ ì €ì¥ ë° ë¡œê¹…
5. í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (BLEU, ROUGE, METEOR, SPICE, CIDEr, CLIP-S, RefCLIP-S)

ì‚¬ìš©ë²•:
    python eval.py --ckpt runs/e2p_finetune_mlp/best.ckpt --lora-weights-path runs/e2p_finetune_mlp/lora_weights --csv-input data/quic360/test.csv
"""

import argparse
import torch
import json
import logging
import time
import traceback
import os
# Avoid tokenizers fork/parallelism warnings and potential deadlocks
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
from pathlib import Path
from tqdm import tqdm
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple

# ë‚´ë¶€ ëª¨ë“ˆ
from train import VLMModule, VLMDataModule, safe_load_checkpoint
from panovlm.processors.universal_text_formatter import UniversalTextFormatter

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)




def load_model_and_lora(checkpoint_path: str, lora_weights_path: Optional[str], device: torch.device, **model_kwargs):
    """
    1ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ì™€ LoRA ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ì—¬ ìƒì„±ìš© ëª¨ë¸ ì¤€ë¹„ (ê°œì„ ëœ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©)
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ 1ë‹¨ê³„: ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ (ê°œì„ ëœ ì¸í„°í˜ì´ìŠ¤)")
    logger.info("=" * 60)
    
    # ìƒˆë¡œìš´ í†µí•© ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
    from panovlm.model import PanoramaVLM
    
    # ë””ë°”ì´ìŠ¤ ë¬¸ìì—´ë¡œ ë³€í™˜
    device_str = str(device) if device != "auto" else "auto"
    
    try:
        # í•œ ì¤„ë¡œ ëª¨ë¸ ë¡œë”© (LoRA ìë™ ê°ì§€ í¬í•¨)
        model = PanoramaVLM.from_checkpoint(
            checkpoint_path,
            lora_weights_path=lora_weights_path,
            device=device_str,
            **model_kwargs
        )
        
        # í˜¸í™˜ì„±ì„ ìœ„í•´ wrapper í´ë˜ìŠ¤ ìƒì„±
        class ModelWrapper:
            def __init__(self, panorama_model):
                self.model = panorama_model  # ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„± ìœ ì§€
                self._stage_key = "finetune"
            
            def eval(self):
                self.model.eval()
                return self
            
            def to(self, device):
                self.model = self.model.to(device)
                return self
        
        wrapped_model = ModelWrapper(model)
        wrapped_model.eval()
        
        logger.info(f"âœ“ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ - Device: {device}")
        return wrapped_model
        
    except Exception as e:
        logger.error(f"âŒ ìƒˆë¡œìš´ ì¸í„°í˜ì´ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
        logger.info("ğŸ”„ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±...")
        
        # ê¸°ì¡´ ë°©ì‹ í´ë°± (í˜¸í™˜ì„± ë³´ì¥)
        from train import VLMModule, safe_load_checkpoint
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        logger.info(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
        checkpoint = safe_load_checkpoint(checkpoint_path)
        if not checkpoint:
            raise ValueError(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {checkpoint_path}")
        
        # LoRA ê²½ë¡œ ìë™ ê°ì§€
        if lora_weights_path is None:
            checkpoint_dir = Path(checkpoint_path).parent
            potential_lora_path = checkpoint_dir / "lora_weights"
            if potential_lora_path.exists():
                lora_weights_path = str(potential_lora_path)
                logger.info(f"ğŸ” LoRA ê°€ì¤‘ì¹˜ ìë™ ê°ì§€: {lora_weights_path}")
        
        # ëª¨ë¸ ë¡œë“œ (finetune ë‹¨ê³„)
        model = VLMModule.load_from_checkpoint(
            checkpoint_path,
            stage="finetune",
            map_location=device,
            strict=False,
            **model_kwargs
        )
        
        # LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
        if lora_weights_path and Path(lora_weights_path).exists():
            logger.info(f"ğŸ”§ LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ: {lora_weights_path}")
            
            # LoRA íŒŒì¼ êµ¬ì¡° ê²€ì¦
            lora_path = Path(lora_weights_path)
            adapter_config = lora_path / "adapter_config.json"
            adapter_model = lora_path / "adapter_model.safetensors"
            
            if adapter_config.exists() and adapter_model.exists():
                success = model.model.load_lora_weights(lora_weights_path)
                if success:
                    logger.info("âœ… LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ!")
                    
                    # LoRA ì„¤ì • ì •ë³´ ì¶œë ¥
                    lora_info = model.model.get_lora_info()
                    if lora_info.get("is_lora_enabled", False):
                        logger.info(f"ğŸ“Š LoRA ì„¤ì • - Rank: {lora_info.get('lora_r')}, Alpha: {lora_info.get('lora_alpha')}")
                        logger.info(f"   Target modules: {lora_info.get('target_modules')}")
                else:
                    logger.warning("âš ï¸ LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ë¡œ ì§„í–‰")
            else:
                logger.warning(f"âš ï¸ LoRA íŒŒì¼ ëˆ„ë½: {lora_weights_path}")
        else:
            logger.info("ğŸ“ LoRA ê°€ì¤‘ì¹˜ ì—†ìŒ, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
        
        # í‰ê°€ ëª¨ë“œ ì„¤ì •
        model.eval()
        model = model.to(device)
        model.model.requires_grad_(False)
        
        logger.info(f"âœ“ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ - Device: {device}, Stage: {model._stage_key}")
        return model


def prepare_test_dataset(csv_input: str, batch_size: int, max_text_length: int, crop_strategy: str, lm_name: str, num_workers: int = 0, overlap_ratio: float = 0.5) -> Tuple[VLMDataModule, Any]:
    """
    2ë‹¨ê³„: ChatPanoTestDatasetê³¼ VLMDataModuleì„ í™œìš©í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    """
    logger.info("=" * 60)
    logger.info("ğŸ“Š 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„")
    logger.info("=" * 60)
    
    # ë°ì´í„° ëª¨ë“ˆ ì´ˆê¸°í™”
    logger.info(f"ğŸ“‚ CSV ì…ë ¥: {csv_input}")
    # config.shì˜ FINETUNE_SYSTEM_MSGì™€ ë™ì¼í•œ system ë©”ì‹œì§€ ì‚¬ìš©
    system_msg = "You are an expert assistant specialized in analyzing panoramic images. Please provide detailed, accurate, and helpful responses about what you observe in the panoramic view shortly."
    
    datamodule = VLMDataModule(
        csv_train=csv_input,
        csv_val=csv_input,  # í‰ê°€ìš©ìœ¼ë¡œ ë™ì¼í•œ íŒŒì¼ ì‚¬ìš©
        batch_size=batch_size,
        num_workers=num_workers,
        tokenizer_name=lm_name,
        max_text_length=max_text_length,
        crop_strategy=crop_strategy,
        eval_mode=True,  # í‰ê°€ ëª¨ë“œ í™œì„±í™”
        system_msg=system_msg,  # system ë©”ì‹œì§€ ì¶”ê°€
        overlap_ratio=overlap_ratio
    )
    
    # ë°ì´í„°ì…‹ ì„¤ì •
    datamodule.setup()
    test_dataloader = datamodule.val_dataloader()
    
    logger.info(f"âœ“ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
    logger.info(f"   - ì´ ë°°ì¹˜ ìˆ˜: {len(test_dataloader)}")
    logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    logger.info(f"   - í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´: {max_text_length}")
    logger.info(f"   - í¬ë¡­ ì „ëµ: {crop_strategy}")
    logger.info(f"   - ì›Œì»¤ ìˆ˜: {num_workers}")
    
    return datamodule, test_dataloader


def generate_predictions(model: VLMModule, test_dataloader, datamodule: VLMDataModule, device: torch.device,
                        max_new_tokens: int = 128, temperature: float = 0.7,
                        top_p: float = 0.9, top_k: int = 50,
                        repetition_penalty: float = 1.1, length_penalty: float = 1.0,
                        min_new_tokens: int = 5) -> Tuple[List[str], List[str], List[str], List[str]]:
    """
    3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë°°ì¹˜ë³„ í…ìŠ¤íŠ¸ ìƒì„± (ê°œì„ ëœ UniversalTextFormatter ì‚¬ìš©)
    """
    logger.info("=" * 60)
    logger.info("ğŸ¤– 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ìƒì„± (UniversalTextFormatter í™œìš©)")
    logger.info("=" * 60)
    
    predictions = []
    references = []
    image_paths = []
    input_texts = []
    
    # UniversalTextFormatter ì´ˆê¸°í™” (ë°ì´í„°ëª¨ë“ˆì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©)
    tokenizer = datamodule.tokenizer
    tokenizer_name = getattr(tokenizer, 'name_or_path', 'Qwen/Qwen2.5-0.5B')  # ê¸°ë³¸ê°’
    text_formatter = UniversalTextFormatter(
        tokenizer_name_or_path=tokenizer_name,
        system_msg="You are an expert assistant specialized in analyzing panoramic images. Please provide detailed, accurate, and helpful responses about what you observe in the panoramic view shortly."
    )
    
    logger.info(f"ğŸ¯ ìƒì„± íŒŒë¼ë¯¸í„° - Max tokens: {max_new_tokens}, Min tokens: {min_new_tokens}, Temperature: {temperature}")
    logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ í¬ë§·í„° - ëª¨ë¸: {text_formatter.model_family} ({'Instruct' if text_formatter.is_instruct else 'Base'})")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(test_dataloader, desc="ìƒì„± ì¤‘")):
            try:
                # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
                pixel_values = batch["pixel_values"].to(device)
                input_ids = batch.get("input_ids")
                if input_ids is not None:
                    input_ids = input_ids.to(device)
                attention_mask = batch.get("attention_mask")
                if attention_mask is not None:
                    attention_mask = attention_mask.to(device)
                
                batch_size = pixel_values.shape[0]
                
                # VLM ëª¨ë¸ì„ ìœ„í•œ ë””ë²„ê¹… ì •ë³´
                if batch_idx == 0:
                    logger.info(f"=== VLM ì…ë ¥ ë””ë²„ê¹… (ë°°ì¹˜ {batch_idx}) ===")
                    logger.info(f"pixel_values shape: {pixel_values.shape}")
                    logger.info(f"input_ids shape: {input_ids.shape if input_ids is not None else 'None'}")
                    if input_ids is not None:
                        logger.info(f"input_ids sample: {input_ids[0][:20]}")  # ì²˜ìŒ 20ê°œ í† í°ë§Œ
                    logger.info("=" * 45)
                
                # ê°„ì†Œí™”ëœ ì •ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                batch_references = []
                if "reference" in batch:
                    refs = batch["reference"]
                    if isinstance(refs, list):
                        batch_references = [str(ref).strip() for ref in refs]
                    else:
                        batch_references = [str(refs).strip()] * batch_size
                else:
                    batch_references = [f"no_reference_{i}" for i in range(batch_size)]
                
                # ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ
                batch_image_paths = batch.get("image_path", [f"batch_{batch_idx}_sample_{i}" for i in range(batch_size)])
                
                # original_query ì¶”ì¶œ (ì›ë˜ ì‚¬ìš©ì ì§ˆë¬¸)
                batch_input_texts = batch.get("original_query", batch.get("input_text", [f"no_query_{i}" for i in range(batch_size)]))
                if not isinstance(batch_input_texts, list):
                    batch_input_texts = [batch_input_texts] * batch_size
                
                # ê°œì„ ëœ VLM ìƒì„± (UniversalTextFormatter í™œìš©)
                try:
                    if batch_idx == 0:
                        logger.info(f"=== ê°œì„ ëœ ìƒì„± í”„ë¡œì„¸ìŠ¤ ===")
                        sample_input_text = batch_input_texts[0] if batch_input_texts else ""
                        logger.info(f"Input text preview: {sample_input_text[:150]}...")
                        logger.info(f"Formatter config: {text_formatter.format_config['assistant_start'][:50]}...")
                        logger.info("=" * 40)
                    
                    # ê°œì„ ëœ ìƒì„± íŒŒë¼ë¯¸í„° (UniversalTextFormatter ì •ì§€ í† í° í™œìš©)
                    generation_config = text_formatter.get_generation_config()
                    
                    gen_kwargs = {
                        "pixel_values": pixel_values,
                        "input_ids": input_ids,
                        "attention_mask": attention_mask,
                        "max_new_tokens": max_new_tokens,
                        "temperature": temperature,
                        "top_p": top_p,
                        "top_k": top_k,
                        "repetition_penalty": repetition_penalty,
                        "length_penalty": length_penalty,
                        "min_new_tokens": min_new_tokens,
                        "do_sample": True,
                        "pad_token_id": tokenizer.pad_token_id,
                        "eos_token_id": tokenizer.eos_token_id,
                    }
                    
                    # ì •ì§€ ë¬¸ìì—´ ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
                    if hasattr(model.model, 'generation_config'):
                        if hasattr(model.model.generation_config, 'stop_strings'):
                            gen_kwargs["stop_strings"] = generation_config["stop_strings"][:3]  # ìµœëŒ€ 3ê°œ
                    
                    # ìƒì„± ì‹¤í–‰ (ìƒˆ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜)
                    if hasattr(model, 'model') and hasattr(model.model, 'generate'):
                        # ê¸°ì¡´ VLMModule ë˜í¼ì¸ ê²½ìš°
                        output = model.model.generate(**gen_kwargs)
                    elif hasattr(model, 'generate'):
                        # ìƒˆë¡œìš´ PanoramaVLM ì¸í„°í˜ì´ìŠ¤ì¸ ê²½ìš°
                        output = model.generate(**gen_kwargs)
                    else:
                        raise AttributeError("ëª¨ë¸ì— generate ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                    
                    # ê°œì„ ëœ ê²°ê³¼ ì²˜ë¦¬ (UniversalTextFormatter ì‚¬ìš©)
                    batch_predictions = []
                    
                    if isinstance(output, torch.Tensor):
                        # í† í° ID ì¶œë ¥ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        for i in range(batch_size):
                            # ìƒì„±ëœ í† í° ì¶”ì¶œ (ì…ë ¥ ê¸¸ì´ ì´í›„ ë¶€ë¶„)
                            input_length = input_ids[i].shape[0] if input_ids is not None else 0
                            generated_tokens = output[i][input_length:]
                            
                            # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
                            raw_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                            
                            # UniversalTextFormatterë¡œ Assistant ì‘ë‹µ ì¶”ì¶œ
                            clean_prediction = text_formatter.extract_assistant_response(raw_text)
                            batch_predictions.append(clean_prediction)
                            
                            # ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œ ë””ë²„ê¹…
                            if batch_idx == 0 and i == 0:
                                logger.info(f"=== í…ìŠ¤íŠ¸ ì¶”ì¶œ ê³¼ì • ===")
                                logger.info(f"Raw generated: '{raw_text[:200]}...'")
                                logger.info(f"Clean prediction: '{clean_prediction}'")
                                logger.info("=" * 30)
                    
                    elif isinstance(output, dict) and "text" in output:
                        # ì´ë¯¸ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜ëœ ê²½ìš°
                        raw_texts = output["text"]
                        for raw_text in raw_texts:
                            clean_prediction = text_formatter.extract_assistant_response(raw_text)
                            batch_predictions.append(clean_prediction)
                    
                    else:
                        logger.warning(f"Unexpected output format: {type(output)}")
                        batch_predictions = ["[ìƒì„± ì‹¤íŒ¨]"] * batch_size
                        
                except Exception as gen_error:
                    logger.error(f"VLM ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {gen_error}")
                    logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: ", exc_info=True)
                    batch_predictions = [f"[ìƒì„± ì˜¤ë¥˜_{i}]" for i in range(batch_size)]
                
                # ë°°ì¹˜ í¬ê¸° ê²€ì¦ ë° ì¡°ì •
                if len(batch_predictions) != batch_size:
                    logger.warning(f"ë°°ì¹˜ í¬ê¸° ë¶ˆì¼ì¹˜: ì˜ˆìƒ {batch_size}, ì‹¤ì œ {len(batch_predictions)}")
                    # í¬ê¸° ì¡°ì •
                    if len(batch_predictions) < batch_size:
                        batch_predictions.extend(["[í¬ê¸° ë¶€ì¡±]"] * (batch_size - len(batch_predictions)))
                    else:
                        batch_predictions = batch_predictions[:batch_size]
                
                # ì˜ˆì¸¡ê°’ í’ˆì§ˆ ê²€ì¦ ë° ì •ë¦¬
                cleaned_predictions = []
                for pred in batch_predictions:
                    # ë¹ˆ ì˜ˆì¸¡ê°’ ì²˜ë¦¬
                    if not pred or pred.strip() == "":
                        cleaned_predictions.append("[ë¹ˆ ì‘ë‹µ]")
                    else:
                        # ê¸°ë³¸ ì •ë¦¬: ì•ë’¤ ê³µë°± ì œê±°, ê°œí–‰ ì •ë¦¬
                        cleaned_pred = pred.strip().replace('\n\n', '\n')
                        cleaned_predictions.append(cleaned_pred)
                
                # ë°°ì¹˜ë³„ predictionê³¼ reference ë¡œê·¸ ì¶œë ¥ (ê°œì„ ëœ í¬ë§·)
                logger.info(f"=== ë°°ì¹˜ {batch_idx} ê²°ê³¼ ë¡œê·¸ ===")
                for i, (pred, ref) in enumerate(zip(cleaned_predictions, batch_references)):
                    logger.info(f"  ìƒ˜í”Œ {len(predictions) + i}")
                    logger.info(f"    ì˜ˆì¸¡: '{pred}'")
                    logger.info(f"    ì •ë‹µ: '{ref}'")
                logger.info(f"==========================")
                
                # ê²°ê³¼ ì €ì¥
                predictions.extend(cleaned_predictions)
                references.extend(batch_references)
                image_paths.extend(batch_image_paths)
                input_texts.extend(batch_input_texts)
                
                # ì§„í–‰ ìƒí™© ë¡œê¹…
                if batch_idx % 10 == 0:
                    logger.info(f"ì§„í–‰: {batch_idx + 1}/{len(test_dataloader)} ë°°ì¹˜ ì™„ë£Œ ({len(predictions)} ìƒ˜í”Œ)")
                
            except Exception as e:
                logger.error(f"ë°°ì¹˜ {batch_idx} ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: ", exc_info=True)
                # ë¹ˆ ê²°ê³¼ë¡œ ëŒ€ì²´
                batch_size = pixel_values.shape[0] if 'pixel_values' in locals() else 1
                predictions.extend([f"[ë°°ì¹˜ ì˜¤ë¥˜_{i}]" for i in range(batch_size)])
                references.extend(batch_references if 'batch_references' in locals() else [f"[ì •ë‹µ ì—†ìŒ_{i}]" for i in range(batch_size)])
                image_paths.extend(batch_image_paths if 'batch_image_paths' in locals() else [f"error_batch_{batch_idx}_sample_{i}" for i in range(batch_size)])
                input_texts.extend(batch_input_texts if 'batch_input_texts' in locals() else [f"error_input_{i}" for i in range(batch_size)])
                continue
    
    logger.info(f"âœ“ í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ!")
    logger.info(f"  ì´ ìƒ˜í”Œ ìˆ˜: {len(predictions)}")
    logger.info(f"  ì„±ê³µì  ì˜ˆì¸¡: {len([p for p in predictions if not p.startswith('[')])} ({len([p for p in predictions if not p.startswith('[')]) / len(predictions) * 100:.1f}%)")
    
    return predictions, references, image_paths, input_texts


def save_and_log_results(predictions: List[str], references: List[str], image_paths: List[str], input_texts: List[str], output_dir: Path, timestamp: str) -> pd.DataFrame:
    """
    4ë‹¨ê³„: ìƒì„±ëœ ë‹µë³€ê³¼ ì •ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ê³  ë¡œê¹… (ê°œì„ ëœ ë¶„ì„ í¬í•¨)
    """
    logger.info("=" * 60)
    logger.info("ğŸ’¾ 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ë¶„ì„")
    logger.info("=" * 60)
    
    # ê°œì„ ëœ CSV ë°ì´í„° ì¤€ë¹„
    results_data = []
    for i, (pred, ref, img_path) in enumerate(zip(predictions, references, image_paths)):
        # ë¹ˆ ê°’ ì²˜ë¦¬ ë° ê¸°ë³¸ ì •ë¦¬
        pred_str = str(pred).strip() if pred is not None else ""
        ref_str = str(ref).strip() if ref is not None else ""
        img_path_str = str(img_path) if img_path is not None else ""
        
        # ì˜ˆì¸¡ê°’ í’ˆì§ˆ ë¶„ì„
        is_error = pred_str.startswith('[') and pred_str.endswith(']')
        is_empty = not pred_str or pred_str in ["", "[ë¹ˆ ì‘ë‹µ]"]
        
        # input_text ì¶”ì¶œ (ì¸ë±ìŠ¤ í™•ì¸ í›„ ì•ˆì „í•˜ê²Œ)
        input_text_str = ""
        if i < len(input_texts):
            input_text_str = str(input_texts[i]).strip() if input_texts[i] is not None else ""
        
        results_data.append({
            'sample_id': i,
            'image_path': img_path_str,
            'original_query': input_text_str,
            'prediction': pred_str,
            'reference': ref_str,
            'pred_length': len(pred_str.split()),
            'ref_length': len(ref_str.split()),
            'is_error': is_error,
            'is_empty': is_empty
        })
    
    # DataFrame ìƒì„± ë° ì €ì¥
    df = pd.DataFrame(results_data)
    csv_path = output_dir / f"predictions_{timestamp}.csv"
    df.to_csv(csv_path, index=False, encoding='utf-8')
    
    # ê°œì„ ëœ ê²°ê³¼ í†µê³„ ë¶„ì„
    total_samples = len(df)
    error_count = df['is_error'].sum()
    empty_count = df['is_empty'].sum()
    valid_count = total_samples - error_count - empty_count
    
    # ê¸¸ì´ í†µê³„ (ìœ íš¨í•œ ì˜ˆì¸¡ê°’ë§Œ)
    valid_df = df[~df['is_error'] & ~df['is_empty']]
    if len(valid_df) > 0:
        avg_pred_length = valid_df['pred_length'].mean()
        avg_ref_length = valid_df['ref_length'].mean()
        pred_length_std = valid_df['pred_length'].std()
    else:
        avg_pred_length = avg_ref_length = pred_length_std = 0.0
    
    logger.info(f"ğŸ“Š ìƒì„± í’ˆì§ˆ ë¶„ì„:")
    logger.info(f"   - ì´ ìƒ˜í”Œ: {total_samples}")
    logger.info(f"   - ì„±ê³µì  ìƒì„±: {valid_count}ê°œ ({valid_count/total_samples*100:.1f}%)")
    logger.info(f"   - ìƒì„± ì˜¤ë¥˜: {error_count}ê°œ ({error_count/total_samples*100:.1f}%)")
    logger.info(f"   - ë¹ˆ ì‘ë‹µ: {empty_count}ê°œ ({empty_count/total_samples*100:.1f}%)")
    
    if valid_count > 0:
        logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„:")
        logger.info(f"   - í‰ê·  ì˜ˆì¸¡ ê¸¸ì´: {avg_pred_length:.1f} Â± {pred_length_std:.1f} ë‹¨ì–´")
        logger.info(f"   - í‰ê·  ì •ë‹µ ê¸¸ì´: {avg_ref_length:.1f} ë‹¨ì–´")
        logger.info(f"   - ê¸¸ì´ ë¹„ìœ¨ (ì˜ˆì¸¡/ì •ë‹µ): {avg_pred_length/avg_ref_length:.2f}")
    
    logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
    return df
    


def calculate_evaluation_metrics(data_input, output_dir: Path, timestamp: str) -> Dict[str, float]:
    """
    5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (BLEU-4, METEOR, ROUGE-L, SPICE, CIDEr, CLIP-S, RefCLIP-S)
    
    Args:
        data_input: pandas DataFrame ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œ (str/Path)
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´
    """
    logger.info("=" * 60)
    logger.info("ğŸ“ˆ 5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°")
    logger.info("=" * 60)
    
    # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬: CSV íŒŒì¼ì´ë©´ DataFrameìœ¼ë¡œ ë³€í™˜
    if isinstance(data_input, (str, Path)):
        csv_path = Path(data_input)
        if not csv_path.exists():
            raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        
        logger.info(f"ğŸ“‚ CSV íŒŒì¼ ë¡œë“œ: {csv_path}")
        df = pd.read_csv(csv_path, encoding='utf-8')
        logger.info(f"âœ“ DataFrame ë³€í™˜ ì™„ë£Œ - ì´ {len(df)}ê°œ ìƒ˜í”Œ")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['prediction', 'reference']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"CSV íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}. í•„ìš”í•œ ì»¬ëŸ¼: {required_columns}")
        
        # ì˜µì…˜ ì»¬ëŸ¼ í™•ì¸ ë° ë¡œê·¸
        optional_columns = ['image_path']
        available_optional = [col for col in optional_columns if col in df.columns]
        logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: í•„ìˆ˜ {required_columns} + ì„ íƒ {available_optional}")
    
    elif isinstance(data_input, pd.DataFrame):
        df = data_input
        logger.info(f"âœ“ DataFrame ì…ë ¥ - ì´ {len(df)}ê°œ ìƒ˜í”Œ")
    else:
        raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {type(data_input)}. pandas DataFrame ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    # ìœ íš¨í•œ ìƒ˜í”Œë§Œ ì„ íƒ (ì˜ˆì¸¡ê³¼ ì •ë‹µê°€ ëª¨ë‘ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°)
    valid_df = df[(df['prediction'].str.strip() != '') & (df['reference'].str.strip() != '')]
    
    if len(valid_df) == 0:
        logger.error("âŒ ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    logger.info(f"ğŸ“Š í‰ê°€ ëŒ€ìƒ: {len(valid_df)}/{len(df)} ìƒ˜í”Œ")
    
    # ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (NaN ê°’ ì²˜ë¦¬)
    predictions = [str(pred) if pred is not None and not pd.isna(pred) else "" for pred in valid_df['prediction'].tolist()]
    references = [str(ref) if ref is not None and not pd.isna(ref) else "" for ref in valid_df['reference'].tolist()]
    
    # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
    valid_pairs = [(pred, ref) for pred, ref in zip(predictions, references) if pred.strip() and ref.strip()]
    
    if not valid_pairs:
        logger.error("âŒ ìœ íš¨í•œ ì˜ˆì¸¡-ì •ë‹µ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    predictions, references = zip(*valid_pairs)
    predictions = list(predictions)
    references = list(references)
    
    logger.info(f"ğŸ“Š ìµœì¢… í‰ê°€ ëŒ€ìƒ: {len(valid_pairs)} ìƒ˜í”Œ")
    
    metrics = {}
    
    # Assistant ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì •ë‹µìš©) - NaN ì²˜ë¦¬ ì¶”ê°€
    ref_texts_for_bleu = []
    for ref in references:
        if "Assistant:" in ref:
            assistant_part = ref.split("Assistant:")[-1].strip()
            ref_texts_for_bleu.append(assistant_part)
        else:
            ref_texts_for_bleu.append(ref)
    
    # 1. BLEU-4 ê³„ì‚°
    try:
        from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
        
        ref_tokens = [[ref.split()] for ref in ref_texts_for_bleu if ref.strip()]
        pred_tokens = [pred.split() for pred in predictions if pred.strip()]
        
        
        if len(ref_tokens) == 0 or len(pred_tokens) == 0:
            logger.warning("âš ï¸ BLEU-4: ìœ íš¨í•œ í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
            metrics['bleu4'] = 0.0
        else:
            smoothing = SmoothingFunction().method1
            metrics['bleu4'] = corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)
            logger.info(f"âœ“ BLEU-4: {metrics['bleu4']:.4f}")
    except Exception as e:
        logger.error(f"âŒ BLEU-4 ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['bleu4'] = 0.0
    
    # 2. METEOR ê³„ì‚°
    try:
        import nltk
        try:
            nltk.data.find('corpora/wordnet')
        except LookupError:
            logger.info("NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
            nltk.download('wordnet', quiet=True)
            nltk.download('punkt', quiet=True)
        
        from nltk.translate.meteor_score import meteor_score
        
        meteor_scores = []
        for ref, pred in zip(ref_texts_for_bleu, predictions):
            if ref.strip() and pred.strip():  # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                ref_tokens = ref.split()
                pred_tokens = pred.split()
                if len(ref_tokens) > 0 and len(pred_tokens) > 0:
                    score = meteor_score([ref_tokens], pred_tokens)
                    meteor_scores.append(score)
        
        if meteor_scores:
            metrics['meteor'] = float(np.mean(meteor_scores))
            logger.info(f"âœ“ METEOR: {metrics['meteor']:.4f}")
        else:
            logger.warning("âš ï¸ METEOR: ìœ íš¨í•œ ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['meteor'] = 0.0
    except Exception as e:
        logger.error(f"âŒ METEOR ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['meteor'] = 0.0
    
    # 3. ROUGE-L ê³„ì‚°
    try:
        from rouge_score import rouge_scorer
        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        
        rouge_scores = []
        for ref, pred in zip(ref_texts_for_bleu, predictions):
            if ref.strip() and pred.strip():  # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                scores = scorer.score(ref, pred)
                rouge_scores.append(scores['rougeL'].fmeasure)
        
        if rouge_scores:
            metrics['rougeL'] = float(np.mean(rouge_scores))
            logger.info(f"âœ“ ROUGE-L: {metrics['rougeL']:.4f}")
        else:
            logger.warning("âš ï¸ ROUGE-L: ìœ íš¨í•œ ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['rougeL'] = 0.0
        logger.info(f"âœ“ ROUGE-L: {metrics['rougeL']:.4f}")
    except Exception as e:
        logger.error(f"âŒ ROUGE-L ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['rougeL'] = 0.0
    
    # 4. SPICE ê³„ì‚° (ë” ì•ˆì „í•œ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬)
    try:
        from pycocoevalcap.spice.spice import Spice
        
        # SPICE ê³„ì‚°ì„ ë” ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        spice_scorer = Spice()
        
        # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
        valid_refs_for_spice = [ref for ref in ref_texts_for_bleu if ref.strip()]
        valid_preds_for_spice = [pred for pred in predictions if pred.strip()]
        
        if len(valid_refs_for_spice) == 0 or len(valid_preds_for_spice) == 0:
            logger.warning("âš ï¸ SPICE: ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['spice'] = 0.0
        else:
            gts = {str(i): [ref] for i, ref in enumerate(valid_refs_for_spice)}
            res = {str(i): [pred] for i, pred in enumerate(valid_preds_for_spice)}
            
            # ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ì´ìš©í•œ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ (ë” ì•ˆì „í•¨)
            import multiprocessing
            import queue
            
            def spice_calculate(gts, res, result_queue):
                try:
                    spice_score, _ = spice_scorer.compute_score(gts, res)
                    result_queue.put(('success', spice_score))
                except Exception as e:
                    result_queue.put(('error', str(e)))
            
            # í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•œ íƒ€ì„ì•„ì›ƒ
            result_queue = multiprocessing.Queue()
            process = multiprocessing.Process(target=spice_calculate, args=(gts, res, result_queue))
            process.start()
            process.join(timeout=60)  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
            
            if process.is_alive():
                process.terminate()
                process.join()
                raise TimeoutError("SPICE calculation timeout (60s)")
            
            # ê²°ê³¼ í™•ì¸
            try:
                result_type, result_value = result_queue.get_nowait()
                if result_type == 'success':
                    metrics['spice'] = float(result_value)
                    logger.info(f"âœ“ SPICE: {metrics['spice']:.4f}")
                else:
                    raise Exception(f"SPICE calculation failed: {result_value}")
            except queue.Empty:
                raise Exception("SPICE calculation returned no result")
            
    except (Exception, TimeoutError) as e:
        logger.warning(f"âš ï¸ SPICE ê³„ì‚° ì˜¤ë¥˜: {e}")
        # SPICE ëŒ€ì•ˆ: ê°„ë‹¨í•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
        try:
            logger.info("SPICE ëŒ€ì•ˆìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì‹œë„...")
            from sentence_transformers import SentenceTransformer
            model_st = SentenceTransformer('all-MiniLM-L6-v2')
            
            pred_embeddings = model_st.encode(predictions)
            ref_embeddings = model_st.encode(ref_texts_for_bleu)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = []
            for pred_emb, ref_emb in zip(pred_embeddings, ref_embeddings):
                sim = cosine_similarity([pred_emb], [ref_emb])[0][0]
                similarities.append(sim)
            
            metrics['spice'] = float(np.mean(similarities))
            logger.info(f"âœ“ SPICE (ëŒ€ì•ˆ-ì˜ë¯¸ìœ ì‚¬ë„): {metrics['spice']:.4f}")
        except Exception as fallback_e:
            logger.warning(f"âš ï¸ SPICE ëŒ€ì•ˆ ê³„ì‚°ë„ ì‹¤íŒ¨: {fallback_e}")
            metrics['spice'] = 0.0
    
    # 5. CIDEr ê³„ì‚°
    try:
        from pycocoevalcap.cider.cider import Cider
        cider_scorer = Cider()
        
        # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
        valid_refs_for_cider = [ref for ref in ref_texts_for_bleu if ref.strip()]
        valid_preds_for_cider = [pred for pred in predictions if pred.strip()]
        
        if len(valid_refs_for_cider) == 0 or len(valid_preds_for_cider) == 0:
            logger.warning("âš ï¸ CIDEr: ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['cider'] = 0.0
        else:
            gts = {str(i): [ref] for i, ref in enumerate(valid_refs_for_cider)}
            res = {str(i): [pred] for i, pred in enumerate(valid_preds_for_cider)}
            
            cider_score, _ = cider_scorer.compute_score(gts, res)
            metrics['cider'] = float(cider_score)
            logger.info(f"âœ“ CIDEr: {metrics['cider']:.4f}")
    except Exception as e:
        logger.warning(f"âš ï¸ CIDEr ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['cider'] = 0.0
    
    # 6. CLIP-S ë° RefCLIP-S ê³„ì‚° (ë” ì•ˆì „í•œ importì™€ ì˜¤ë¥˜ ì²˜ë¦¬)
    try:
        # ì—¬ëŸ¬ CLIP êµ¬í˜„ ì‹œë„ - ë” ì•ˆì „í•œ import ë°©ì‹
        clip_model = None
        preprocess = None
        device_clip = "cuda" if torch.cuda.is_available() else "cpu"
        
        try:
            # ì²« ë²ˆì§¸: openai-clip ì‹œë„ (ë” ëª…ì‹œì ì¸ import)
            import sys
            import importlib
            
            # ê¸°ì¡´ clip ëª¨ë“ˆì´ ìˆë‹¤ë©´ ì œê±° (ì¶©ëŒ ë°©ì§€)
            if 'clip' in sys.modules:
                del sys.modules['clip']
            
            # ìƒˆë¡œ import
            import clip as openai_clip
            
            # clip.load í•¨ìˆ˜ ì¡´ì¬ í™•ì¸
            if hasattr(openai_clip, 'load'):
                clip_model, preprocess = openai_clip.load("ViT-B/32", device=device_clip)
                logger.info("âœ“ OpenAI CLIP ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                clip_model = clip_model.to(device_clip)
                clip_tokenize = openai_clip.tokenize  # tokenize í•¨ìˆ˜ ì €ì¥
            else:
                raise AttributeError("clip.load í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e1:
            logger.warning(f"OpenAI CLIP ë¡œë“œ ì‹¤íŒ¨: {e1}")
            try:
                # ë‘ ë²ˆì§¸: transformers CLIP ì‹œë„
                from transformers import CLIPModel, CLIPProcessor
                clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
                preprocess = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                clip_model = clip_model.to(device_clip)
                logger.info("âœ“ HuggingFace CLIP ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                clip_tokenize = None  # HuggingFaceëŠ” tokenize í•¨ìˆ˜ ë¶ˆí•„ìš”
            except Exception as e2:
                logger.warning(f"HuggingFace CLIP ë¡œë“œë„ ì‹¤íŒ¨: {e2}")
                raise Exception(f"ëª¨ë“  CLIP êµ¬í˜„ ë¡œë“œ ì‹¤íŒ¨. OpenAI: {e1}, HuggingFace: {e2}")
        
        if clip_model is not None:
            from PIL import Image
            
            clip_s_scores = []
            ref_clip_s_scores = []
            
            # ìœ íš¨í•œ ì´ë¯¸ì§€ê°€ ìˆëŠ” ìƒ˜í”Œë§Œ ì²˜ë¦¬
            if 'image_path' in valid_df.columns:
                valid_image_samples = valid_df[valid_df['image_path'].apply(lambda x: os.path.exists(str(x)) if pd.notna(x) else False)]
            else:
                valid_image_samples = pd.DataFrame()  # ë¹ˆ DataFrame
            
            if len(valid_image_samples) > 0:
                logger.info(f"CLIP ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘... ({len(valid_image_samples)}ê°œ ì´ë¯¸ì§€)")
                
                for idx, (_, sample) in enumerate(valid_image_samples.iterrows()):
                    try:
                        # ì´ë¯¸ì§€ ë¡œë“œ
                        image_path = sample['image_path']
                        if not os.path.exists(image_path):
                            continue
                            
                        image_pil = Image.open(image_path).convert("RGB")
                        
                        # í…ìŠ¤íŠ¸ ì¤€ë¹„ (ê¸¸ì´ ì œí•œ)
                        pred_text = str(sample['prediction'])[:200]  
                        ref_text = str(sample['reference'])[:200] if "Assistant:" not in str(sample['reference']) else str(sample['reference']).split("Assistant:")[-1].strip()[:200]
                        
                        if not pred_text.strip() or not ref_text.strip():
                            continue
                        
                        # OpenAI CLIP ì‚¬ìš© (ë” ì•ˆì „í•œ ë°©ì‹)
                        if hasattr(clip_model, 'encode_image') and clip_tokenize is not None:
                            try:
                                image = preprocess(image_pil).unsqueeze(0).to(device_clip)
                                pred_tokens = clip_tokenize([pred_text], truncate=True).to(device_clip)
                                ref_tokens = clip_tokenize([ref_text], truncate=True).to(device_clip)
                                
                                with torch.no_grad():
                                    # íŠ¹ì§• ì¶”ì¶œ
                                    image_features = clip_model.encode_image(image)
                                    pred_text_features = clip_model.encode_text(pred_tokens)
                                    ref_text_features = clip_model.encode_text(ref_tokens)
                                    
                                    # ì •ê·œí™”
                                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                                    pred_text_features = pred_text_features / pred_text_features.norm(dim=-1, keepdim=True)
                                    ref_text_features = ref_text_features / ref_text_features.norm(dim=-1, keepdim=True)
                                    
                                    # CLIP-S (Image-Prediction similarity)
                                    clip_s_score = (image_features @ pred_text_features.T).item()
                                    clip_s_scores.append(clip_s_score)
                                    
                                    # RefCLIP-S (Reference-Prediction similarity)
                                    ref_clip_s_score = (ref_text_features @ pred_text_features.T).item()
                                    ref_clip_s_scores.append(ref_clip_s_score)
                                    
                            except Exception as e_openai:
                                logger.debug(f"OpenAI CLIP ì²˜ë¦¬ ì˜¤ë¥˜: {e_openai}")
                                continue
                                
                        # HuggingFace CLIP ì‚¬ìš©
                        elif hasattr(clip_model, 'get_image_features'):
                            try:
                                inputs = preprocess(text=[pred_text, ref_text], images=image_pil, return_tensors="pt", padding=True)
                                inputs = {k: v.to(device_clip) for k, v in inputs.items()}
                                
                                with torch.no_grad():
                                    outputs = clip_model(**inputs)
                                    image_features = outputs.image_embeds
                                    text_features = outputs.text_embeds
                                    
                                    # ì •ê·œí™”
                                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                                    
                                    # CLIP-S (Image-Prediction similarity)
                                    clip_s_score = (image_features @ text_features[0:1].T).item()
                                    clip_s_scores.append(clip_s_score)
                                    
                                    # RefCLIP-S (Reference-Prediction similarity) 
                                    ref_clip_s_score = (text_features[1:2] @ text_features[0:1].T).item()
                                    ref_clip_s_scores.append(ref_clip_s_score)
                                    
                            except Exception as e_hf:
                                logger.debug(f"HuggingFace CLIP ì²˜ë¦¬ ì˜¤ë¥˜: {e_hf}")
                                continue
                        else:
                            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” CLIP ëª¨ë¸ íƒ€ì…: {type(clip_model)}")
                            continue
                    
                    except Exception as e:
                        logger.debug(f"ì´ë¯¸ì§€ {image_path} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue
                
                if clip_s_scores:
                    metrics['clip_s'] = float(np.mean(clip_s_scores))
                    logger.info(f"âœ“ CLIP-S: {metrics['clip_s']:.4f}")
                
                if ref_clip_s_scores:
                    metrics['ref_clip_s'] = float(np.mean(ref_clip_s_scores))
                    logger.info(f"âœ“ RefCLIP-S: {metrics['ref_clip_s']:.4f}")
            else:
                logger.warning("âš ï¸ ìœ íš¨í•œ ì´ë¯¸ì§€ê°€ ì—†ì–´ CLIP ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
    except Exception as e:
        logger.warning(f"âš ï¸ CLIP ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
        logger.debug(f"CLIP ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        
        # CLIP ëŒ€ì•ˆ: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë§Œ ê³„ì‚°
        try:
            logger.info("CLIP ëŒ€ì•ˆìœ¼ë¡œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹œë„...")
            from sentence_transformers import SentenceTransformer
            model_st = SentenceTransformer('all-MiniLM-L6-v2')
            
            pred_embeddings = model_st.encode(predictions)
            ref_embeddings = model_st.encode(ref_texts_for_bleu)
            
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = []
            for pred_emb, ref_emb in zip(pred_embeddings, ref_embeddings):
                sim = cosine_similarity([pred_emb], [ref_emb])[0][0]
                similarities.append(sim)
            
            metrics['clip_s'] = float(np.mean(similarities))
            metrics['ref_clip_s'] = float(np.mean(similarities))  # ë™ì¼í•œ ê°’ ì‚¬ìš©
            logger.info(f"âœ“ CLIP-S (ëŒ€ì•ˆ-í…ìŠ¤íŠ¸ìœ ì‚¬ë„): {metrics['clip_s']:.4f}")
            logger.info(f"âœ“ RefCLIP-S (ëŒ€ì•ˆ-í…ìŠ¤íŠ¸ìœ ì‚¬ë„): {metrics['ref_clip_s']:.4f}")
        except Exception as fallback_e:
            logger.warning(f"âš ï¸ CLIP ëŒ€ì•ˆ ê³„ì‚°ë„ ì‹¤íŒ¨: {fallback_e}")
            metrics['clip_s'] = 0.0
            metrics['ref_clip_s'] = 0.0
    
    # ë©”íŠ¸ë¦­ ì €ì¥
    metrics_path = output_dir / f"metrics_{timestamp}.json"
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ“ ë©”íŠ¸ë¦­ ì €ì¥: {metrics_path}")
    return metrics


def print_final_results(metrics: Dict[str, float]):
    """
    ìµœì¢… ê²°ê³¼ ì¶œë ¥
    """
    print("\n" + "=" * 80)
    print("ğŸ‰ PanoLLaVA ëª¨ë¸ í‰ê°€ ì™„ë£Œ")
    print("=" * 80)
    
    print("\nğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­ ê²°ê³¼:")
    print("-" * 40)
    
    if 'bleu4' in metrics:
        print(f"BLEU-4     (â†‘): {metrics['bleu4']:.4f}")
    if 'meteor' in metrics:
        print(f"METEOR     (â†‘): {metrics['meteor']:.4f}")
    if 'rougeL' in metrics:
        print(f"ROUGE-L    (â†‘): {metrics['rougeL']:.4f}")
    if 'spice' in metrics:
        print(f"SPICE      (â†‘): {metrics['spice']:.4f}")
    if 'cider' in metrics:
        print(f"CIDEr      (â†‘): {metrics['cider']:.4f}")
    if 'clip_s' in metrics:
        print(f"CLIP-S     (â†‘): {metrics['clip_s']:.4f}")
    if 'ref_clip_s' in metrics:
        print(f"RefCLIP-S  (â†‘): {metrics['ref_clip_s']:.4f}")
    
    print("-" * 40)
    print("ğŸ’¡ (â†‘) í‘œì‹œëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤.")
    print("=" * 80)


def main():
    parser = argparse.ArgumentParser(description="PanoLLaVA ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ")
    parser.add_argument('--ckpt', default='runs/e2p_finetune_mlp/best.ckpt', help='ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ê¸°ë³¸: runs/e2p_finetune_mlp/best.ckpt)')
    parser.add_argument('--lora-weights-path', default='runs/e2p_finetune_mlp/lora_weights', help='LoRA ê°€ì¤‘ì¹˜ ê²½ë¡œ (ê¸°ë³¸: runs/e2p_finetune_mlp/lora_weights)')
    parser.add_argument('--csv-input', default = 'data/quic360/test.csv', help='í…ŒìŠ¤íŠ¸ CSV íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output-dir', default='eval_results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--vision-name', default='google/siglip-base-patch16-224')
    parser.add_argument('--lm-name', default='Qwen/Qwen2.5-0.5B')
    parser.add_argument('--resampler', default='mlp')
    parser.add_argument('--crop-strategy', default='e2p', choices=['sliding_window', 'e2p', 'cubemap', 'resize', 'anyres', 'anyres_max'])
    parser.add_argument('--max-text-length', type=int, default=256)
    parser.add_argument('--max-new-tokens', type=int, default=128)
    parser.add_argument('--temperature', type=float, default=0.7)
    parser.add_argument('--min-new-tokens', type=int, default=5)
    parser.add_argument('--top-p', type=float, default=0.9)
    parser.add_argument('--top-k', type=int, default=50)
    parser.add_argument('--repetition-penalty', type=float, default=1.1)
    parser.add_argument('--length-penalty', type=float, default=1.0)
    parser.add_argument('--batch-size', type=int, default=16)
    parser.add_argument('--num-workers', type=int, default=16, help='ë°ì´í„°ë¡œë” ì›Œì»¤ ìˆ˜')
    parser.add_argument('--overlap-ratio', type=float, default=0.5, help='ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œ ë·° ê°„ ê²¹ì¹¨ ë¹„ìœ¨')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    try:
        # 1ë‹¨ê³„: ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
        model_kwargs = {
            "vision_name": args.vision_name,
            "lm_name": args.lm_name,
            "resampler": args.resampler,
            "lr": 1e-5,
            "max_text_length": args.max_text_length
        }
        model = load_model_and_lora(args.ckpt, args.lora_weights_path, device, **model_kwargs)
        
        # 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„
        datamodule, test_dataloader = prepare_test_dataset(
            args.csv_input, args.batch_size, args.max_text_length, 
            args.crop_strategy, args.lm_name, args.num_workers, args.overlap_ratio
        )
        
        # 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ìƒì„±
        predictions, references, image_paths, input_texts = generate_predictions(
            model, test_dataloader, datamodule, device,
            max_new_tokens=args.max_new_tokens, temperature=args.temperature,
            top_p=args.top_p, top_k=args.top_k,
            repetition_penalty=args.repetition_penalty, length_penalty=args.length_penalty,
            min_new_tokens=args.min_new_tokens,
        )
        
        # 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ë¡œê¹…
        df = save_and_log_results(predictions, references, image_paths, input_texts, output_dir, timestamp)
        
        # 5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = calculate_evaluation_metrics(df, output_dir, timestamp)
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print_final_results(metrics)
        
    except Exception as e:
        logger.error(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise


if __name__ == '__main__':
    main()
