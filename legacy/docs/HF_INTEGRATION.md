# HuggingFace Integration Guide

ë³¸ ë¬¸ì„œëŠ” PanoLLaVAë¥¼ HuggingFace Transformersì™€ í†µí•©í•œ êµ¬í˜„ ìƒì„¸ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

### ëª©ì 
ê¸°ì¡´ PanoLLaVA ëª¨ë¸ì„ HuggingFace Hubì— ì—…ë¡œë“œí•˜ê³  `from_pretrained()`, `push_to_hub()` ë“±ì˜ í‘œì¤€ HF APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜

### ì£¼ìš” ë³€ê²½ì‚¬í•­
1. **ìƒˆë¡œìš´ ë””ë ‰í† ë¦¬**: `hf_models/` - HF í˜¸í™˜ ë˜í¼ í´ë˜ìŠ¤
2. **ì´ì¤‘ ëª©ì  ì„¤ê³„**: 
   - Full VLM (ì´ë¯¸ì§€ â†’ í…ìŠ¤íŠ¸)
   - Feature Extractor (ì´ë¯¸ì§€ â†’ ì„ë² ë”©)
3. **3ë‹¨ê³„ í•™ìŠµ ì§€ì›**: VICReg â†’ Resampler â†’ Finetune

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### ê¸°ì¡´ êµ¬ì¡° (src/panovlm/)
```
src/panovlm/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model.py              # PanoramaVLM (Lightning ëª¨ë“ˆ)
â”‚   â”œâ”€â”€ vision/               # VisionBackbone, ResamplerModule
â”‚   â””â”€â”€ language_fusion.py
â”œâ”€â”€ processors/               # PanoramaImageProcessor
â”œâ”€â”€ losses/                   # VicRegLoss
â””â”€â”€ config/                   # Config
```

### HuggingFace êµ¬ì¡° (hf_models/)
```
hf_models/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ configuration_panollava.py    # PanoLLaVaConfig (PretrainedConfig)
â”œâ”€â”€ modeling_panollava.py         # ëª¨ë¸ í´ë˜ìŠ¤ 3ê°œ
â”œâ”€â”€ processing_panollava.py       # PanoLLaVaProcessor
â”œâ”€â”€ example_usage.py              # ì‚¬ìš© ì˜ˆì‹œ
â”œâ”€â”€ test_hf_compatibility.py      # í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
â””â”€â”€ README.md
```

## ğŸ”„ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 1. Configuration (configuration_panollava.py)

**í´ë˜ìŠ¤**: `PanoLLaVaConfig(PretrainedConfig)`

**ì£¼ìš” íŒŒë¼ë¯¸í„°**:
```python
{
  "model_type": "panollava",
  "is_composition": true,
  
  # Vision
  "vision_config": {
    "model_name": "google/siglip-base-patch16-224",
    "image_size": 224,
    "patch_size": 16
  },
  
  # Text
  "text_config": {
    "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
    "hidden_size": 896
  },
  
  # Resampler
  "resampler_type": "mlp",  # or "qformer", "bimamba"
  "latent_dimension": 768,
  "num_latent_tokens": 64,
  
  # VICReg
  "vicreg_similarity_weight": 25.0,
  "vicreg_variance_weight": 25.0,
  "vicreg_covariance_weight": 1.0,
  "overlap_ratio": 0.3,
  
  # Image Processing
  "crop_strategy": "anyres_e2p",
  "num_views": 4,
  "fov_deg": 90.0,
  
  # LoRA
  "use_lora": false,
  "lora_r": 16,
  "lora_alpha": 32
}
```

**HF í˜¸í™˜ì„±**:
- âœ… `from_pretrained()` - JSON config ìë™ ë¡œë“œ
- âœ… `save_pretrained()` - config.json ìë™ ì €ì¥
- âœ… `to_dict()` - ì§ë ¬í™”
- âœ… `is_composition=True` - ë©€í‹°ëª¨ë‹¬ ë§ˆì»¤

### 2. Modeling (modeling_panollava.py)

#### 2.1 PanoLLaVaPreTrainedModel (Base)

**ì—­í• **: ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”, gradient checkpointing ì§€ì›

```python
class PanoLLaVaPreTrainedModel(PreTrainedModel):
    config_class = PanoLLaVaConfig
    base_model_prefix = "panollava"
    supports_gradient_checkpointing = True
    _no_split_modules = ["PanoLLaVaVisionEncoder", "PanoLLaVaResampler"]
```

#### 2.2 PanoLLaVaForConditionalGeneration

**ìš©ë„**: ì „ì²´ VLM (ë¹„ì „ â†’ ì–¸ì–´)

**êµ¬ì„±ìš”ì†Œ**:
```python
self.vision_backbone    # VisionBackbone (from src/panovlm)
self.resampler          # ResamplerModule
self.projector          # PanoramaProjector
self.language_model     # AutoModelForCausalLM (Qwen/Llama/Gemma)
self.language_fusion    # LanguageFusion
```

**Forward íë¦„**:
1. `pixel_values` â†’ vision_backbone â†’ `[B*V, H'*W', D_vision]`
2. Resampler â†’ `[B*V, N, D_latent]`
3. Projector â†’ `[B*V, N, D_lm]`
4. LanguageFusion.fuse() â†’ `{inputs_embeds, attention_mask, labels}`
5. language_model â†’ `{loss, logits}`

**ì¶œë ¥**: `PanoLLaVaCausalLMOutput`

#### 2.3 PanoLLaVaForVICReg

**ìš©ë„**: Stage 1 VICReg í•™ìŠµ ì „ìš©

**êµ¬ì„±ìš”ì†Œ**:
```python
self.vision_backbone
self.resampler
self.vicreg_projector   # VICRegProjector (í•™ìŠµ í›„ íê¸°)
```

**Forward íë¦„**:
1. `pixel_values [B, V, C, H, W]` â†’ vision features
2. Resampler â†’ `[B, V, N, D]`
3. VICReg projector â†’ `[B, V, N, D']`
4. `_compute_vicreg_overlap_loss()` â†’ invariance, variance, covariance

**ì¶œë ¥**: `PanoLLaVaVICRegOutput`

#### 2.4 PanoLLaVaForFeatureExtraction

**ìš©ë„**: ë¹„ì „ ì„ë² ë”© ì¶”ì¶œ ì „ìš© (LM ì—†ìŒ)

**êµ¬ì„±ìš”ì†Œ**:
```python
self.vision_backbone
self.resampler
```

**Forward íë¦„**:
1. `pixel_values` â†’ vision features â†’ `[B*V, H'*W', D]`
2. Resampler â†’ `[B, V*N, D]`
3. Pooling (mean/max/first) â†’ `[B, D]`

**ì¶œë ¥**: `PanoLLaVaFeatureOutput`

### 3. Processing (processing_panollava.py)

**í´ë˜ìŠ¤**: `PanoLLaVaProcessor(ProcessorMixin)`

**ì—­í• **: ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ í†µí•© ì²˜ë¦¬

**êµ¬ì„±ìš”ì†Œ**:
```python
self.image_processor  # AutoImageProcessor (from vision model)
self.tokenizer        # AutoTokenizer (from text model)
self.vision_token     # '<|vision|>'
```

**`__call__()` ë¡œì§**:
```python
# ì´ë¯¸ì§€ ì²˜ë¦¬
image_inputs = self.image_processor(images)  # {pixel_values}

# í…ìŠ¤íŠ¸ ì²˜ë¦¬ (chat template ì ìš©)
if hasattr(tokenizer, 'apply_chat_template'):
    messages = [{"role": "user", "content": text}]
    formatted = tokenizer.apply_chat_template(messages, ...)
text_inputs = self.tokenizer(formatted)  # {input_ids, attention_mask}

# í†µí•©
return {**image_inputs, **text_inputs}
```

## ğŸ”— ê¸°ì¡´ ì½”ë“œì™€ì˜ ì—°ê²°

### Import êµ¬ì¡°

**HF ëª¨ë¸ì—ì„œ ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ ì¬ì‚¬ìš©**:

```python
# modeling_panollava.py ë‚´ë¶€
from ..src.panovlm.models.vision import VisionBackbone, ResamplerModule
from ..src.panovlm.models.language_fusion import LanguageFusion
from ..src.panovlm.losses.vicreg_overlap import VICRegProjector, compute_vicreg_overlap_loss
```

**ì¥ì **:
- âœ… ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš© (ì¤‘ë³µ ì—†ìŒ)
- âœ… ë²„ê·¸ í”½ìŠ¤ ìë™ ë°˜ì˜
- âœ… ë‹¨ì¼ êµ¬í˜„ ìœ ì§€

### í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ í†µí•©

**ê¸°ì¡´ Lightning í•™ìŠµ ìœ ì§€**:
```bash
python scripts/train.py --config configs/default.yaml
```

**HF ì²´í¬í¬ì¸íŠ¸ ë³€í™˜**:
```python
# Lightning checkpoint â†’ HF format
from hf_models import PanoLLaVaForConditionalGeneration

# 1. Load Lightning checkpoint
ckpt = torch.load("runs/stage3_final.ckpt")
state_dict = ckpt['state_dict']

# 2. Create HF model
model = PanoLLaVaForConditionalGeneration.from_pretrained(config)

# 3. Load weights (key mapping may be needed)
model.load_state_dict(state_dict, strict=False)

# 4. Save in HF format
model.save_pretrained("hf_checkpoints/panollava-vlm")
```

## ğŸ“¦ HuggingFace Hub ì›Œí¬í”Œë¡œìš°

### 1. ì²´í¬í¬ì¸íŠ¸ ì¤€ë¹„

**Stageë³„ ë³€í™˜**:

```bash
# Stage 1: VICReg
python scripts/convert_to_hf.py \
  --lightning-ckpt runs/ADDDATA_SQ3_1_latent768_PE_anyres_e2p_vision_mlp/vision_final.ckpt \
  --output-dir hf_checkpoints/panollava-stage1-vicreg \
  --model-type vicreg

# Stage 2: Resampler
python scripts/convert_to_hf.py \
  --lightning-ckpt runs/.../resampler_final.ckpt \
  --output-dir hf_checkpoints/panollava-stage2-resampler \
  --model-type conditional-generation

# Stage 3: Final VLM
python scripts/convert_to_hf.py \
  --lightning-ckpt runs/.../finetune_final.ckpt \
  --output-dir hf_checkpoints/panollava-vlm \
  --model-type conditional-generation
```

### 2. Hub ì—…ë¡œë“œ

```python
from huggingface_hub import HfApi, login

login(token="your_hf_token")

# Model
model.push_to_hub("your-org/panollava-vlm")

# Processor
processor.push_to_hub("your-org/panollava-vlm")

# Config (ìë™ í¬í•¨ë¨)
```

### 3. ì‚¬ìš©ì ë¡œë“œ

```python
from transformers import AutoProcessor, AutoModelForVision2Seq

processor = AutoProcessor.from_pretrained(
    "your-org/panollava-vlm",
    trust_remote_code=True  # ì¤‘ìš”!
)

model = AutoModelForVision2Seq.from_pretrained(
    "your-org/panollava-vlm",
    trust_remote_code=True,
    dtype=torch.bfloat16,
    device_map="auto"
)
```

**ì£¼ì˜**: `trust_remote_code=True` í•„ìˆ˜ (ì»¤ìŠ¤í…€ ëª¨ë¸ í´ë˜ìŠ¤)

## ğŸ§ª í…ŒìŠ¤íŠ¸

### í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸

```bash
python hf_models/test_hf_compatibility.py
```

**í…ŒìŠ¤íŠ¸ í•­ëª©**:
1. âœ… Import ì„±ê³µ
2. âœ… Config ìƒì„± ë° ì§ë ¬í™”
3. âœ… ëª¨ë¸ êµ¬ì¡° ìƒì„± (ê°€ì¤‘ì¹˜ ë¯¸í¬í•¨)
4. âœ… AutoClass ë“±ë¡
5. âœ… Processor ìƒì„±

### í†µí•© í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ê°€ì¤‘ì¹˜)

```python
# Stage 1 í…ŒìŠ¤íŠ¸
python hf_models/example_usage.py --example 3

# Full VLM í…ŒìŠ¤íŠ¸
python hf_models/example_usage.py --example 1

# Feature ì¶”ì¶œ í…ŒìŠ¤íŠ¸
python hf_models/example_usage.py --example 2
```

## ğŸ”§ ë””ë²„ê¹… ê°€ì´ë“œ

### Import Error

**ë¬¸ì œ**: `ModuleNotFoundError: No module named 'panovlm'`

**í•´ê²°**:
```bash
cd /data/1_personal/4_SWWOO/panollava
pip install -e .
```

### trust_remote_code ê²½ê³ 

**ë¬¸ì œ**: `trust_remote_code=True` ì—†ì´ ë¡œë“œ ì‹œë„

**í•´ê²°**:
```python
# í•­ìƒ trust_remote_code=True ì¶”ê°€
AutoModelForVision2Seq.from_pretrained(..., trust_remote_code=True)
```

### ê°€ì¤‘ì¹˜ ë¶ˆì¼ì¹˜

**ë¬¸ì œ**: Lightning checkpoint keyì™€ HF key ë¶ˆì¼ì¹˜

**í•´ê²°**:
```python
# Key mapping ì ìš©
state_dict_mapped = {}
for k, v in lightning_state_dict.items():
    new_key = k.replace('model.', '').replace('module.', '')
    state_dict_mapped[new_key] = v

model.load_state_dict(state_dict_mapped, strict=False)
```

## ğŸ“ TODO

- [ ] `scripts/convert_to_hf.py` êµ¬í˜„ (Lightning â†’ HF ë³€í™˜)
- [ ] CSV íŒŒì¼ ìˆ˜ì • (unquoted comma ë¬¸ì œ)
- [ ] Model Card í…œí”Œë¦¿ ì‘ì„±
- [ ] CI/CD for HF Hub ìë™ ì—…ë¡œë“œ
- [ ] Gradio ë°ëª¨ ì•±

## ğŸ“ ì°¸ê³  ë¬¸ì„œ

- HuggingFace Custom Models: https://huggingface.co/docs/transformers/custom_models
- Vision-Language Models: https://huggingface.co/docs/transformers/model_doc/llava
- PEFT/LoRA: https://huggingface.co/docs/peft/
- Model Hub: https://huggingface.co/docs/hub/models-adding-libraries

## ğŸ™ ê¸°ì—¬

ì´ êµ¬í˜„ì€ PanoLLaVA í”„ë¡œì íŠ¸ì˜ ì¼ë¶€ë¡œ, HuggingFace ìƒíƒœê³„ì™€ì˜ í†µí•©ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
