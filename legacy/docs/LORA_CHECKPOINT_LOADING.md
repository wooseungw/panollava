# LoRA Checkpoint Loading κ°€μ΄λ“

## π” "λ„λ½λ ν‚¤ / μμƒμΉ λ»ν• ν‚¤" λ©”μ‹μ§€λ” μ •μƒμ…λ‹λ‹¤!

### μ¶λ ¥ μμ‹
```
π” LoRA κ°μ§€: Lightning μ²΄ν¬ν¬μΈνΈμ— LoRA state_dict ν¬ν•¨
β™οΈ κ°€μ¤‘μΉ λ΅λ”© μ¤‘...
   - λ΅λ“λ ν‚¤: 1024/1335
   β… LoRA κ΄€λ ¨ λ„λ½ ν‚¤: 311κ° (μ •μƒ - LoRA κµ¬μ΅° μ°¨μ΄)
   β… LoRA κ΄€λ ¨ μ¶”κ°€ ν‚¤: 703κ° (μ •μƒ - Lightning μ²΄ν¬ν¬μΈνΈ ν¬ν•¨)
```

## β“ μ™ μ΄λ° λ©”μ‹μ§€κ°€ λ‚μ¤λ‚μ”?

### 1. LoRA μ μ© μ‹ ν‚¤ κµ¬μ΅° λ³€κ²½

**μΌλ° λ¨λΈ (LoRA μ—†μ):**
```python
language_model.model.layers.0.self_attn.q_proj.weight
language_model.model.layers.0.self_attn.k_proj.weight
```

**LoRA μ μ©λ λ¨λΈ:**
```python
# Base modelμ€ frozen
language_model.base_model.model.layers.0.self_attn.q_proj.weight

# LoRA adapters (trainable)
language_model.base_model.model.layers.0.self_attn.q_proj.lora_A.default.weight
language_model.base_model.model.layers.0.self_attn.q_proj.lora_B.default.weight
```

β†’ **ν‚¤ κµ¬μ΅°κ°€ μ™„μ „ν λ‹¤λ¦…λ‹λ‹¤!**

### 2. PyTorch Lightning μ²΄ν¬ν¬μΈνΈ κµ¬μ΅°

**Lightning μ²΄ν¬ν¬μΈνΈ (.ckpt)**μ—λ” λ‹¤μμ΄ ν¬ν•¨λ©λ‹λ‹¤:
```python
{
    'state_dict': {
        'model.vision_encoder.*': ...,           # Vision encoder weights
        'model.resampler_module.*': ...,         # Resampler weights
        'model.language_model.base_model.*': ..., # LoRA-adapted LM weights
    },
    'optimizer_states': [...],
    'lr_schedulers': [...],
    'hparams': {...},
    ...
}
```

**ν•μ§€λ§ `from_checkpoint()`μ—μ„ μƒμ„±ν•λ” λ¨λΈ:**
```python
PanoramaVLM(
    vision_encoder=...,          # 'vision_encoder.*'
    resampler_module=...,        # 'resampler_module.*'
    language_model=...,          # 'language_model.*' (LoRA μ μ©λ¨)
)
```

β†’ **`model.` ν”„λ¦¬ν”½μ¤κ°€ λ‹¤λ¦…λ‹λ‹¤!**

### 3. Missing vs Unexpected ν‚¤

**Missing Keys (λ„λ½λ ν‚¤):**
- Lightning μ²΄ν¬ν¬μΈνΈμ—λ” μ—†μ§€λ§ ν„μ¬ λ¨λΈμ΄ κΈ°λ€ν•λ” ν‚¤
- μ: `language_model.base_model.*` (μ²΄ν¬ν¬μΈνΈλ” `model.language_model.*`)
- **LoRA κ΄€λ ¨μ€ μ •μƒ!** LoRA κµ¬μ΅° μ°¨μ΄ λ•λ¬Έ

**Unexpected Keys (μμƒμΉ λ»ν• ν‚¤):**
- μ²΄ν¬ν¬μΈνΈμ—λ” μμ§€λ§ ν„μ¬ λ¨λΈμ— μ—†λ” ν‚¤
- μ: `model.vision_encoder.*` (ν„μ¬ λ¨λΈμ€ `vision_encoder.*`)
- Lightning λ©”νƒ€λ°μ΄ν„° ν‚¤λ“¤ (optimizer, scheduler λ“±)
- **λ€λ¶€λ¶„ μ •μƒ!** Lightning μ²΄ν¬ν¬μΈνΈ νΉμ„±

## β… μ •μƒμ μΈ λ΅λ”© ν™•μΈ λ°©λ²•

### 1. λ΅κ·Έ λ©”μ‹μ§€ ν™•μΈ

**β… μ •μƒ:**
```
   - λ΅λ“λ ν‚¤: 1024/1335
   β… LoRA κ΄€λ ¨ λ„λ½ ν‚¤: 311κ° (μ •μƒ - LoRA κµ¬μ΅° μ°¨μ΄)
   β… LoRA κ΄€λ ¨ μ¶”κ°€ ν‚¤: 703κ° (μ •μƒ - Lightning μ²΄ν¬ν¬μΈνΈ ν¬ν•¨)
```

**β οΈ μ£Όμ ν•„μ”:**
```
   - λ΅λ“λ ν‚¤: 100/1335  β† λ„λ¬΄ μ μ!
   β οΈ  Non-LoRA λ„λ½ ν‚¤: 500κ°  β† Vision/Resampler κ°€μ¤‘μΉ λ„λ½?
```

### 2. ν•µμ‹¬ μ»΄ν¬λ„νΈ λ΅λ”© ν™•μΈ

**ν™•μΈν•΄μ•Ό ν•  κ²ƒ:**
- β… Vision Encoder: `vision_encoder.*` ν‚¤λ“¤ λ΅λ“λ¨
- β… Resampler: `resampler_module.*` ν‚¤λ“¤ λ΅λ“λ¨
- β… Language Model Base: `language_model.base_model.model.*` ν‚¤λ“¤ λ΅λ“λ¨
- β… LoRA Adapters: `language_model.*.lora_A.*`, `language_model.*.lora_B.*` ν‚¤λ“¤ λ΅λ“λ¨

### 3. μ‹¤μ  μ¶”λ΅  ν…μ¤νΈ

```python
# κ°„λ‹¨ν• forward pass ν…μ¤νΈ
import torch
output = model.generate(
    pixel_values=torch.randn(1, 3, 224, 224),
    input_ids=torch.tensor([[1, 2, 3]]),
    max_new_tokens=10
)
print("β… λ¨λΈ μ •μƒ μ‘λ™!" if output.shape[1] > 3 else "β λ¬Έμ  λ°μƒ")
```

## π”§ λ¬Έμ  ν•΄κ²°

### λ¬Έμ  1: Non-LoRA λ„λ½ ν‚¤κ°€ λ§μ

**μ¦μƒ:**
```
β οΈ  Non-LoRA λ„λ½ ν‚¤: 500κ°
     β€Ά vision_encoder.embeddings.position_embedding
     β€Ά resampler_module.resampler.blocks.0.weight
```

**μ›μΈ:** Vision encoder λλ” Resampler κ°€μ¤‘μΉκ°€ μ²΄ν¬ν¬μΈνΈμ— μ—†μ

**ν•΄κ²°:**
1. μ¬λ°”λ¥Έ μ²΄ν¬ν¬μΈνΈ κ²½λ΅ ν™•μΈ
2. μ²΄ν¬ν¬μΈνΈκ°€ μ¬λ°”λ¥Έ stageμ—μ„ μ €μ¥λμ—λ”μ§€ ν™•μΈ
   - Vision stage: vision_encoderλ§ ν•™μµ
   - Resampler stage: + resampler μ¶”κ°€
   - Finetune stage: λ¨λ“  κ°€μ¤‘μΉ ν¬ν•¨

### λ¬Έμ  2: λ΅λ“λ ν‚¤κ°€ λ„λ¬΄ μ μ

**μ¦μƒ:**
```
   - λ΅λ“λ ν‚¤: 50/1335  β† λ§¤μ° μ μ!
```

**μ›μΈ:** μ²΄ν¬ν¬μΈνΈ νμΌμ΄ μ†μƒλμ—κ±°λ‚ μλ»λ νμΌ

**ν•΄κ²°:**
```bash
# μ²΄ν¬ν¬μΈνΈ κ²€μ‚¬
python -c "
import torch
ckpt = torch.load('checkpoint.ckpt', map_location='cpu')
print('Keys in checkpoint:', ckpt.keys())
print('State dict keys:', len(ckpt.get('state_dict', {}).keys()))
"
```

### λ¬Έμ  3: LoRA κ°€μ¤‘μΉκ°€ λ΅λ“λμ§€ μ•μ

**μ¦μƒ:**
```
π” LoRA κ°μ§€ μ‹¤ν¨ - lora_weights/ λ””λ ‰ν† λ¦¬ μ‚¬μ©
```

**μ›μΈ:** Lightning μ²΄ν¬ν¬μΈνΈμ— LoRA state_dictκ°€ ν¬ν•¨λμ§€ μ•μ

**ν•΄κ²°:**
```bash
# LoRA κ°€μ¤‘μΉκ°€ λ³„λ„ λ””λ ‰ν† λ¦¬μ— μλ”μ§€ ν™•μΈ
ls -la runs/.../finetune/.../lora_weights/

# μλ‹¤λ©΄ μλ™μΌλ΅ λ΅λ“λ¨
# μ—†λ‹¤λ©΄ training μ‹ LoRAκ°€ μ λ€λ΅ μ €μ¥λμ§€ μ•μ€ κ²ƒ
```

## π“ ν‚¤ λ΅λ”© ν†µκ³„ μμ‹

### μ •μƒμ μΈ μΌ€μ΄μ¤

**Finetune Stage (LoRA μ μ©)**
```
Total keys in checkpoint: 1335
Loaded keys: 1024/1335
Missing keys:
  β… LoRA-related: 311 (language_model.base_model.*)
  β οΈ  Non-LoRA: 0
Unexpected keys:
  β… LoRA-related: 700 (model.*, optimizer.*, etc.)
  β οΈ  Non-LoRA: 3 (minor metadata)
```

**Resampler Stage (LoRA μ—†μ)**
```
Total keys in checkpoint: 800
Loaded keys: 750/800
Missing keys:
  β… LoRA-related: 0
  β οΈ  Non-LoRA: 50 (language_model.* - not trained yet)
Unexpected keys:
  β… Lightning metadata: 50
  β οΈ  Non-LoRA: 0
```

## π“ FAQ

**Q: λ„λ½λ ν‚¤ 311κ°λ” λ¬Έμ  μ•„λ‹κ°€μ”?**  
A: LoRA κ΄€λ ¨μ΄λ©΄ μ •μƒμ…λ‹λ‹¤. LoRAλ” `base_model.model.*` κµ¬μ΅°λ¥Ό μ‚¬μ©ν•λ”λ°, μ²΄ν¬ν¬μΈνΈλ” Lightningμ `model.*` κµ¬μ΅°λ΅ μ €μ¥λμ–΄ ν‚¤ μ΄λ¦„μ΄ λ‹¤λ¦…λ‹λ‹¤.

**Q: μμƒμΉ λ»ν• ν‚¤ 703κ°λ”?**  
A: Lightning μ²΄ν¬ν¬μΈνΈμ—λ” optimizer, scheduler, hparams λ“± λ©”νƒ€λ°μ΄ν„°κ°€ ν¬ν•¨λ©λ‹λ‹¤. μ΄λ“¤μ€ λ¨λΈ κ°€μ¤‘μΉκ°€ μ•„λ‹λ―€λ΅ λ¬΄μ‹λ©λ‹λ‹¤.

**Q: λ΅λ“λ ν‚¤ 1024/1335λ” μ¶©λ¶„ν•κ°€μ”?**  
A: λ„¤! λ‚λ¨Έμ§€λ” LoRA κµ¬μ΅° μ°¨μ΄ λ•λ¬Έμ…λ‹λ‹¤. μ‹¤μ λ΅ ν•„μ”ν• κ°€μ¤‘μΉλ” λ¨λ‘ λ΅λ“λ©λ‹λ‹¤.

**Q: strict=Falseλ” μ•μ „ν•κ°€μ”?**  
A: λ„¤! LoRA μ‚¬μ© μ‹ ν•„μμ…λ‹λ‹¤. `strict=True`λ©΄ ν‚¤ μ΄λ¦„μ΄ μ •ν™•ν μΌμΉν•΄μ•Ό ν•λ”λ°, LoRAλ” κµ¬μ΅°κ°€ λ‹¬λΌμ„ λ¶κ°€λ¥ν•©λ‹λ‹¤.

## π“ μ”μ•½

| λ©”μ‹μ§€ | μλ―Έ | μ •μƒ μ—¬λ¶€ |
|--------|------|-----------|
| "LoRA κ΄€λ ¨ λ„λ½ ν‚¤: 311κ°" | LoRA κµ¬μ΅° μ°¨μ΄ | β… μ •μƒ |
| "LoRA κ΄€λ ¨ μ¶”κ°€ ν‚¤: 703κ°" | Lightning λ©”νƒ€λ°μ΄ν„° | β… μ •μƒ |
| "Non-LoRA λ„λ½ ν‚¤: 0κ°" | ν•µμ‹¬ κ°€μ¤‘μΉ μ™„μ „ λ΅λ“ | β… μ •μƒ |
| "Non-LoRA λ„λ½ ν‚¤: 500κ°" | Vision/Resampler λ„λ½? | β οΈ ν™•μΈ ν•„μ” |
| "λ΅λ“λ ν‚¤: 1024/1335" | λ€λ¶€λ¶„ λ΅λ“λ¨ | β… μ •μƒ (LoRA μ‚¬μ© μ‹) |
| "λ΅λ“λ ν‚¤: 50/1335" | λ€λ¶€λ¶„ λ„λ½λ¨ | β λ¬Έμ  μμ |

**κ²°λ΅ **: LoRA μ‚¬μ© μ‹ ν‚¤ λ¶μΌμΉλ” μ •μƒμ…λ‹λ‹¤! Non-LoRA ν‚¤λ§ ν™•μΈν•μ„Έμ”. π‰
