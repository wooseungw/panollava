# default.yaml ê¸°ë°˜ PanoLLaVA vs. ìƒìš© VLM ì„¤ì • ë¹„êµ

## ğŸ“‹ í˜„ì¬ ì„¤ì • ìš”ì•½ (default.yaml)

```yaml
# í•µì‹¬ ì„¤ì •
vision_encoder: siglip2-so400m-patch16-256 (400M íŒŒë¼ë¯¸í„°)
language_model: Qwen3-0.6B (600M íŒŒë¼ë¯¸í„°)
resampler: BiMamba
crop_strategy: anyres_e2p (íŒŒë…¸ë¼ë§ˆ íŠ¹í™”)
positional_encoding: enabled
stages: [vision, resampler, finetune]
lora: enabled (rank=32, alpha=64)
```

---

## âœ… ê³µí†µì  (ìƒìš© VLMê³¼ ë™ì¼í•œ ì„¤ê³„ ì„ íƒ)

### 1. **Vision Encoder ì„ íƒ**

**PanoLLaVA (default.yaml)**:
```yaml
vision_name: "google/siglip2-so400m-patch16-256"
```

**ìƒìš© VLM**:
- âœ… LLaVA-OneVision: rice-vit (SigLIP ê¸°ë°˜)
- âœ… Qwen2.5-VL: SigLIP-Large
- âœ… InternVL: InternViT (DINOv2 ìœ ì‚¬)

**ê³µí†µì **: ëª¨ë‘ **CLIP/SigLIP ê³„ì—´** ì‚¬ìš© (ê°•ë ¥í•œ vision-language ì •ë ¬)

---

### 2. **Language Model í¬ê¸°**

**PanoLLaVA (default.yaml)**:
```yaml
language_model_name: "Qwen/Qwen3-0.6B"  # 0.6B
```

**ìƒìš© VLM**:
- âœ… LLaVA-OneVision-0.5B: 0.5B (ìœ ì‚¬)
- âœ… Qwen2.5-VL: 2B, 7B, 32B
- âœ… InternVL: 1B, 2B, 4B, 8B

**ê³µí†µì **: **ì†Œí˜• ëª¨ë¸** ì§€ì› (ëª¨ë°”ì¼/ì—£ì§€ ë°°í¬ ê³ ë ¤)

---

### 3. **LoRA í™œìš©**

**PanoLLaVA (default.yaml)**:
```yaml
lora:
  use_lora: true
  rank: 32  
  alpha: 64
  dropout: 0.1
  target_modules: ["q_proj", "k_proj"]
```

**ìƒìš© VLM**:
- âš ï¸ ëŒ€ë¶€ë¶„ **LoRAë¥¼ ê³µì‹ì ìœ¼ë¡œ ì§€ì›í•˜ì§€ ì•ŠìŒ** (full fine-tuning)
- ì¼ë¶€ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ LoRA ì ìš© ì‹œë„

**ë¶€ë¶„ ê³µí†µì **: LoRAëŠ” **PEFT (Parameter-Efficient Fine-Tuning)** íŠ¸ë Œë“œë¥¼ ë”°ë¦„

---

### 4. **Multi-Stage Training**

**PanoLLaVA (default.yaml)**:
```yaml
stages: ["vision", "resampler", "finetune"]
```

**ìƒìš© VLM**:
- âœ… LLaVA ê³„ì—´: 2-3ë‹¨ê³„ (pretraining â†’ SFT)
- âœ… Qwen2.5-VL: Vision pretraining â†’ SFT
- âœ… InternVL: Multi-stage progressive training

**ê³µí†µì **: **ì ì§„ì  í•™ìŠµ** (progressive training) ì „ëµ ì‚¬ìš©

---

### 5. **Flash Attention 2 (ì•”ë¬µì )**

**PanoLLaVA**:
- ì½”ë“œì—ì„œ Flash Attention 2 ìë™ ê°ì§€ ë° ì‚¬ìš©
- ë©”ëª¨ë¦¬ ~30% ì ˆê°, ì†ë„ ~2ë°° í–¥ìƒ

**ìƒìš© VLM**:
- âœ… ëª¨ë“  ìµœì‹  VLMì´ Flash Attention ì‚¬ìš©

**ê³µí†µì **: **ë©”ëª¨ë¦¬ ìµœì í™”** ê¸°ë²• í‘œì¤€í™”

---

## âŒ ì°¨ì´ì  (PanoLLaVAë§Œì˜ ë…íŠ¹í•œ ì„¤ì •)

### 1. **íŒŒë…¸ë¼ë§ˆ íŠ¹í™” ì´ë¯¸ì§€ ì²˜ë¦¬** â­â­â­

**PanoLLaVA (default.yaml)**:
```yaml
image_processing:
  crop_strategy: "anyres_e2p"  # Equirectangular-to-Perspective
  overlap_ratio: 0.5           # 50% ê²¹ì¹¨
  fov_deg: 90.0                # 90ë„ ì‹œì•¼ê°
  anyres_max_patches: 9        # ìµœëŒ€ 9ê°œ íƒ€ì¼
```

**ì˜ë¯¸**:
- 360Â° íŒŒë…¸ë¼ë§ˆë¥¼ **ì—¬ëŸ¬ perspective ë·°ë¡œ ë³€í™˜**
- ì¸ì ‘ ë·° ê°„ **50% ê²¹ì¹¨** â†’ VICReg lossì—ì„œ í™œìš©
- ìµœëŒ€ 9ê°œ íƒ€ì¼ ìƒì„± â†’ ê³ í•´ìƒë„ ì •ë³´ ë³´ì¡´

**ìƒìš© VLM**:
```yaml
# ì¼ë°˜ì ì¸ ì²˜ë¦¬
image_processing:
  resize: true
  max_size: [384, 384]  # ë˜ëŠ” ë” í¼
  strategy: "letterbox"  # ë˜ëŠ” "center_crop"
```

**ì°¨ì´ì **: 
- âŒ ìƒìš© VLM: ë‹¨ìˆœ **ë¦¬ì‚¬ì´ì§•/í¬ë¡­**
- âœ… PanoLLaVA: **ê¸°í•˜í•™ì  ë³€í™˜ + ë‹¤ì¤‘ ë·° ìƒì„±**

---

### 2. **VICReg Loss (Overlap ì •ê·œí™”)** â­â­â­

**PanoLLaVA (default.yaml)**:
```yaml
vision:
  vicreg_loss_weight: 1.0
  vicreg_mode: "pairwise"  # ì¸ì ‘ ë·° ìŒ ë¹„êµ
  vicreg_similarity_weight: 25.0   # Invariance
  vicreg_variance_weight: 25.0     # Variance
  vicreg_covariance_weight: 1.0    # Covariance
```

**ì˜ë¯¸**:
- **ì¸ì ‘ ë·°ì˜ ê²¹ì¹˜ëŠ” ì˜ì—­**ì—ì„œ feature ì¼ê´€ì„± ê°•ì œ
- `25*invariance + 25*variance + 1*covariance`
- íŒŒë…¸ë¼ë§ˆì˜ **ê³µê°„ ì—°ì†ì„±** í•™ìŠµ

**ìƒìš© VLM**:
```yaml
# VICReg loss ì—†ìŒ
# ì¼ë°˜ì ì¸ contrastive learningë§Œ ì‚¬ìš© (optional)
```

**ì°¨ì´ì **:
- âŒ ìƒìš© VLM: VICReg **ë¯¸ì‚¬ìš©**
- âœ… PanoLLaVA: **VICReg ê¸°ë°˜** ìê¸° ê°ì‹œ í•™ìŠµ

---

### 3. **Resampler êµ¬ì¡° ì„ íƒ** â­â­

**PanoLLaVA (default.yaml)**:
```yaml
resampler_type: "bimamba"  # BiDirectional Mamba
```

**ì§€ì› ì˜µì…˜**:
- `mlp`: ê°„ë‹¨í•œ MLP (ë¹ ë¦„)
- `qformer`: BLIP2 ìŠ¤íƒ€ì¼ (ì •í™•í•¨)
- `bimamba`: ì–‘ë°©í–¥ Mamba (ìµœì‹ , ë¹ ë¦„)
- `perceiver`: Perceiver IO (ìœ ì—°í•¨)

**ìƒìš© VLM**:
```yaml
# ëŒ€ë¶€ë¶„ ê³ ì •
resampler: "linear_projection"  # ì„ í˜• í”„ë¡œì ì…˜ë§Œ
```

**ì°¨ì´ì **:
- âŒ ìƒìš© VLM: **ì„ í˜• í”„ë¡œì ì…˜** ê³ ì •
- âœ… PanoLLaVA: **5ê°€ì§€ êµ¬ì¡°** ì¤‘ ì„ íƒ ê°€ëŠ¥

---

### 4. **Positional Encoding (Projection Layer)** â­

**PanoLLaVA (default.yaml)**:
```yaml
use_projection_positional_encoding: true
# pe_view_encoding_type: "sinusoidal"      # ë·° ìœ„ì¹˜ ì¸ì½”ë”©
# pe_spatial_encoding_type: "sinusoidal"   # ê³µê°„ ìœ„ì¹˜ ì¸ì½”ë”©
# pe_enable_continuity: true               # 360Â° ì—°ì†ì„±
```

**ì˜ë¯¸**:
- ë‹¤ì¤‘ ë·°ì˜ **ìƒëŒ€ ìœ„ì¹˜** ì •ë³´ ì¸ì½”ë”©
- 360Â° íŒŒë…¸ë¼ë§ˆì˜ **ìˆœí™˜ êµ¬ì¡°** ë°˜ì˜
- Projection layerì—ì„œ ìœ„ì¹˜ ì •ë³´ ì¶”ê°€

**ìƒìš© VLM**:
```yaml
# ê¸°ë³¸ Vision Transformerì˜ positional encodingë§Œ ì‚¬ìš©
# ë‹¤ì¤‘ ë·° ìœ„ì¹˜ ì •ë³´ëŠ” ê³ ë ¤ ì•ˆ í•¨
```

**ì°¨ì´ì **:
- âŒ ìƒìš© VLM: **ë‹¨ì¼ ë·°** PEë§Œ
- âœ… PanoLLaVA: **ë‹¤ì¤‘ ë·° ìœ„ì¹˜** + **360Â° ì—°ì†ì„±** PE

---

### 5. **Vision Encoder Fine-tuning ì œì–´** â­

**PanoLLaVA (default.yaml)**:
```yaml
vision:
  vision_trainable_blocks: 2  # ë§ˆì§€ë§‰ 2ê°œ ë¸”ë¡ í•™ìŠµ

resampler:
  vision_trainable_blocks: 0  # ì™„ì „ ë™ê²°

finetune:
  vision_trainable_blocks: 0  # ì™„ì „ ë™ê²°
```

**ì˜ë¯¸**:
- **Stageë³„ë¡œ ë‹¤ë¥´ê²Œ ì„¤ì •** ê°€ëŠ¥
- Vision stage: ë§ˆì§€ë§‰ 2ê°œ ë¸”ë¡ë§Œ í•™ìŠµ (VICReg ìµœì í™”)
- Resampler/Finetune: ì™„ì „ ë™ê²° (ì•ˆì •ì„±)

**ìƒìš© VLM**:
```yaml
# ì¼ë°˜ì ìœ¼ë¡œ
vision_encoder: frozen  # í•­ìƒ ë™ê²°
# ë˜ëŠ”
vision_encoder: trainable  # ì „ì²´ í•™ìŠµ
```

**ì°¨ì´ì **:
- âŒ ìƒìš© VLM: **ì „ë¶€ ë™ê²°** ë˜ëŠ” **ì „ë¶€ í•™ìŠµ**
- âœ… PanoLLaVA: **ë¶€ë¶„ í•™ìŠµ** + **Stageë³„ ì œì–´**

---

### 6. **Batch Size & Accumulation ì „ëµ** âš ï¸

**PanoLLaVA (default.yaml)**:
```yaml
vision:
  batch_size: 16               # í° ë°°ì¹˜
  accumulate_grad_batches: 2   # ì‹¤íš¨ ë°°ì¹˜: 32

resampler:
  batch_size: 1                # ì‘ì€ ë°°ì¹˜
  accumulate_grad_batches: 2   # ì‹¤íš¨ ë°°ì¹˜: 2

finetune:
  batch_size: 1                # ì‘ì€ ë°°ì¹˜
  accumulate_grad_batches: 2   # ì‹¤íš¨ ë°°ì¹˜: 2
```

**ìƒìš© VLM**:
```yaml
# ì¼ë°˜ì ìœ¼ë¡œ
batch_size: 64-256  # í° ë°°ì¹˜ (ë°ì´í„°ì„¼í„° í™˜ê²½)
accumulate_grad_batches: 1  # ì§ì ‘ ì—…ë°ì´íŠ¸
```

**ì°¨ì´ì **:
- âŒ ìƒìš© VLM: **í° ë°°ì¹˜** + **ê°•ë ¥í•œ í•˜ë“œì›¨ì–´**
- âš ï¸ PanoLLaVA: **ì‘ì€ ë°°ì¹˜** (resampler/finetune) â†’ **ë©”ëª¨ë¦¬ ì œì•½**

**ë¬¸ì œì **: resamplerì™€ finetuneì˜ batch_size=1ì€ **ë„ˆë¬´ ì‘ìŒ** â†’ ë¶ˆì•ˆì •

---

### 7. **ë°ì´í„°ì…‹ êµ¬ì„±** â­

**PanoLLaVA (default.yaml)**:
```yaml
vision:
  data:
    csv_train:
      - "data/quic360/train.csv"           # íŒŒë…¸ë¼ë§ˆ
      - "data/train_stanford_dummy_anno.csv"  # ì‹¤ë‚´ íŒŒë…¸ë¼ë§ˆ
      - "data/train_zind_dummy_anno.csv"      # ì‹¤ë‚´ íŒŒë…¸ë¼ë§ˆ
    csv_val:
      - "data/quic360/valid.csv"
```

**ì˜ë¯¸**:
- **ì—¬ëŸ¬ íŒŒë…¸ë¼ë§ˆ ë°ì´í„°ì…‹** í˜¼í•© í•™ìŠµ
- Stageë³„ë¡œ ë‹¤ë¥¸ ë°ì´í„° ì„¤ì • ê°€ëŠ¥

**ìƒìš© VLM**:
```yaml
data:
  train: ["large_mixed_dataset.json"]  # ìˆ˜ë°±ë§Œ ì¥
  # ì˜ˆ: LLaVA-558K, ShareGPT4V, etc.
```

**ì°¨ì´ì **:
- âŒ ìƒìš© VLM: **ì¼ë°˜ ì´ë¯¸ì§€** ëŒ€ê·œëª¨ ë°ì´í„°ì…‹
- âœ… PanoLLaVA: **íŒŒë…¸ë¼ë§ˆ ì „ìš©** ì†Œê·œëª¨ ë°ì´í„°ì…‹

---

## ğŸ“Š ì¢…í•© ë¹„êµí‘œ

| ì„¤ì • í•­ëª© | PanoLLaVA (default.yaml) | ìƒìš© VLM | ê³µí†µì /ì°¨ì´ì  |
|----------|-------------------------|----------|-------------|
| **Vision Encoder** | SigLIP2-SO400M | SigLIP/RICE | âœ… ë™ì¼ ê³„ì—´ |
| **Language Model** | Qwen3-0.6B | Qwen2/Llama | âœ… ë™ì¼ ê³„ì—´ |
| **Resampler** | BiMamba (5ê°€ì§€ ì„ íƒ) | ì„ í˜• í”„ë¡œì ì…˜ | âŒ êµ¬ì¡° ë‹¤ì–‘ì„± |
| **ì´ë¯¸ì§€ ì²˜ë¦¬** | anyres_e2p (íŒŒë…¸ë¼ë§ˆ) | resize/crop | âŒ ê¸°í•˜ ë³€í™˜ |
| **VICReg Loss** | âœ… (overlap) | âŒ | âŒ íŒŒë…¸ë¼ë§ˆ íŠ¹í™” |
| **Positional Encoding** | Multi-view PE | Single-view PE | âŒ ë‹¤ì¤‘ ë·° ìœ„ì¹˜ |
| **Vision í•™ìŠµ** | ë¶€ë¶„ í•™ìŠµ (ë¸”ë¡ 2ê°œ) | ì „ë¶€ ë™ê²°/í•™ìŠµ | âŒ ì„¸ë°€í•œ ì œì–´ |
| **LoRA** | âœ… (rank=32) | ëŒ€ë¶€ë¶„ âŒ | âœ… PEFT íŠ¸ë Œë“œ |
| **Multi-Stage** | 3ë‹¨ê³„ ëª…ì‹œì  | 2-3ë‹¨ê³„ | âœ… Progressive |
| **Batch Size** | 16 â†’ 1 â†’ 1 | 64-256 | âš ï¸ ë©”ëª¨ë¦¬ ì œì•½ |
| **ë°ì´í„°** | íŒŒë…¸ë¼ë§ˆ ì „ìš© | ì¼ë°˜ ì´ë¯¸ì§€ | âŒ ë„ë©”ì¸ íŠ¹í™” |

---

## ğŸ¯ í•µì‹¬ ê²°ë¡ 

### **ê³µí†µì  (í‘œì¤€ VLM ì„¤ê³„)**

1. âœ… **Vision Encoder**: SigLIP/CLIP ê³„ì—´ ì‚¬ìš©
2. âœ… **Language Model**: Transformer ê¸°ë°˜ LLM (Qwen/Llama)
3. âœ… **Multi-Stage Training**: ì ì§„ì  í•™ìŠµ ì „ëµ
4. âœ… **Flash Attention**: ë©”ëª¨ë¦¬ ìµœì í™”
5. âœ… **Small Model Support**: 0.5B-7B ë²”ìœ„

### **ì°¨ì´ì  (PanoLLaVA ë…ì°½ì„±)** â­

1. âŒ **íŒŒë…¸ë¼ë§ˆ íŠ¹í™” ì²˜ë¦¬**: anyres_e2p (E2P íƒ€ì¼í™”)
2. âŒ **VICReg Overlap Loss**: ì¸ì ‘ ë·° ì •ê·œí™”
3. âŒ **Resampler ë‹¤ì–‘ì„±**: 5ê°€ì§€ êµ¬ì¡° ì„ íƒ
4. âŒ **Multi-view Positional Encoding**: 360Â° ì—°ì†ì„±
5. âŒ **ì„¸ë°€í•œ Vision Encoder ì œì–´**: ë¸”ë¡ë³„ í•™ìŠµ/ë™ê²°
6. âš ï¸ **ì‘ì€ ë°°ì¹˜ í¬ê¸°**: ë©”ëª¨ë¦¬ ì œì•½ (ê°œì„  í•„ìš”)

---

## ğŸ’¡ ê°œì„  ì œì•ˆ

### 1. **Batch Size ì¦ê°€** (ì¤‘ìš”)

**í˜„ì¬ ë¬¸ì œ**:
```yaml
resampler:
  batch_size: 1  # âŒ ë„ˆë¬´ ì‘ìŒ
  
finetune:
  batch_size: 1  # âŒ ë„ˆë¬´ ì‘ìŒ
```

**ê¶Œì¥ ìˆ˜ì •**:
```yaml
resampler:
  batch_size: 4-8  # âœ… ì•ˆì •ì  í•™ìŠµ
  accumulate_grad_batches: 4  # ì‹¤íš¨ ë°°ì¹˜: 16-32
  
finetune:
  batch_size: 4-8  # âœ… ì•ˆì •ì  í•™ìŠµ
  accumulate_grad_batches: 4  # ì‹¤íš¨ ë°°ì¹˜: 16-32
```

### 2. **ë°ì´í„° ê²½ë¡œ í†µì¼**

**í˜„ì¬ ë¬¸ì œ**: 3ê³³ì— ì¤‘ë³µ ì •ì˜
```yaml
paths:
  csv_train: "data/quic360/train.csv"  # 1ë²ˆ
  
vision:
  data:
    csv_train: [...]  # 2ë²ˆ
    
data:
  train: [...]  # 3ë²ˆ
```

**ê¶Œì¥**: Stageë³„ ì„¤ì •ì„ ìš°ì„ í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ì œê±°

### 3. **Vision Encoder Fine-tuning ì „ëµ**

**í˜„ì¬ ì„¤ì •**: vision stageë§Œ 2ê°œ ë¸”ë¡ í•™ìŠµ
```yaml
vision:
  vision_trainable_blocks: 2  # âœ… OK
  
finetune:
  vision_trainable_blocks: 0  # âš ï¸ ê³ ë ¤: 2-4ë¡œ ì¦ê°€?
```

**ëŒ€ì•ˆ**: Finetune stageì—ì„œë„ ì¼ë¶€ ë¸”ë¡ í•™ìŠµ (end-to-end)

---

## ğŸ“š ì°¸ê³ : ìƒìš© VLM ì„¤ì • ì˜ˆì‹œ

### LLaVA-OneVision

```yaml
vision_encoder: rice-vit-large (300M)
language_model: Qwen2-4B
projection: linear (1024 â†’ 4000)
training_stages: [pretraining, SFT]
batch_size: 256
data: LLaVA-558K + custom
```

### Qwen2.5-VL

```yaml
vision_encoder: SigLIP-Large (427M)
language_model: Qwen2.5-7B
projection: linear (4096 â†’ 5120)
training_stages: [vision_pretrain, SFT]
batch_size: 128
data: ëŒ€ê·œëª¨ ì¼ë°˜ ì´ë¯¸ì§€
```

---

## ê²°ë¡ 

**PanoLLaVAì˜ default.yaml**ì€:
- âœ… **í‘œì¤€ VLM ì„¤ê³„ ì›ì¹™**ì„ ë”°ë¥´ë©´ì„œë„
- â­ **íŒŒë…¸ë¼ë§ˆ íŠ¹í™” ê¸°ëŠ¥**ì„ ì¶”ê°€í•œ **í•˜ì´ë¸Œë¦¬ë“œ ì„¤ê³„**
- âš ï¸ ì¼ë¶€ ì„¤ì • (batch_size) ê°œì„  í•„ìš”
- ğŸ¯ **360Â° ì´ë¯¸ì§€ ì´í•´**ì— ìµœì í™”ëœ ë…ì°½ì  ì•„í‚¤í…ì²˜
