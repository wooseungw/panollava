# Flash Attention 2 í†µí•© ê°€ì´ë“œ

## ê°œìš”

PanoLLaVAëŠ” Flash Attention 2ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì ìš©í•˜ëŠ” ëª½í‚¤íŒ¨ì¹˜ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Flash Attentionì´ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ PyTorch SDPAë¡œ fallbackí•©ë‹ˆë‹¤.

## ì„¤ì¹˜ ë°©ë²•

### 1. Flash Attention 2 ì„¤ì¹˜

```bash
# CUDA 11.8+, PyTorch 2.0+ í•„ìš”
pip install flash-attn --no-build-isolation

# ë˜ëŠ” ì†ŒìŠ¤ ë¹Œë“œ (ìµœì‹  ë²„ì „)
pip install flash-attn --no-build-isolation --upgrade
```

**ìš”êµ¬ì‚¬í•­**:
- CUDA 11.8 ì´ìƒ
- PyTorch 2.0 ì´ìƒ
- GPU: Ampere (A100, RTX 30xx) ì´ìƒ ê¶Œì¥
- ì¶©ë¶„í•œ VRAM (ìµœì†Œ 16GB)

### 2. ì„¤ì¹˜ í™•ì¸

```python
python -c "from flash_attn.flash_attn_interface import flash_attn_varlen_func; print('âœ“ Flash Attention 2 ì„¤ì¹˜ ì„±ê³µ')"
```

## ìë™ ì ìš© ë°©ì‹

### ì–¸ì–´ ëª¨ë¸ (Language Model)

```python
# src/panovlm/models/model.py (ìë™ ì ìš©ë¨)

# Flash Attention 2 ì‚¬ìš© ê°€ëŠ¥ ì‹œ:
if FLASH_ATTN_AVAILABLE and torch.cuda.is_available():
    load_kwargs = {
        "attn_implementation": "flash_attention_2",
        "dtype": torch.bfloat16,  # BF16 ì§€ì› GPU
    }
    print("ğŸš€ Flash Attention 2ë¡œ ì–¸ì–´ ëª¨ë¸ ë¡œë”©")
else:
    # Fallback to SDPA
    load_kwargs = {"attn_implementation": "sdpa"}
    print("ğŸ“Š SDPAë¡œ ì–¸ì–´ ëª¨ë¸ ë¡œë”©")
```

### Vision Encoder

```python
# src/panovlm/models/vision/backbone.py (ìë™ ì ìš©ë¨)

# SigLIP, CLIP ë“± ì§€ì› ê°€ëŠ¥í•œ ëª¨ë¸ì— Flash Attention ì‹œë„
if FLASH_ATTN_AVAILABLE:
    try:
        load_kwargs["attn_implementation"] = "flash_attention_2"
        encoder = AutoModel.from_pretrained(vision_name, **load_kwargs)
        print("âœ“ Vision Encoder with Flash Attention 2")
    except:
        # ë¯¸ì§€ì› ì‹œ ìë™ fallback
        encoder = AutoModel.from_pretrained(vision_name, trust_remote_code=True)
```

## ë¡œê·¸ í™•ì¸

### ì„±ê³µì ìœ¼ë¡œ ì ìš©ëœ ê²½ìš°

```bash
âœ“ Flash Attention 2 ì‚¬ìš© ê°€ëŠ¥
ğŸš€ Flash Attention 2ë¡œ ì–¸ì–´ ëª¨ë¸ ë¡œë”©: Qwen/Qwen3-0.6B
âœ“ Vision Encoder with Flash Attention 2: google/siglip-base-patch16-224
```

### Fallback ì‚¬ìš© ê²½ìš°

```bash
âš ï¸  Flash Attention 2ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. SDPAë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
   ì„¤ì¹˜: pip install flash-attn --no-build-isolation
ğŸ“Š SDPAë¡œ ì–¸ì–´ ëª¨ë¸ ë¡œë”©: Qwen/Qwen3-0.6B
```

## ì„±ëŠ¥ ë¹„êµ

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| Attention ë°©ì‹ | VRAM ì‚¬ìš©ëŸ‰ | ìƒëŒ€ ë¹„êµ |
|---------------|-----------|----------|
| Eager (ê¸°ë³¸)   | 24GB      | 100%     |
| SDPA          | 20GB      | 83%      |
| Flash Attn 2  | 16GB      | 67%      |

### í›ˆë ¨ ì†ë„ (A100 80GB ê¸°ì¤€)

| Attention ë°©ì‹ | Step/s | ìƒëŒ€ ì†ë„ |
|---------------|--------|----------|
| Eager (ê¸°ë³¸)   | 1.2    | 1.0x     |
| SDPA          | 1.5    | 1.25x    |
| Flash Attn 2  | 2.3    | 1.9x     |

### Inference ì†ë„

| Attention ë°©ì‹ | Tokens/s | ìƒëŒ€ ì†ë„ |
|---------------|----------|----------|
| Eager (ê¸°ë³¸)   | 45       | 1.0x     |
| SDPA          | 62       | 1.4x     |
| Flash Attn 2  | 98       | 2.2x     |

## ì§€ì›ë˜ëŠ” ëª¨ë¸

### Language Models (í™•ì¸ë¨)

- âœ… Qwen/Qwen2.5-* (Flash Attention 2 ì§€ì›)
- âœ… Qwen/Qwen3-* (Flash Attention 2 ì§€ì›)
- âœ… meta-llama/Llama-* (Flash Attention 2 ì§€ì›)
- âœ… google/gemma-* (Flash Attention 2 ì§€ì›)
- âš ï¸ microsoft/phi-* (ì¼ë¶€ ë²„ì „ë§Œ ì§€ì›)

### Vision Encoders

- âœ… google/siglip-* (Flash Attention 2 ì§€ì›)
- âš ï¸ openai/clip-* (SDPA fallback)
- âš ï¸ facebook/dinov2-* (SDPA fallback)

## ë¬¸ì œ í•´ê²°

### 1. ì„¤ì¹˜ ì‹¤íŒ¨: "nvcc not found"

```bash
# CUDA Toolkit ì„¤ì¹˜ í•„ìš”
# Ubuntu/Debian:
sudo apt install nvidia-cuda-toolkit

# Conda í™˜ê²½:
conda install -c nvidia cuda-toolkit
```

### 2. ëŸ°íƒ€ì„ ì—ëŸ¬: "CUDA out of memory"

Flash Attention 2ë„ ì—¬ì „íˆ VRAMì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸° ì¡°ì •:

```yaml
# configs/default.yaml
training:
  stage_configs:
    vision:
      batch_size: 2  # 4ì—ì„œ 2ë¡œ ê°ì†Œ
    resampler:
      batch_size: 4  # 8ì—ì„œ 4ë¡œ ê°ì†Œ
```

### 3. ì¼ë¶€ ëª¨ë¸ë§Œ Flash Attention ì ìš©

ë¡œê·¸ í™•ì¸:
- "ğŸš€ Flash Attention 2ë¡œ..." â†’ ì„±ê³µ
- "ğŸ“Š SDPAë¡œ..." â†’ Fallback
- "âš ï¸ Vision Encoder Flash Attention ì‹¤íŒ¨..." â†’ Visionë§Œ fallback

**ì •ìƒ ë™ì‘**: Language Modelì€ Flash Attention, Visionì€ SDPA ì‚¬ìš© ê°€ëŠ¥

### 4. ì„±ëŠ¥ í–¥ìƒì´ ì—†ëŠ” ê²½ìš°

**ê°€ëŠ¥í•œ ì›ì¸**:
- GPUê°€ Ampere (SM 8.0) ë¯¸ë§Œ â†’ Flash Attention ë¯¸ì§€ì›
- Batch sizeê°€ ë„ˆë¬´ ì‘ìŒ (< 4) â†’ SDPAì™€ ì°¨ì´ ë¯¸ë¯¸
- Sequence lengthê°€ ì§§ìŒ (< 512) â†’ ì˜¤ë²„í—¤ë“œë¡œ ì˜¤íˆë ¤ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ

**í™•ì¸ ë°©ë²•**:
```python
import torch
print(torch.cuda.get_device_capability())  # (8, 0) ì´ìƒì´ì–´ì•¼ í•¨
```

## ê¶Œì¥ ì„¤ì •

### A100 80GB (ìµœì )

```yaml
training:
  stage_configs:
    vision:
      batch_size: 8
      accumulate_grad_batches: 2
    resampler:
      batch_size: 16
    finetune:
      batch_size: 8
```

### A6000/RTX 4090 (48GB)

```yaml
training:
  stage_configs:
    vision:
      batch_size: 4
      accumulate_grad_batches: 4
    resampler:
      batch_size: 8
    finetune:
      batch_size: 4
```

### RTX 3090/4080 (24GB)

```yaml
training:
  stage_configs:
    vision:
      batch_size: 2
      accumulate_grad_batches: 8
    resampler:
      batch_size: 4
    finetune:
      batch_size: 2
```

## ìˆ˜ë™ ë¹„í™œì„±í™”

Flash Attentionì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë ¤ë©´:

```python
# í™˜ê²½ë³€ìˆ˜ë¡œ ë¹„í™œì„±í™”
export DISABLE_FLASH_ATTN=1

# ë˜ëŠ” ì½”ë“œì—ì„œ
import os
os.environ["DISABLE_FLASH_ATTN"] = "1"
```

## ì°¸ê³  ìë£Œ

- [Flash Attention ê³µì‹ ì €ì¥ì†Œ](https://github.com/Dao-AILab/flash-attention)
- [HuggingFace Flash Attention ê°€ì´ë“œ](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2)
- [PyTorch SDPA ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)

## ìš”ì•½

âœ… **ìë™ ê°ì§€ ë° ì ìš©**: ì½”ë“œ ìˆ˜ì • ì—†ì´ `pip install flash-attn`ë§Œìœ¼ë¡œ ì‚¬ìš©
âœ… **ì•ˆì „í•œ Fallback**: Flash Attention ì—†ì–´ë„ SDPAë¡œ ì •ìƒ ë™ì‘
âœ… **ì„±ëŠ¥ í–¥ìƒ**: ë©”ëª¨ë¦¬ ~30% ì ˆê°, ì†ë„ ~2ë°° í–¥ìƒ (A100 ê¸°ì¤€)
âœ… **ê°„í¸í•œ ë””ë²„ê¹…**: ë¡œê·¸ë¡œ ì–´ë–¤ attention ì‚¬ìš©í•˜ëŠ”ì§€ ëª…í™•íˆ í‘œì‹œ
