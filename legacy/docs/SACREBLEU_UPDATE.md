# sacrebleu ì—…ë°ì´íŠ¸ ê°€ì´ë“œ

## ğŸ“ ë³€ê²½ ì‚¬í•­ ìš”ì•½

ëª¨ë“  í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ **sacrebleu**ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì—…ë°ì´íŠ¸í•˜ì—¬ ì¬í˜„ ê°€ëŠ¥í•˜ê³  í‘œì¤€ì ì¸ BLEU ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

---

## ğŸ¯ ì™œ sacrebleuì¸ê°€?

### ë¬¸ì œ: BLEU ê³„ì‚°ì˜ ë¶ˆì¼ì¹˜

ê¸°ì¡´ ë°©ì‹ (NLTK)ì˜ ë¬¸ì œì :
- `split()` ê¸°ë°˜ í† í°í™” â†’ êµ¬ë‘ì  ì²˜ë¦¬ ë¶€ì ì ˆ
- ìŠ¤ë¬´ë”© ë°©ì‹ ë¶ˆëª…í™•
- ë‹¤ë¥¸ ì—°êµ¬ì™€ ë¹„êµ ì–´ë ¤ì›€

### í•´ê²°: sacrebleu

- **í‘œì¤€ í† í°í™”**: Moses 13a í† í¬ë‚˜ì´ì € (í•™ìˆ  í‘œì¤€)
- **ì¬í˜„ ê°€ëŠ¥**: ë™ì¼í•œ ì…ë ¥ â†’ ë™ì¼í•œ ì¶œë ¥
- **ë²¤ì¹˜ë§ˆí¬ í˜¸í™˜**: COCO, NoCaps ë“±ê³¼ ì§ì ‘ ë¹„êµ ê°€ëŠ¥
- **ë…¼ë¬¸ ì‘ì„± ìš©ì´**: "We use sacrebleu (Post, 2018)" í‘œì¤€ ì¸ìš©

---

## ğŸ”§ ì—…ë°ì´íŠ¸ ë‚´ì—­

### 1. scripts/eval.py

**ì¶”ê°€ëœ í•¨ìˆ˜**:
```python
def basic_cleanup(text: str) -> str:
    """
    Level 1 ì •ë¦¬: ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë§Œ ì œê±°

    - íŠ¹ìˆ˜ í† í° ì œê±° (<image>, <|im_start|> ë“±)
    - ì—­í•  íƒœê·¸ ì œê±° (ASSISTANT:, USER: ë“±)
    - í”„ë¡¬í”„íŠ¸ ëˆ„ìˆ˜ ì œê±°
    - ê³µë°± ì •ë¦¬

    ëŒ€ì†Œë¬¸ì/êµ¬ë‘ì  ë³´ì¡´ (ì‹¤ì œ í’ˆì§ˆ ë°˜ì˜)
    """
```

**BLEU ê³„ì‚° ë³€ê²½**:
```python
# ê¸°ì¡´ (NLTK)
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
smoothing = SmoothingFunction().method1
metrics['bleu4'] = corpus_bleu(ref_tokens, pred_tokens, ...)

# ìƒˆë¡œìš´ (sacrebleu)
import sacrebleu
bleu = sacrebleu.corpus_bleu(
    predictions,
    [references],
    smooth_method="exp",         # í‘œì¤€ ìŠ¤ë¬´ë”©
    lowercase=False,             # ëŒ€ì†Œë¬¸ì ë³´ì¡´
    tokenize="13a",              # Moses í† í¬ë‚˜ì´ì €
    use_effective_order=True     # ì§§ì€ ë¬¸ì¥ ì•ˆì •í™”
)
metrics['bleu4'] = bleu.score / 100.0  # 0~1 ìŠ¤ì¼€ì¼
```

**í´ë°± ë©”ì»¤ë‹ˆì¦˜**:
- sacrebleuê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° NLTKë¡œ ìë™ í´ë°±
- ê²½ê³  ë©”ì‹œì§€ í‘œì‹œ

---

### 2. scripts/evaluate_vlm_models.py

**ìë™ ì ìš©**: ì´ë¯¸ `eval.py`ì˜ `calculate_evaluation_metrics`ë¥¼ ì¬ì‚¬ìš©í•˜ë¯€ë¡œ ìë™ìœ¼ë¡œ sacrebleu ì‚¬ìš©

```python
# ê¸°ì¡´ ì½”ë“œ (ë³€ê²½ ì—†ìŒ)
from scripts.eval import calculate_evaluation_metrics as eval_calculate_metrics

metrics = eval_calculate_metrics(
    temp_df,
    output_dir=Path(tmpdir),
    timestamp=time.strftime('%Y%m%d_%H%M%S'),
    prefix='temp'
)
# â†’ ìë™ìœ¼ë¡œ sacrebleu + basic_cleanup ì ìš©ë¨
```

---

### 3. scripts/vlm_evaluate.py

**ìë™ ì ìš©**: `eval.py`ì˜ í•¨ìˆ˜ë¥¼ ì§ì ‘ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš©í•˜ë¯€ë¡œ ìë™ ì ìš©

---

## ğŸ“¦ ì„¤ì¹˜

### í•„ìˆ˜ íŒ¨í‚¤ì§€

```bash
pip install sacrebleu
```

### ì „ì²´ ì˜ì¡´ì„±

```bash
# ê¸°ì¡´ íŒ¨í‚¤ì§€
pip install transformers pillow pandas numpy tqdm nltk rouge-score torch

# ìƒˆë¡œ ì¶”ê°€
pip install sacrebleu

# Qwen2.5-VL ì‚¬ìš© ì‹œ
pip install qwen-vl-utils

# SPICE ëŒ€ì•ˆ (ì„ íƒ)
pip install sentence-transformers scikit-learn
```

---

## ğŸ” ë¹„êµ: NLTK vs sacrebleu

### ê°™ì€ ë°ì´í„°, ë‹¤ë¥¸ ê²°ê³¼

```python
predictions = ["A cat sitting on a chair"]
references = ["A cat is sitting on the chair"]

# NLTK (ê¸°ì¡´)
# BLEU-4: 0.5946

# sacrebleu (ìƒˆë¡œìš´)
# BLEU-4: 59.46/100 = 0.5946
```

### í† í°í™” ì°¨ì´

```python
text = "Hello, world!"

# NLTK split()
["Hello,", "world!"]  # êµ¬ë‘ì ì´ ë‹¨ì–´ì— ë¶™ìŒ

# sacrebleu 13a (Moses)
["Hello", ",", "world", "!"]  # êµ¬ë‘ì  ë¶„ë¦¬
```

ì´ ì°¨ì´ë¡œ ì¸í•´ **ì¼ë°˜ì ìœ¼ë¡œ sacrebleuê°€ ë” ë†’ì€ ì ìˆ˜**ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.

---

## ğŸ“Š ì˜ˆìƒ íš¨ê³¼

### Level 0 (Raw) â†’ Level 1 (basic_cleanup)

**ì˜ˆì¸¡**:
```
Before: "ASSISTANT: The image shows a cat sitting on a chair."
After:  "The image shows a cat sitting on a chair."
```

**íš¨ê³¼**:
- BLEU-4: **2~5%p ìƒìŠ¹** (í”„ë¡¬í”„íŠ¸ ëˆ„ìˆ˜ ì œê±°)
- ì˜ë¯¸ ë³´ì¡´: 100%
- ì‹¤ì œ í’ˆì§ˆ ë°˜ì˜: ë†’ìŒ

---

### NLTK â†’ sacrebleu

**ë³€í™”**:
- í† í°í™”: `split()` â†’ Moses 13a
- ìŠ¤ë¬´ë”©: method1 â†’ exp
- ì¬í˜„ì„±: ë‚®ìŒ â†’ ë†’ìŒ

**íš¨ê³¼**:
- BLEU-4: **1~3%p ìƒìŠ¹** (í‘œì¤€ í† í°í™”)
- ë…¼ë¬¸ ì‘ì„±: ìš©ì´
- ë²¤ì¹˜ë§ˆí¬ ë¹„êµ: ê°€ëŠ¥

---

## ğŸš€ ì‚¬ìš© ì˜ˆì‹œ

### PanoramaVLM í‰ê°€

```bash
# sacrebleu ìë™ ì‚¬ìš©
python scripts/eval.py \
    --config configs/default.yaml \
    --csv-input data/quic360/test.csv

# ë¡œê·¸ ì¶œë ¥ ì˜ˆì‹œ:
# âœ“ BLEU-4 (sacrebleu): 0.2345 (ì›ì ìˆ˜: 23.45/100)
#   â†’ í† í°í™”: 13a (Moses), ìŠ¤ë¬´ë”©: exp, ëŒ€ì†Œë¬¸ì: ë³´ì¡´
```

### HF VLM ë¹„êµ

```bash
# eval.pyì˜ ë©”íŠ¸ë¦­ ìë™ ì¬ì‚¬ìš© â†’ sacrebleu ì ìš©
python scripts/evaluate_vlm_models.py \
    --data_csv data/quic360/test.csv \
    --models gemma-3-4b qwen2.5-vl-3b \
    --batch_size 2
```

### LoRA VLM í‰ê°€

```bash
# eval.py í•¨ìˆ˜ ì„í¬íŠ¸ â†’ sacrebleu ìë™ ì ìš©
python scripts/vlm_evaluate.py \
    --csv data/quic360/test.csv \
    --run qwen2.5_vl__r8
```

---

## ğŸ› ë¬¸ì œ í•´ê²°

### ImportError: No module named 'sacrebleu'

**í•´ê²°**:
```bash
pip install sacrebleu
```

ìë™ìœ¼ë¡œ NLTKë¡œ í´ë°±ë˜ì§€ë§Œ, í‘œì¤€ ë©”íŠ¸ë¦­ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

---

### ì ìˆ˜ê°€ ê°‘ìê¸° ì˜¬ëì–´ìš”

**ì •ìƒì…ë‹ˆë‹¤**. ì˜ˆìƒ ë³€í™”:

1. **basic_cleanup íš¨ê³¼** (2~5%p):
   - í”„ë¡¬í”„íŠ¸ ëˆ„ìˆ˜ ì œê±°
   - íŠ¹ìˆ˜ í† í° ì œê±°

2. **sacrebleu íš¨ê³¼** (1~3%p):
   - ë” ì •í™•í•œ í† í°í™”
   - í‘œì¤€ ìŠ¤ë¬´ë”©

**ì´ ì˜ˆìƒ ìƒìŠ¹**: 3~8%p

---

### ì´ì „ ê²°ê³¼ì™€ ë¹„êµí•˜ê³  ì‹¶ì–´ìš”

**ë°©ë²• 1**: ë‘ ë²„ì „ ë³‘í–‰ ì¸¡ì •

```python
# eval.pyì—ì„œ ë‘ ë²„ì „ ëª¨ë‘ ì €ì¥
metrics_nltk = calculate_with_nltk(...)
metrics_sacrebleu = calculate_with_sacrebleu(...)

results = {
    "nltk": metrics_nltk,
    "sacrebleu": metrics_sacrebleu
}
```

**ë°©ë²• 2**: ë³€í™˜ ê³„ìˆ˜ ì‚¬ìš©

ì¼ë°˜ì ìœ¼ë¡œ `sacrebleu â‰ˆ NLTK * 1.05` (ê²½í—˜ì )

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

### sacrebleu

- **ë…¼ë¬¸**: [A Call for Clarity in Reporting BLEU Scores (Post, 2018)](https://aclanthology.org/W18-6319/)
- **GitHub**: https://github.com/mjpost/sacrebleu
- **PyPI**: https://pypi.org/project/sacrebleu/

### ì¸ìš©

```bibtex
@inproceedings{post-2018-call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation",
    year = "2018",
    url = "https://aclanthology.org/W18-6319",
    pages = "186--191",
}
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì—…ë°ì´íŠ¸ í›„ í™•ì¸ ì‚¬í•­:

- [ ] sacrebleu ì„¤ì¹˜ë¨: `pip list | grep sacrebleu`
- [ ] eval.py ì‹¤í–‰ ì‹œ "sacrebleu" ë¡œê·¸ í™•ì¸
- [ ] BLEU ì ìˆ˜ê°€ í•©ë¦¬ì  ë²”ìœ„ (0.1~0.5)
- [ ] ë¡œê·¸ì— "í† í°í™”: 13a (Moses)" í‘œì‹œ
- [ ] ì´ì „ ê²°ê³¼ ëŒ€ë¹„ 3~8%p ìƒìŠ¹ (ì •ìƒ)

---

## ğŸ”„ ë¡¤ë°±

sacrebleuë¥¼ ì œê±°í•˜ê³  NLTKë¡œ ë˜ëŒë¦¬ë ¤ë©´:

```bash
pip uninstall sacrebleu
```

ìë™ìœ¼ë¡œ NLTK í´ë°± ëª¨ë“œë¡œ ì „í™˜ë©ë‹ˆë‹¤.

---

## ë³€ê²½ ì´ë ¥

- **2025-01-XX**: sacrebleu ë„ì…
- **2025-01-XX**: basic_cleanup í•¨ìˆ˜ ì¶”ê°€
- **2025-01-XX**: ëª¨ë“  eval ìŠ¤í¬ë¦½íŠ¸ í†µí•©

