# Vision Encoder ì‹œê°í™” ê°€ì´ë“œ

ì´ ê°€ì´ë“œëŠ” `dino.py`ë¥¼ ì‚¬ìš©í•˜ì—¬ vision encoderì˜ hidden statesë¥¼ ì‹œê°í™”í•˜ê³  ë¶„ì„í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ëª©ì°¨
1. [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
2. [ìƒì„¸ ì‚¬ìš©ë²•](#ìƒì„¸-ì‚¬ìš©ë²•)
3. [ì‹œê°í™” ê²°ê³¼ í•´ì„](#ì‹œê°í™”-ê²°ê³¼-í•´ì„)
4. [Python API ì‚¬ìš©](#python-api-ì‚¬ìš©)
5. [ê³ ê¸‰ ì˜µì…˜](#ê³ ê¸‰-ì˜µì…˜)

---

## ë¹ ë¥¸ ì‹œì‘

### 1. ê¸°ë³¸ ì‚¬ìš©ë²•

```bash
# SigLIP ëª¨ë¸ë¡œ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ ì‹œê°í™”
python scripts/visualize_vision_encoder.py \
    --image data/quic360/train/pano_001.jpg \
    --vision_model google/siglip-base-patch16-224 \
    --crop_strategy e2p \
    --output_dir results/vision_viz/siglip_e2p
```

### 2. DINOv2 ëª¨ë¸ ì‚¬ìš©

```bash
# DINOv2ë¡œ ì‹œê°í™”
python scripts/visualize_vision_encoder.py \
    --image data/quic360/train/pano_001.jpg \
    --vision_model facebook/dinov2-base \
    --crop_strategy anyres_e2p \
    --output_dir results/vision_viz/dinov2_anyres
```

### 3. CLIP ëª¨ë¸ ì‚¬ìš©

```bash
# CLIPìœ¼ë¡œ ì‹œê°í™”
python scripts/visualize_vision_encoder.py \
    --image data/quic360/train/pano_001.jpg \
    --vision_model openai/clip-vit-base-patch16 \
    --crop_strategy cubemap \
    --output_dir results/vision_viz/clip_cubemap
```

---

## ìƒì„¸ ì‚¬ìš©ë²•

### ëª…ë ¹ì¤„ ì¸ì

| ì¸ì | í•„ìˆ˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|------|------|--------|------|
| `--image` | âœ… | - | ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ |
| `--vision_model` | âŒ | `google/siglip-base-patch16-224` | Vision encoder ëª¨ë¸ |
| `--crop_strategy` | âŒ | `e2p` | ì´ë¯¸ì§€ crop ì „ëµ |
| `--image_size` | âŒ | `224` | ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° |
| `--output_dir` | âŒ | `results/vision_viz` | ì¶œë ¥ ë””ë ‰í† ë¦¬ |
| `--n_components` | âŒ | `3` | PCA ì£¼ì„±ë¶„ ê°œìˆ˜ |
| `--no_cls_token` | âŒ | - | CLS í† í° ì œê±° ì•ˆ í•¨ |
| `--bg_removal` | âŒ | `threshold` | ë°°ê²½ ì œê±° ë°©ë²• |
| `--no_similarity` | âŒ | - | ìœ ì‚¬ë„ ë¶„ì„ ê±´ë„ˆë›°ê¸° |
| `--device` | âŒ | `auto` | ë””ë°”ì´ìŠ¤ (auto/cuda/cpu) |

### Crop ì „ëµ

- **`e2p`**: Equirectangular-to-Perspective (ì¤‘ì•™ 90Â° FOV)
- **`anyres_e2p`**: AnyRes ìŠ¤íƒ€ì¼ ERP íƒ€ì¼ë§ (yaw/pitch ê·¸ë¦¬ë“œ)
- **`cubemap`**: 4ë©´ íë¸Œë§µ (ì•/ë’¤/ì¢Œ/ìš°)
- **`sliding_window`**: ìˆ˜í‰ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
- **`anyres`**: ê·¸ë¦¬ë“œ ê¸°ë°˜ íŒ¨ì¹˜
- **`resize`**: ë‹¨ìˆœ ë¦¬ì‚¬ì´ì¦ˆ (ë² ì´ìŠ¤ë¼ì¸)

### ë°°ê²½ ì œê±° ë°©ë²•

- **`threshold`**: ì²« ë²ˆì§¸ PCA ì„±ë¶„ì— ì„ê³„ê°’ ì ìš© (ê¸°ë³¸ê°’)
- **`remove_first_pc`**: ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ ì™„ì „ ì œê±°
- **`outlier_removal`**: Mahalanobis ê±°ë¦¬ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±°
- **`none`**: ë°°ê²½ ì œê±° ì•ˆ í•¨

---

## ì‹œê°í™” ê²°ê³¼ í•´ì„

### ì¶œë ¥ íŒŒì¼

ì‹¤í–‰ í›„ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

```
results/vision_viz/
â”œâ”€â”€ pca_visualization.png       # PCA RGB ì‹œê°í™”
â”œâ”€â”€ original_views.png          # ì›ë³¸ view ì´ë¯¸ì§€
â””â”€â”€ similarity_analysis.txt     # ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼
```

### PCA ì‹œê°í™” ì´í•´í•˜ê¸°

**`pca_visualization.png`**:
- ê° viewì˜ hidden statesë¥¼ 3ê°œ ì£¼ì„±ë¶„ìœ¼ë¡œ ì••ì¶•í•˜ì—¬ RGBë¡œ í‘œí˜„
- **ë¹¨ê°• ì±„ë„**: ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ (ê°€ì¥ í° ë¶„ì‚°)
- **ì´ˆë¡ ì±„ë„**: ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„
- **íŒŒë‘ ì±„ë„**: ì„¸ ë²ˆì§¸ ì£¼ì„±ë¶„

**ìƒ‰ìƒ íŒ¨í„´ í•´ì„**:
- ë¹„ìŠ·í•œ ìƒ‰ìƒ = ë¹„ìŠ·í•œ semantic features
- ìƒ‰ìƒ ëŒ€ë¹„ = feature spaceì—ì„œì˜ ì°¨ì´
- ë°°ê²½(ë‹¨ì¡°ë¡œìš´ ì˜ì—­) vs ì „ê²½(ë³µì¡í•œ ì˜ì—­) êµ¬ë¶„ ê°€ëŠ¥

### ìœ ì‚¬ë„ ë¶„ì„ ì§€í‘œ

**í† í° ë ˆë²¨ ìœ ì‚¬ë„** (`similarity_analysis.txt`):
- **MSE (Mean Squared Error)**: ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
- **Cosine**: ë†’ì„ìˆ˜ë¡ ìœ ì‚¬ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
- **Hungarian**: ìµœì  ë§¤ì¹­ í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
- **CKA (Centered Kernel Alignment)**: í‘œí˜„ ê³µê°„ ì •ë ¬ ì¸¡ì •

**PCA-RGB ì´ë¯¸ì§€ ìœ ì‚¬ë„**:
- **MSE**: í”½ì…€ ë ˆë²¨ ì°¨ì´
- **SSIM**: êµ¬ì¡°ì  ìœ ì‚¬ë„ (0~1, ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
- **LPIPS**: ì§€ê°ì  ìœ ì‚¬ë„ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)

---

## Python API ì‚¬ìš©

### ê¸°ë³¸ ì‚¬ìš© ì˜ˆì‹œ

```python
from panovlm.evaluation.dino import DinoVisualizer
from panovlm.processors.image import PanoramaImageProcessor
from transformers import AutoModel, AutoImageProcessor
from PIL import Image
import torch

# 1. ì´ë¯¸ì§€ ë¡œë”© ë° ì „ì²˜ë¦¬
image = Image.open("data/quic360/train/pano_001.jpg")

vision_model = AutoModel.from_pretrained(
    "google/siglip-base-patch16-224"
).cuda()
vision_model.eval()

hf_processor = AutoImageProcessor.from_pretrained(
    "google/siglip-base-patch16-224"
)

pano_processor = PanoramaImageProcessor(
    crop_strategy="e2p",
    image_size=224,
    use_vision_processor=True,
    vision_processor=hf_processor
)

pixel_values = pano_processor(image).unsqueeze(0)  # [V, C, H, W]

# 2. Hidden states ì¶”ì¶œ
hidden_states_list = []
with torch.no_grad():
    for i in range(pixel_values.shape[0]):
        view = pixel_values[i:i+1].cuda()
        outputs = vision_model(view, output_hidden_states=True)
        last_hidden = outputs.hidden_states[-1]  # [1, seq_len, hidden_dim]
        hidden_states_list.append(last_hidden.cpu().numpy())

# 3. DinoVisualizerë¡œ ì‹œê°í™”
visualizer = DinoVisualizer(
    hidden_states_list=hidden_states_list,
    remove_cls_token=True
)

# 4. PCA í•™ìŠµ
visualizer.fit_pca(
    n_components=3,
    use_background_removal=True,
    bg_removal_method="threshold"
)

# 5. ê²°ê³¼ í”Œë¡¯
visualizer.plot_pca_results(
    titles=[f'View {i+1}' for i in range(len(hidden_states_list))],
    save_path="results/my_pca_viz.png"
)

# 6. ìœ ì‚¬ë„ ë¶„ì„
pairs = [(0, 1), (1, 2), (2, 3)]
token_sim = visualizer.get_token_similarity(pairs=pairs)
pca_sim = visualizer.get_pca_similarity(pairs=pairs)

print("í† í° ë ˆë²¨ ì½”ì‚¬ì¸ ìœ ì‚¬ë„:", token_sim['cosine'])
print("PCA SSIM:", pca_sim['ssim'])
```

### ê³ ê¸‰: ì»¤ìŠ¤í…€ ë¶„ì„

```python
# íŠ¹ì • ë ˆì´ì–´ì˜ hidden states ì¶”ì¶œ
layer_idx = 6  # ì¤‘ê°„ ë ˆì´ì–´

with torch.no_grad():
    for view in pixel_values:
        outputs = vision_model(view.unsqueeze(0).cuda(), output_hidden_states=True)
        layer_hidden = outputs.hidden_states[layer_idx]
        hidden_states_list.append(layer_hidden.cpu().numpy())

# ë” ë§ì€ ì£¼ì„±ë¶„ ì‚¬ìš©
visualizer.fit_pca(n_components=10)

# PCA ì„¤ëª… ë¶„ì‚° í™•ì¸
explained_var = visualizer.pca_model.explained_variance_ratio_
print(f"ìƒìœ„ 10ê°œ ì£¼ì„±ë¶„ ì„¤ëª… ë¶„ì‚°: {explained_var.sum():.2%}")
```

---

## ê³ ê¸‰ ì˜µì…˜

### 1. ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ

```bash
# ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±: compare_models.sh
for model in \
    "google/siglip-base-patch16-224" \
    "facebook/dinov2-base" \
    "openai/clip-vit-base-patch16"
do
    model_name=$(echo $model | tr '/' '_')
    python scripts/visualize_vision_encoder.py \
        --image data/quic360/train/pano_001.jpg \
        --vision_model $model \
        --crop_strategy e2p \
        --output_dir results/vision_viz/$model_name
done
```

### 2. ë°°ì¹˜ ì²˜ë¦¬

```python
from pathlib import Path
import subprocess

image_dir = Path("data/quic360/train")
output_base = Path("results/vision_viz_batch")

for img_path in image_dir.glob("*.jpg")[:10]:  # ì²˜ìŒ 10ê°œë§Œ
    output_dir = output_base / img_path.stem
    
    subprocess.run([
        "python", "scripts/visualize_vision_encoder.py",
        "--image", str(img_path),
        "--vision_model", "google/siglip-base-patch16-224",
        "--output_dir", str(output_dir)
    ])
```

### 3. ë‹¤ì–‘í•œ crop ì „ëµ ë¹„êµ

```bash
for strategy in e2p anyres_e2p cubemap sliding_window
do
    python scripts/visualize_vision_encoder.py \
        --image data/quic360/train/pano_001.jpg \
        --crop_strategy $strategy \
        --output_dir results/vision_viz/strategy_$strategy
done
```

### 4. Warp ê¸°ë°˜ ìœ ì‚¬ë„ ë¶„ì„ (ê³ ê¸‰)

```python
from panovlm.evaluation.dino import compute_overlap_consistency_score

# ERP ì¢Œí‘œ ìƒì„± ë° warp ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
# (dino.pyì˜ compute_overlap_consistency_score í•¨ìˆ˜ ì°¸ì¡°)

hidden_tensor = torch.from_numpy(hidden_states_list[0])  # [1, seq_len, dim]
ocs_result = compute_overlap_consistency_score(
    hidden_tensor,
    yaw_offset_deg=45.0,  # 45ë„ íšŒì „
    overlap_ratio=0.5
)

print(f"Overlap Consistency Score: {ocs_result['ocs']:.4f}")
print(f"Residual Mean: {ocs_result['residual_mean']:.4f}")
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### LPIPS ì‚¬ìš© ë¶ˆê°€

```bash
# LPIPS ì„¤ì¹˜
pip install lpips

# ë˜ëŠ” LPIPS ì—†ì´ ì‹¤í–‰ (ìë™ìœ¼ë¡œ ê±´ë„ˆëœ€)
python scripts/visualize_vision_encoder.py --image ... --no_similarity
```

### CUDA Out of Memory

```bash
# CPU ì‚¬ìš©
python scripts/visualize_vision_encoder.py --image ... --device cpu

# ë˜ëŠ” ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸°
python scripts/visualize_vision_encoder.py --image ... --image_size 128
```

### í•œê¸€ í°íŠ¸ ê¹¨ì§

```python
# dino.pyì˜ _setup_korean_font() í•¨ìˆ˜ê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
# í•„ìš”ì‹œ ì‹œìŠ¤í…œì— í•œê¸€ í°íŠ¸ ì„¤ì¹˜:
# Ubuntu: sudo apt-get install fonts-nanum
# macOS: ì‹œìŠ¤í…œ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
```

---

## ì°¸ê³  ìë£Œ

- **DINOv2 ë…¼ë¬¸**: [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)
- **PCA ì‹œê°í™”**: Vision Transformerì˜ ì£¼ì„±ë¶„ì´ semantic featuresë¥¼ ì˜ í¬ì°©í•¨
- **CKA ìœ ì‚¬ë„**: [Similarity of Neural Network Representations Revisited](https://arxiv.org/abs/1905.00414)

---

## ì˜ˆì‹œ ê²°ê³¼

### SigLIP (E2P strategy)
```
ğŸ“Š PCA ë¶„ì„ ê²°ê³¼
ì£¼ì„±ë¶„ 1 ì„¤ëª… ë¶„ì‚°: 18.45%
ì£¼ì„±ë¶„ 2 ì„¤ëª… ë¶„ì‚°: 12.32%
ì£¼ì„±ë¶„ 3 ì„¤ëª… ë¶„ì‚°: 8.76%
ì´ ì„¤ëª… ë¶„ì‚° (ìƒìœ„ 3ê°œ): 39.53%

ğŸ” í† í° ë ˆë²¨ ìœ ì‚¬ë„:
  Pair (0, 1): Cosine=0.8234
  í‰ê· : 0.8234

ğŸ¨ PCA-RGB ì´ë¯¸ì§€ ìœ ì‚¬ë„:
  Pair (0, 1): SSIM=0.7456
  í‰ê· : 0.7456
```

### DINOv2 (AnyRes ERP strategy)
```
ğŸ“Š PCA ë¶„ì„ ê²°ê³¼
ì£¼ì„±ë¶„ 1 ì„¤ëª… ë¶„ì‚°: 22.14%
ì£¼ì„±ë¶„ 2 ì„¤ëª… ë¶„ì‚°: 15.67%
ì£¼ì„±ë¶„ 3 ì„¤ëª… ë¶„ì‚°: 11.23%
ì´ ì„¤ëª… ë¶„ì‚° (ìƒìœ„ 3ê°œ): 49.04%

ğŸ” í† í° ë ˆë²¨ ìœ ì‚¬ë„:
  ì—¬ëŸ¬ view ê°„ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„: 0.7892
```

---

**Happy Visualizing! ğŸ¨ğŸ”**
