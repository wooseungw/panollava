# VLM ëª¨ë¸ ì—…ë°ì´íŠ¸ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” `scripts/evaluate_vlm_models.py`ì˜ ì§€ì› ëª¨ë¸ ë³€ê²½ ì‚¬í•­ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“ ë³€ê²½ ì‚¬í•­ ìš”ì•½

### ì¶”ê°€ëœ ëª¨ë¸

1. **Gemma 3 4B** (`gemma-3-4b`)
   - Model ID: `google/gemma-3-4b-it`
   - í¬ê¸°: 4B íŒŒë¼ë¯¸í„°
   - íŠ¹ì§•: Chat template ì‚¬ìš©, ìµœì‹  Google VLM
   - ì‚¬ìš© ì˜ˆì‹œ:
     ```bash
     python scripts/evaluate_vlm_models.py \
         --data_csv data/quic360/test.csv \
         --models gemma-3-4b \
         --batch_size 2
     ```

2. **Qwen2.5-VL 3B** (`qwen2.5-vl-3b`)
   - Model ID: `Qwen/Qwen2.5-VL-3B-Instruct`
   - í¬ê¸°: 3B íŒŒë¼ë¯¸í„°
   - íŠ¹ì§•: Chat template + `qwen_vl_utils` í•„ìš”
   - **ì¶”ê°€ ì„¤ì¹˜ í•„ìš”**:
     ```bash
     pip install qwen-vl-utils
     ```
   - ì‚¬ìš© ì˜ˆì‹œ:
     ```bash
     python scripts/evaluate_vlm_models.py \
         --data_csv data/quic360/test.csv \
         --models qwen2.5-vl-3b \
         --batch_size 2
     ```

### ì œê±°ëœ ëª¨ë¸

1. **Qwen-VL-Chat** (`qwen-vl-chat`)
   - ì´ìœ : Qwen2.5-VLë¡œ ì—…ê·¸ë ˆì´ë“œ

2. **Qwen2-VL-2B** (`qwen2-vl-2b`)
   - ì´ìœ : Qwen2.5-VL-3Bë¡œ ì—…ê·¸ë ˆì´ë“œ

3. **CogVLM2-Llama3-Chat-19B** (`cogvlm2-llama3-chat-19b`)
   - ì´ìœ : ëª¨ë¸ í¬ê¸°ê°€ ë„ˆë¬´ í¼ (19B), ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³ ë ¤

## ğŸ”§ ê¸°ìˆ ì  ë³€ê²½ ì‚¬í•­

### 1. Chat Template ì§€ì› ì¶”ê°€

ìƒˆë¡œìš´ ëª¨ë¸ë“¤(Gemma3, Qwen2.5-VL)ì€ ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ëŒ€ì‹  **chat template**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### ëª¨ë¸ ì •ì˜ í˜•ì‹:
```python
"gemma-3-4b": {
    "model_id": "google/gemma-3-4b-it",
    "processor_id": "google/gemma-3-4b-it",
    "model_class": "Gemma3ForConditionalGeneration",
    "processor_class": "AutoProcessor",
    "use_chat_template": True,  # Chat template ì‚¬ìš©
},

"qwen2.5-vl-3b": {
    "model_id": "Qwen/Qwen2.5-VL-3B-Instruct",
    "processor_id": "Qwen/Qwen2.5-VL-3B-Instruct",
    "model_class": "Qwen2_5_VLForConditionalGeneration",
    "processor_class": "AutoProcessor",
    "use_chat_template": True,
    "requires_vision_utils": True,  # qwen_vl_utils í•„ìš”
},
```

#### Chat Template ì²˜ë¦¬ ë¡œì§:
```python
# Gemma3ì˜ ê²½ìš°
messages = [{
    "role": "user",
    "content": [
        {"type": "image", "image": img},
        {"type": "text", "text": instruction}
    ]
}]
inputs = processor.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
)

# Qwen2.5-VLì˜ ê²½ìš°
from qwen_vl_utils import process_vision_info

text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
```

### 2. ëª¨ë¸ í´ë˜ìŠ¤ ì¶”ê°€

- `Gemma3ForConditionalGeneration` (transformers)
- `Qwen2_5_VLForConditionalGeneration` (transformers)

### 3. í‰ê°€ ë©”íŠ¸ë¦­ í†µì¼

ëª¨ë“  ëª¨ë¸ì´ ë™ì¼í•œ í‰ê°€ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ë„ë¡ `scripts/eval.py`ì˜ `calculate_evaluation_metrics` í•¨ìˆ˜ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.

## ğŸ“Š í˜„ì¬ ì§€ì› ëª¨ë¸ ëª©ë¡

| ëª¨ë¸ ID | HuggingFace Model | í¬ê¸° | íŠ¹ì§• |
|---------|------------------|------|------|
| `llava-1.5-7b` | llava-hf/llava-1.5-7b-hf | 7B | ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ |
| `llava-1.6-mistral-7b` | llava-hf/llava-v1.6-mistral-7b-hf | 7B | ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ |
| `blip2-opt-2.7b` | Salesforce/blip2-opt-2.7b | 2.7B | ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ |
| `blip2-flan-t5-xl` | Salesforce/blip2-flan-t5-xl | 3B | ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ |
| `instructblip-vicuna-7b` | Salesforce/instructblip-vicuna-7b | 7B | ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ |
| `qwen2.5-vl-3b` | Qwen/Qwen2.5-VL-3B-Instruct | 3B | Chat template + vision utils |
| `internvl2-2b` | OpenGVLab/InternVL2-2B | 2B | ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ |
| `gemma-3-4b` | google/gemma-3-4b-it | 4B | Chat template |

## ğŸš€ ì‚¬ìš© ì˜ˆì‹œ

### ëª¨ë“  ê²½ëŸ‰ ëª¨ë¸ í‰ê°€
```bash
python scripts/evaluate_vlm_models.py \
    --data_csv data/quic360/test.csv \
    --models blip2-opt-2.7b internvl2-2b qwen2.5-vl-3b gemma-3-4b \
    --batch_size 2 \
    --output_dir results
```

### ìµœì‹  ëª¨ë¸ë§Œ í‰ê°€
```bash
python scripts/evaluate_vlm_models.py \
    --data_csv data/quic360/test.csv \
    --models qwen2.5-vl-3b gemma-3-4b \
    --batch_size 1 \
    --max_samples 50
```

### Ablation study ì‹¤í–‰
```bash
bash scripts/run_vlm_ablation.sh
```

## ğŸ› ë¬¸ì œ í•´ê²°

### Qwen2.5-VL ê´€ë ¨ ì˜¤ë¥˜

**ì—ëŸ¬**: `ModuleNotFoundError: No module named 'qwen_vl_utils'`

**í•´ê²°**:
```bash
pip install qwen-vl-utils
```

### Gemma3 ê´€ë ¨ ì˜¤ë¥˜

**ì—ëŸ¬**: `ImportError: cannot import name 'Gemma3ForConditionalGeneration'`

**í•´ê²°**:
```bash
pip install --upgrade transformers
```

ìµœì†Œ ë²„ì „: `transformers >= 4.40.0`

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

**í•´ê²° ë°©ë²•**:
1. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°: `--batch_size 1`
2. ì‘ì€ ëª¨ë¸ ì‚¬ìš©: `blip2-opt-2.7b`, `internvl2-2b`
3. ìƒ˜í”Œ ì œí•œ: `--max_samples 50`

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [VLM_EVALUATION_GUIDE.md](./VLM_EVALUATION_GUIDE.md): ì „ì²´ í‰ê°€ ê°€ì´ë“œ
- [EVALUATION_SCRIPTS_SUMMARY.md](./EVALUATION_SCRIPTS_SUMMARY.md): í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ìš”ì•½
- [evaluate_vlm_models.py](../scripts/evaluate_vlm_models.py): ì†ŒìŠ¤ ì½”ë“œ

## ğŸ”„ ì´ì „ ë§ˆì´ê·¸ë ˆì´ì…˜

### qwen-vl-chat â†’ qwen2.5-vl-3b

```bash
# ì´ì „
python scripts/evaluate_vlm_models.py \
    --models qwen-vl-chat

# ìƒˆë¡œìš´
pip install qwen-vl-utils
python scripts/evaluate_vlm_models.py \
    --models qwen2.5-vl-3b
```

### qwen2-vl-2b â†’ qwen2.5-vl-3b

```bash
# ì´ì „
python scripts/evaluate_vlm_models.py \
    --models qwen2-vl-2b

# ìƒˆë¡œìš´
pip install qwen-vl-utils
python scripts/evaluate_vlm_models.py \
    --models qwen2.5-vl-3b
```

### cogvlm2-llama3-chat-19b (ì œê±°ë¨)

19B íŒŒë¼ë¯¸í„° ëª¨ë¸ì€ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ì´ ë„ˆë¬´ ë†’ì•„ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.
ëŒ€ì•ˆìœ¼ë¡œ `llava-1.5-7b` ë˜ëŠ” `llava-1.6-mistral-7b`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

## ë³€ê²½ ì´ë ¥

- **2025-01-XX**: Gemma 3 4B ì¶”ê°€
- **2025-01-XX**: Qwen2.5-VL 3B ì¶”ê°€
- **2025-01-XX**: Qwen-VL-Chat, Qwen2-VL-2B, CogVLM2 ì œê±°
- **2025-01-XX**: Chat template ì§€ì› ì¶”ê°€

