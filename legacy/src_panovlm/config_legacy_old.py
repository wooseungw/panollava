#!/usr/bin/env python3
# coding: utf-8
"""
PanoramaVLM ëª¨ë¸ ì„¤ì • ê´€ë¦¬ì
===========================

ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥/ë¡œë”©í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
í›ˆë ¨ ì‹œ ì„¤ì •ì„ ì €ì¥í•˜ê³ , ì¶”ë¡ /í‰ê°€ ì‹œ ì¼ê´€ëœ ì„¤ì •ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
"""

import json
import os
from pathlib import Path
from typing import Dict, Any, Optional, Union
from dataclasses import dataclass, asdict, field
from datetime import datetime
import warnings


@dataclass
class ModelConfig:
    """PanoramaVLM ëª¨ë¸ ì„¤ì •"""
    
    # ëª¨ë¸ ì•„í‚¤í…ì²˜
    vision_name: str = "google/siglip-base-patch16-224"
    language_model_name: str = "Qwen/Qwen2.5-0.5B-Instruct"
    resampler_type: str = "mlp"
    latent_dimension: int = 768
    image_size: Optional[tuple] = None

    # ë¦¬ìƒ˜í”ŒëŸ¬ ì„¸ë¶€ ì„¤ì • (resampler_typeì— ë”°ë¼ ìë™ ì„¤ì •ë¨)
    resampler_depth: int = 2
    resampler_hidden_dim: Optional[int] = None
    resampler_use_ln: bool = True
    resampler_enable_cross_view: bool = False
    resampler_num_views: int = 8
    resampler_dropout: float = 0.1
    resampler_heads: int = 8
    resampler_num_latents: int = 32
    
    # VICReg ê´€ë ¨ ì„¤ì •
    vicreg_loss_weight: float = 1.0
    overlap_ratio: float = 0.5
    use_vicreg_norm: bool = True  # VICReg ê²½ë¡œì—ì„œ LayerNorm ì‚¬ìš© ì—¬ë¶€ (False = ì› ì² í•™ ì¤€ìˆ˜)
    
    # VICReg ì„¤ì • - ê°„ë‹¨í•œ x,y ì…ë ¥ ë°©ì‹
    vicreg_similarity_weight: float = 25.0
    vicreg_variance_weight: float = 25.0  
    vicreg_covariance_weight: float = 1.0
    
    # í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì„¤ì •
    max_text_length: int = 512

    # í† í° ê²°í•©(ìŠ¤í‹°ì¹­) ë°©ì‹
    # - 'drop_overlap' (ê¸°ë³¸): ê° ë·°ì˜ ê²¹ì¹˜ëŠ” ì¢Œì¸¡ ì˜ì—­ì„ ë“œëí•˜ì—¬ ì´ì–´ë¶™ì„
    # - 'stride_views'       : s=ceil(1/(1-overlap)) ê°„ê²©ìœ¼ë¡œ ë·°ë¥¼ ìƒ˜í”Œë§í•´ ì „ì²´ ì—´ ì‚¬ìš©
    # - 'concat'             : ë‹¨ìˆœ ì¸í„°ë¦¬ë¸Œ(ì¤‘ë³µ ì œê±° ì•ˆ í•¨)
    # - 'resample'           : íŒŒë…¸ë¼ë§ˆ ì „ì—­ ì¢Œí‘œë¡œ ì¬í‘œë³¸í™”í•˜ì—¬ ëª©í‘œ ê°€ë¡œ í† í° ìˆ˜ë¡œ ì •ê·œí™”
    stitching_mode: str = "drop_overlap"
    stitch_stride_offset: int = 0
    stitch_target_cols: int = 0              # 0ì´ë©´ ìë™ (ê³ ìœ  í­)
    stitch_target_to_view_width: bool = False  # Trueë©´ ìµœì¢… ê°€ë¡œ í† í° ìˆ˜ë¥¼ W(ë·° í­)ë¡œ ë§ì¶¤
    stitch_interp: str = "nearest"           # 'nearest' | 'linear' (í˜„ì¬ recent êµ¬í˜„ì€ nearest)
    
    # LoRA ì„¤ì • (ì˜µì…˜)
    use_lora: bool = False
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    lora_target_modules: Optional[list] = None
    
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì²˜ë¦¬"""
        # LoRA íƒ€ê²Ÿ ëª¨ë“ˆ ê¸°ë³¸ê°’ ì„¤ì •
        if self.use_lora and self.lora_target_modules is None:
            self.lora_target_modules = [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]

        # ë¦¬ìƒ˜í”ŒëŸ¬ íƒ€ì…ì— ë”°ë¥¸ ê¸°ë³¸ê°’ ìë™ ì ìš©
        self._apply_resampler_defaults()

    def _apply_resampler_defaults(self):
        """ë¦¬ìƒ˜í”ŒëŸ¬ íƒ€ì…ì— ë”°ë¥¸ ê¸°ë³¸ê°’ ìë™ ì ìš© (ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ)"""
        # Config.RESAMPLER_DEFAULTSëŠ” ì´ íŒŒì¼ í•˜ë‹¨ì— ì •ì˜ë¨
        # __post_init__ì—ì„œ í˜¸ì¶œë˜ë¯€ë¡œ í´ë˜ìŠ¤ ì •ì˜ê°€ ì™„ë£Œëœ í›„ì—ë§Œ ì‹¤í–‰ë¨
        pass  # Config í´ë˜ìŠ¤ ì •ì˜ í›„ ì•„ë˜ì—ì„œ ì‹¤ì œ ë¡œì§ êµ¬í˜„

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'ModelConfig':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±"""
        # ì•Œë ¤ì§„ í•„ë“œë§Œ ì¶”ì¶œ
        try:
            valid_fields = {f.name for f in cls.__dataclass_fields__.values()}
        except AttributeError:
            # fallback: dataclass fieldsì˜ í‚¤ë§Œ ì‚¬ìš©
            valid_fields = set(cls.__dataclass_fields__.keys())
        filtered_dict = {k: v for k, v in config_dict.items() if k in valid_fields}
        
        # tuple íƒ€ì… ë³€í™˜
        if 'image_size' in filtered_dict and isinstance(filtered_dict['image_size'], list):
            filtered_dict['image_size'] = tuple(filtered_dict['image_size'])
        
        return cls(**filtered_dict)
    
    def update(self, **kwargs) -> 'ModelConfig':
        """ì„¤ì • ì—…ë°ì´íŠ¸ (ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜)"""
        config_dict = self.to_dict()
        config_dict.update(kwargs)
        return self.from_dict(config_dict)
    
    def save(self, file_path: Union[str, Path]) -> None:
        """ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        ConfigManager.save_config(self, file_path)
    
    @classmethod
    def load(cls, file_path: Union[str, Path]) -> 'ModelConfig':
        """JSON íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë”©"""
        return ConfigManager.load_config(file_path)
    
    def get_model_kwargs(self) -> Dict[str, Any]:
        """PanoramaVLM ëª¨ë¸ ìƒì„±ì— í•„ìš”í•œ kwargs ë°˜í™˜"""
        return {
            'vision_name': self.vision_name,
            'language_model_name': self.language_model_name,
            'resampler_type': self.resampler_type,
            'latent_dimension': self.latent_dimension,
            'vicreg_loss_weight': self.vicreg_loss_weight,
            'overlap_ratio': self.overlap_ratio,
            'use_vicreg_norm': self.use_vicreg_norm,
            'max_text_length': self.max_text_length,
            # VICReg íŒŒë¼ë¯¸í„°ë“¤ - ê°„ë‹¨í•œ x,y ì…ë ¥ ë°©ì‹
            'vicreg_similarity_weight': self.vicreg_similarity_weight,
            'vicreg_variance_weight': self.vicreg_variance_weight,
            'vicreg_covariance_weight': self.vicreg_covariance_weight,
            # stitching
            'stitching_mode': self.stitching_mode,
            'stitch_stride_offset': self.stitch_stride_offset,
            'stitch_target_cols': self.stitch_target_cols,
            'stitch_target_to_view_width': self.stitch_target_to_view_width,
            'stitch_interp': self.stitch_interp,
        }
    
    
    def get_lora_kwargs(self) -> Dict[str, Any]:
        """LoRA ì„¤ì •ì— í•„ìš”í•œ kwargs ë°˜í™˜"""
        return {
            'lora_r': self.lora_r,
            'lora_alpha': self.lora_alpha,
            'lora_dropout': self.lora_dropout,
            'target_modules': self.lora_target_modules,
        }
    
    def validate(self) -> bool:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            # í•„ìˆ˜ ë¬¸ìì—´ í•„ë“œ í™•ì¸
            assert self.vision_name.strip(), "vision_nameì€ ë¹„ì–´ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            assert self.language_model_name.strip(), "language_model_nameì€ ë¹„ì–´ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            allowed_resamplers = {"mlp", "perceiver", "bimamba", "bidirectional_mamba", "bi_mamba"}
            assert self.resampler_type in allowed_resamplers, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” resampler_type: {self.resampler_type}"
            
            # ìˆ«ì ë²”ìœ„ í™•ì¸
            assert self.latent_dimension > 0, "latent_dimensionì€ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
            assert self.vicreg_loss_weight >= 0, "vicreg_loss_weightëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤"
            assert 0 <= self.overlap_ratio <= 1, "overlap_ratioëŠ” 0-1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤"
            assert self.max_text_length > 0, "max_text_lengthëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
            # stitching mode
            assert self.stitch_stride_offset >= 0, "stitch_stride_offsetëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤"
            if self.stitch_interp not in ["nearest", "linear"]:
                raise AssertionError("stitch_interpëŠ” 'nearest' ë˜ëŠ” 'linear' ì—¬ì•¼ í•©ë‹ˆë‹¤")
            if self.stitch_target_cols < 0:
                raise AssertionError("stitch_target_colsëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤")
            
            
            # LoRA ì„¤ì • í™•ì¸
            if self.use_lora:
                assert self.lora_r > 0, "lora_rì€ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
                assert self.lora_alpha > 0, "lora_alphaëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
                assert 0 <= self.lora_dropout <= 1, "lora_dropoutì€ 0-1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤"
            
            return True
            
        except AssertionError as e:
            warnings.warn(f"ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return False
    
    def __str__(self) -> str:
        """ë¬¸ìì—´ í‘œí˜„"""
        return f"ModelConfig(vision={self.vision_name}, language={self.language_model_name}, dim={self.latent_dimension})"

class Config:
    """Training configuration management"""

    STAGE_DEFAULTS = {
        "vision": {
            "epochs": 1,
            "lr": 5e-6,
            "batch_size": 16,
            "vicreg_loss_weight": 1.0,
            "max_text_length": 32
        },
        "resampler": {
            "epochs": 1,
            "lr": 2e-6,
            "batch_size": 8,
            "vicreg_loss_weight": 0.0,
            "max_text_length": 256
        },
        "finetune": {
            "epochs": 1,
            "lr": 2e-6,
            "batch_size": 8,
            "vicreg_loss_weight": 0.0,
            "max_text_length": 256
        }
    }

    # ë¦¬ìƒ˜í”ŒëŸ¬ íƒ€ì…ë³„ ê¸°ë³¸ ì„¤ì •
    RESAMPLER_DEFAULTS = {
        "mlp": {
            "latent_dimension": 768,
            "resampler_depth": 3,
            "resampler_hidden_dim": 1536,
            "resampler_use_ln": True,
        },
        "perceiver": {
            "latent_dimension": 768,
            "resampler_num_latents": 32,
            "resampler_depth": 2,
            "resampler_heads": 8,
            "resampler_use_ln": True,
        },
        "bimamba": {
            "latent_dimension": 768,
            "resampler_depth": 2,
            "resampler_hidden_dim": 1536,
            "resampler_use_ln": True,
            "resampler_enable_cross_view": False,
        },
        "bidirectional_mamba": {
            "latent_dimension": 768,
            "resampler_depth": 2,
            "resampler_hidden_dim": 1536,
            "resampler_use_ln": True,
            "resampler_enable_cross_view": False,
        },
        "bi_mamba": {
            "latent_dimension": 768,
            "resampler_depth": 2,
            "resampler_hidden_dim": 1536,
            "resampler_use_ln": True,
            "resampler_enable_cross_view": False,
        },
    }


# Config í´ë˜ìŠ¤ ì •ì˜ í›„ ModelConfig._apply_resampler_defaults êµ¬í˜„
def _apply_resampler_defaults_impl(self):
    """ë¦¬ìƒ˜í”ŒëŸ¬ íƒ€ì…ì— ë”°ë¥¸ ê¸°ë³¸ê°’ ìë™ ì ìš©"""
    defaults = Config.RESAMPLER_DEFAULTS.get(self.resampler_type, {})

    if not defaults:
        return

    # dataclass í•„ë“œì˜ ê¸°ë³¸ê°’ê³¼ ë¹„êµí•˜ì—¬ ë³€ê²½ë˜ì§€ ì•Šì€ í•„ë“œë§Œ ì—…ë°ì´íŠ¸
    for key, value in defaults.items():
        if hasattr(self, key):
            current_value = getattr(self, key)
            try:
                field_info = self.__dataclass_fields__.get(key)
                if field_info and current_value == field_info.default:
                    setattr(self, key, value)
            except (AttributeError, KeyError):
                # í•„ë“œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                pass


# ModelConfigì— ì‹¤ì œ êµ¬í˜„ ì£¼ì…
ModelConfig._apply_resampler_defaults = _apply_resampler_defaults_impl


class ConfigManager:
    """ì„¤ì • íŒŒì¼ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°"""
    
    DEFAULT_CONFIG_NAME = "model_config.json"
    
    @staticmethod
    def save_config(config: ModelConfig, file_path: Union[str, Path]) -> None:
        """ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        file_path = Path(file_path)
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬
        if not config.validate():
            warnings.warn("ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬ì— ì‹¤íŒ¨í–ˆì§€ë§Œ ì €ì¥ì„ ê³„ì†í•©ë‹ˆë‹¤")
        
        config_dict = config.to_dict()
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(config_dict, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… ì„¤ì • ì €ì¥ ì™„ë£Œ: {file_path}")
            
        except Exception as e:
            raise RuntimeError(f"ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
    
    @staticmethod
    def load_config(file_path: Union[str, Path]) -> ModelConfig:
        """JSON íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë”©"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
            
            # JSON config êµ¬ì¡°ë¥¼ ModelConfig í˜•íƒœë¡œ ë³€í™˜
            flat_config = ConfigManager._flatten_json_config(config_dict)
            config = ModelConfig.from_dict(flat_config)
            
            # ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬
            if not config.validate():
                warnings.warn("ë¡œë“œëœ ì„¤ì •ì´ ìœ íš¨ì„± ê²€ì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            print(f"âœ… ì„¤ì • ë¡œë”© ì™„ë£Œ: {file_path}")
            return config
            
        except json.JSONDecodeError as e:
            raise RuntimeError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        except Exception as e:
            raise RuntimeError(f"ì„¤ì • ë¡œë”© ì‹¤íŒ¨: {e}")
    
    @staticmethod
    def _flatten_json_config(config_dict: Dict[str, Any]) -> Dict[str, Any]:
        """JSON configì˜ nested êµ¬ì¡°ë¥¼ ModelConfigì˜ flat êµ¬ì¡°ë¡œ ë³€í™˜"""
        flat_config = {}
        
        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì • (ì‹ ê·œ í‚¤ ìš°ì„ , êµ¬í‚¤ë„ ë³‘í–‰ ì§€ì›)
        if 'models' in config_dict:
            models = config_dict['models']
            # ì‹ ê·œ í‘œì¤€í™”ëœ í‚¤
            lang_name = models.get('language_model_name', models.get('lm_model', 'Qwen/Qwen2.5-0.5B-Instruct'))
            resampler_type = models.get('resampler_type', models.get('resampler', 'mlp'))
            vision_name = models.get('vision_model_name', models.get('vision_name'))

            flat_config.update({
                'vision_name': vision_name,
                'language_model_name': lang_name,
                'resampler_type': resampler_type,
                'latent_dimension': models.get('latent_dimension', 768),
            })
        
        # ë°ì´í„° ì„¤ì •
        if 'data' in config_dict:
            data = config_dict['data']
            # max_text_length may be "auto"; only forward numeric to ModelConfig
            mtl_val = data.get('max_text_length', 512)
            try:
                if isinstance(mtl_val, (int, float)) and mtl_val > 0:
                    flat_config['max_text_length'] = int(mtl_val)
            except Exception:
                pass
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì •ì—ì„œ overlap_ratioì™€ image_size ì¶”ì¶œ
        if 'image_processing' in config_dict:
            img_proc = config_dict['image_processing']
            flat_config.update({
                'overlap_ratio': img_proc.get('overlap_ratio', 0.5),
                'image_size': img_proc.get('image_size'),
                'crop_strategy': img_proc.get('crop_strategy'),
                'stitching_mode': img_proc.get('stitching_mode', 'drop_overlap'),
                'stitch_stride_offset': img_proc.get('stitch_stride_offset', 0),
                'stitch_target_cols': img_proc.get('stitch_target_cols', 0),
                'stitch_target_to_view_width': img_proc.get('stitch_target_to_view_width', False),
                'stitch_interp': img_proc.get('stitch_interp', 'nearest'),
            })
        
        # í›ˆë ¨ ì„¤ì • (íŠ¹íˆ VICReg Local)
        if 'training' in config_dict:
            training = config_dict['training']

            # Vision stage ì„¤ì •ì—ì„œ VICReg Local íŒŒë¼ë¯¸í„°ë“¤ ì¶”ì¶œ
            vision_cfg = None
            stage_cfgs = training.get('stage_configs')
            if isinstance(stage_cfgs, dict) and isinstance(stage_cfgs.get('vision'), dict):
                vision_cfg = stage_cfgs.get('vision')
            elif isinstance(training.get('vision'), dict):
                vision_cfg = training.get('vision')

            if isinstance(vision_cfg, dict):
                flat_config.update({
                    'vicreg_loss_weight': vision_cfg.get('vicreg_loss_weight', 1.0),
                    'vicreg_similarity_weight': vision_cfg.get('vicreg_similarity_weight', 25.0),
                    'vicreg_variance_weight': vision_cfg.get('vicreg_variance_weight', 25.0),
                    'vicreg_covariance_weight': vision_cfg.get('vicreg_covariance_weight', 1.0),
                })
        
        # LoRA ì„¤ì •
        if 'lora' in config_dict:
            lora = config_dict['lora']
            flat_config.update({
                'use_lora': lora.get('use_lora', False),
                'lora_r': lora.get('rank', 16),
                'lora_alpha': lora.get('alpha', 32),
                'lora_dropout': lora.get('dropout', 0.1),
                'lora_target_modules': lora.get('target_modules', None)
            })
        
        return flat_config
    
    @staticmethod
    def auto_detect_config(checkpoint_path: Union[str, Path]) -> Optional[ModelConfig]:
        """ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œì—ì„œ ì„¤ì • íŒŒì¼ ìë™ ê°ì§€"""
        checkpoint_path = Path(checkpoint_path)
        
        # ì²´í¬í¬ì¸íŠ¸ê°€ íŒŒì¼ì¸ ê²½ìš° ë””ë ‰í† ë¦¬ ì¶”ì¶œ
        if checkpoint_path.is_file():
            search_dir = checkpoint_path.parent
        else:
            search_dir = checkpoint_path
        
        # ì„¤ì • íŒŒì¼ í›„ë³´ë“¤
        config_candidates = [
            # 1. ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
            search_dir / ConfigManager.DEFAULT_CONFIG_NAME,
            search_dir / "config.json",
            search_dir / "model_config.json", 
            search_dir / "panovlm_config.json",
            # 2. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
            Path.cwd() / "config.json",
            Path.cwd() / ConfigManager.DEFAULT_CONFIG_NAME,
            # 3. í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •ëœ ê²½ë¡œ
        ]
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ config ê²½ë¡œ ì¶”ê°€
        env_config = os.environ.get("PANOVLM_CONFIG")
        if env_config:
            config_candidates.append(Path(env_config))
        
        for config_path in config_candidates:
            if config_path.exists():
                try:
                    print(f"ğŸ” ì„¤ì • íŒŒì¼ ë°œê²¬: {config_path}")
                    return ConfigManager.load_config(config_path)
                except Exception as e:
                    warnings.warn(f"ì„¤ì • íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ ({config_path}): {e}")
        
        # ë””ë²„ê¹…: ì°¾ì€ í›„ë³´ë“¤ê³¼ í˜„ì¬ ë””ë ‰í† ë¦¬ ì •ë³´ ì¶œë ¥
        print(f"ğŸ” ì„¤ì • íŒŒì¼ ê°ì§€ ì‹¤íŒ¨")
        print(f"   - ê²€ìƒ‰ ê²½ë¡œ: {search_dir}")
        print(f"   - í˜„ì¬ ë””ë ‰í† ë¦¬: {Path.cwd()}")
        print(f"   - ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: {checkpoint_path}")
        
        return None
    
    @staticmethod
    def create_default_config(**overrides) -> ModelConfig:
        """ê¸°ë³¸ ì„¤ì • ìƒì„± (ì˜¤ë²„ë¼ì´ë“œ ì ìš© ê°€ëŠ¥)"""
        config = ModelConfig()
        if overrides:
            config = config.update(**overrides)
        return config
    
    @staticmethod
    def migrate_old_config(old_config_dict: Dict[str, Any]) -> ModelConfig:
        """êµ¬ë²„ì „ ì„¤ì •ì„ ìƒˆ í˜•ì‹ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        # êµ¬ë²„ì „ í•„ë“œëª… ë§¤í•‘
        field_mapping = {
            'vision_model': 'vision_name',
            'language_model': 'language_model_name',
            'resampler': 'resampler_type',
            'dim': 'latent_dimension',
            'vicreg_weight': 'vicreg_loss_weight',
            'vicreg_overlap': 'overlap_ratio',
            'max_length': 'max_text_length',
        }
        
        # í•„ë“œëª… ë³€í™˜
        migrated_dict = {}
        for old_key, value in old_config_dict.items():
            new_key = field_mapping.get(old_key, old_key)
            migrated_dict[new_key] = value
        
        return ModelConfig.from_dict(migrated_dict)
    


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_config(**kwargs) -> ModelConfig:
    """í¸ì˜ í•¨ìˆ˜: ì„¤ì • ìƒì„±"""
    return ConfigManager.create_default_config(**kwargs)

def save_config(config: ModelConfig, file_path: Union[str, Path]) -> None:
    """í¸ì˜ í•¨ìˆ˜: ì„¤ì • ì €ì¥"""
    ConfigManager.save_config(config, file_path)

def load_config(file_path: Union[str, Path]) -> ModelConfig:
    """í¸ì˜ í•¨ìˆ˜: ì„¤ì • ë¡œë”©"""
    return ConfigManager.load_config(file_path)

def auto_detect_config(checkpoint_path: Union[str, Path]) -> Optional[ModelConfig]:
    """í¸ì˜ í•¨ìˆ˜: ì„¤ì • ìë™ ê°ì§€"""
    return ConfigManager.auto_detect_config(checkpoint_path)
