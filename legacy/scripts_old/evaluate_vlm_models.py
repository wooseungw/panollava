#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""VLM ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (í•™ìŠµ ì—†ì´)

ë‹¤ì–‘í•œ HuggingFace VLM ëª¨ë¸ë“¤ì„ ë™ì¼í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
í‰ê°€ ì§€í‘œ: BLEU-4, METEOR, ROUGE-L, Exact Match ë“±
ì´ë¯¸ì§€ í¬ê¸°: 224x224ë¡œ ê³ ì •
"""

from __future__ import annotations

import argparse
import json
import logging
import math
import os
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
import torch
from PIL import Image
from tqdm.auto import tqdm

# Monkey-patch for LLaVA-OneVision flash attention compatibility
# LLaVA-OneVision models expect flash_attn_varlen_func in transformers.modeling_flash_attention_utils
# but newer transformers versions removed it. We provide a compatibility layer.
import sys
import importlib

# Step 1: Patch transformers.modeling_flash_attention_utils before any model imports
try:
    import transformers.modeling_flash_attention_utils as flash_utils

    if not hasattr(flash_utils, 'flash_attn_varlen_func'):
        # Try importing from flash_attn package
        try:
            from flash_attn import flash_attn_varlen_func
            flash_utils.flash_attn_varlen_func = flash_attn_varlen_func
            print("âœ“ Patched flash_attn_varlen_func from flash_attn package")
        except ImportError:
            # Create a dummy function that will work with eager attention
            def flash_attn_varlen_func(*args, **kwargs):
                raise NotImplementedError(
                    "FlashAttention is not available. Using eager attention instead."
                )
            flash_utils.flash_attn_varlen_func = flash_attn_varlen_func
            print("âš ï¸ Created dummy flash_attn_varlen_func - models will use eager attention")

    # Also patch _flash_attention_forward if needed
    if not hasattr(flash_utils, '_flash_attention_forward'):
        def _flash_attention_forward(*args, **kwargs):
            raise NotImplementedError(
                "FlashAttention is not available. Using eager attention instead."
            )
        flash_utils._flash_attention_forward = _flash_attention_forward

except Exception as e:
    print(f"âš ï¸ Warning: Could not patch flash attention utilities: {e}")
    print("   Models will attempt to use eager attention")

# Allow large images
Image.MAX_IMAGE_PIXELS = None

# Add project root to Python path to import eval.py utilities
_SCRIPT_DIR = Path(__file__).resolve().parent
_REPO_ROOT = _SCRIPT_DIR.parent
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))

# Import shared evaluation metrics from eval.py
try:
    from scripts.eval import calculate_evaluation_metrics as eval_calculate_metrics
    USE_EVAL_METRICS = True
    logging.info("âœ“ Using shared evaluation metrics from scripts/eval.py")
except ImportError:
    USE_EVAL_METRICS = False
    logging.warning("âš ï¸ Could not import eval.py metrics, falling back to local implementation")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# InternVL3.5 ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def build_transform_internvl(input_size):
    """InternVLìš© ì´ë¯¸ì§€ ë³€í™˜"""
    import torchvision.transforms as T
    from torchvision.transforms.functional import InterpolationMode

    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    return transform

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    """InternVLìš© ìµœì  aspect ratio ì°¾ê¸°"""
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def dynamic_preprocess_internvl(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
    """InternVLìš© dynamic preprocessing"""
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height

    # calculate the existing image aspect ratio
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])

    # find the closest aspect ratio to the target
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)

    # calculate the target width and height
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]

    # resize the image
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        # split the image
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images

def load_image_internvl(image, input_size=448, max_num=12):
    """InternVLìš© ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    transform = build_transform_internvl(input_size=input_size)
    images = dynamic_preprocess_internvl(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(img) for img in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# VLM ëª¨ë¸ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Padding sideëŠ” _get_padding_side() ë©”ì„œë“œì—ì„œ ìë™ ì„¤ì •:
#   - Decoder-only ëª¨ë¸: left padding (LLaVA, Qwen2.5-VL, BLIP2-OPT, InstructBLIP-Vicuna, PaliGemma, Gemma-3, InternVL)
#   - Encoder-decoder ëª¨ë¸: right padding (Florence-2, BLIP2-T5, InstructBLIP-T5)

VLM_MODELS = {
    "llava-1.5-7b": {
        "model_id": "llava-hf/llava-1.5-7b-hf",
        "processor_id": "llava-hf/llava-1.5-7b-hf",
        "model_class": "LlavaForConditionalGeneration",
        "processor_class": "LlavaProcessor",
        "prompt_template": "USER: <image>\n{instruction}\nASSISTANT:",
    },
    "llava-1.6-mistral-7b": {
        "model_id": "llava-hf/llava-v1.6-mistral-7b-hf",
        "processor_id": "llava-hf/llava-v1.6-mistral-7b-hf",
        "model_class": "LlavaNextForConditionalGeneration",
        "processor_class": "LlavaNextProcessor",
        "prompt_template": "[INST] <image>\n{instruction} [/INST]",
    },
    "llava-onevision-0.5b": {
        "model_id": "lmms-lab/llava-onevision-qwen2-0.5b-ov",
        "processor_id": "lmms-lab/llava-onevision-qwen2-0.5b-ov",
        "model_class": "LlavaOnevisionForConditionalGeneration",
        "processor_class": "AutoProcessor",
        "use_chat_template": True,
        "requires_vision_utils": True,  # LLaVA-OneVision requires qwen_vl_utils like Qwen2.5-VL
    },
    "llava-onevision-4b": {
        "model_id": "lmms-lab/LLaVA-OneVision-1.5-4B-Instruct",
        "processor_id": "lmms-lab/LLaVA-OneVision-1.5-4B-Instruct",
        "model_class": "LlavaOnevisionForConditionalGeneration",
        "processor_class": "AutoProcessor",
        "use_chat_template": True,
        "requires_vision_utils": True,  # LLaVA-OneVision requires qwen_vl_utils like Qwen2.5-VL
    },
    "llava-onevision-7b": {
        "model_id": "lmms-lab/llava-onevision-qwen2-7b-ov",
        "processor_id": "lmms-lab/llava-onevision-qwen2-7b-ov",
        "model_class": "LlavaOnevisionForConditionalGeneration",
        "processor_class": "AutoProcessor",
        "use_chat_template": True,
        "requires_vision_utils": True,  # LLaVA-OneVision requires qwen_vl_utils like Qwen2.5-VL
    },
    "blip2-opt-2.7b": {
        "model_id": "Salesforce/blip2-opt-2.7b",
        "processor_id": "Salesforce/blip2-opt-2.7b",
        "model_class": "Blip2ForConditionalGeneration",
        "processor_class": "Blip2Processor",
        "prompt_template": "Question: {instruction} Answer:",
    },
    "instructblip-vicuna-7b": {
        "model_id": "Salesforce/instructblip-vicuna-7b",
        "processor_id": "Salesforce/instructblip-vicuna-7b",
        "model_class": "InstructBlipForConditionalGeneration",
        "processor_class": "InstructBlipProcessor",
        "prompt_template": "{instruction}",
    },
    "qwen2.5-vl-3b": {
        "model_id": "Qwen/Qwen2.5-VL-3B-Instruct",
        "processor_id": "Qwen/Qwen2.5-VL-3B-Instruct",
        "model_class": "Qwen2_5_VLForConditionalGeneration",
        "processor_class": "AutoProcessor",
        "use_chat_template": True,  # Use chat template with process_vision_info
        "requires_vision_utils": True,  # Requires qwen_vl_utils
    },
    "paligemma-3b": {
        "model_id": "google/paligemma-3b-mix-448",
        "processor_id": "google/paligemma-3b-mix-448",
        "model_class": "AutoModelForCausalLM",
        "processor_class": "AutoProcessor",
        "prompt_template": "describe en",  # PaliGemma uses simple task prompts
    },
    "florence-2-large": {
        "model_id": "microsoft/Florence-2-large",
        "processor_id": "microsoft/Florence-2-large",
        "model_class": "AutoModelForCausalLM",
        "processor_class": "AutoProcessor",
        "prompt_template": "<MORE_DETAILED_CAPTION>",  # Florence-2 uses task tokens
        "is_florence": True,  # Special handling for Florence-2
    },
    "internvl3.5-1b": {
        "model_id": "OpenGVLab/InternVL3_5-1B",
        "processor_id": "OpenGVLab/InternVL3_5-1B",
        "model_class": "AutoModel",
        "processor_class": "AutoTokenizer",
        "prompt_template": "<image>\n{instruction}",
        "is_internvl": True,  # Special handling for InternVL
        "requires_custom_image_processing": True,
    },
    "internvl3.5-2b": {
        "model_id": "OpenGVLab/InternVL3_5-2B",
        "processor_id": "OpenGVLab/InternVL3_5-2B",
        "model_class": "AutoModel",
        "processor_class": "AutoTokenizer",
        "prompt_template": "<image>\n{instruction}",
        "is_internvl": True,  # Special handling for InternVL
        "requires_custom_image_processing": True,
    },
    "internvl3.5-4b": {
        "model_id": "OpenGVLab/InternVL3_5-4B",
        "processor_id": "OpenGVLab/InternVL3_5-4B",
        "model_class": "AutoModel",
        "processor_class": "AutoTokenizer",
        "prompt_template": "<image>\n{instruction}",
        "is_internvl": True,  # Special handling for InternVL
        "requires_custom_image_processing": True,
    },
    "internvl3.5-8b": {
        "model_id": "OpenGVLab/InternVL3_5-8B",
        "processor_id": "OpenGVLab/InternVL3_5-8B",
        "model_class": "AutoModel",
        "processor_class": "AutoTokenizer",
        "prompt_template": "<image>\n{instruction}",
        "is_internvl": True,  # Special handling for InternVL
        "requires_custom_image_processing": True,
    },
    "gemma-3-4b": {
        "model_id": "google/gemma-3-4b-it",
        "processor_id": "google/gemma-3-4b-it",
        "model_class": "Gemma3ForConditionalGeneration",
        "processor_class": "AutoProcessor",
        "use_chat_template": True,  # Uses chat template like Qwen2.5-VL
        "requires_vision_utils": False,  # Doesn't need qwen_vl_utils
    },
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def compute_text_metrics(predictions: List[str], references: List[str]) -> Dict[str, float]:
    """BLEU-4, METEOR, ROUGE-L, SPICE, CIDEr ê³„ì‚° (eval.pyì™€ ë™ì¼í•œ êµ¬í˜„ ì‚¬ìš©)

    ì´ í•¨ìˆ˜ëŠ” scripts/eval.pyì˜ calculate_evaluation_metricsë¥¼ ì¬ì‚¬ìš©í•˜ì—¬
    ëª¨ë“  í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë™ì¼í•œ ë©”íŠ¸ë¦­ ê³„ì‚°ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    """
    logging.info(f"ğŸ“Š ë©”íŠ¸ë¦­ ê³„ì‚° ì‹œì‘: {len(predictions)} predictions, {len(references)} references")
    
    # ë°ì´í„° ê²€ì¦
    valid_count = sum(1 for p, r in zip(predictions, references) if p.strip() and r.strip())
    empty_pred_count = sum(1 for p in predictions if not p.strip())
    empty_ref_count = sum(1 for r in references if not r.strip())
    
    logging.info(f"  - ìœ íš¨í•œ ìŒ: {valid_count}/{len(predictions)}")
    logging.info(f"  - ë¹ˆ ì˜ˆì¸¡: {empty_pred_count}")
    logging.info(f"  - ë¹ˆ ì°¸ì¡°: {empty_ref_count}")
    
    if valid_count == 0:
        logging.error("âŒ ìœ íš¨í•œ ì˜ˆì¸¡-ì •ë‹µ ìŒì´ ì—†ìŠµë‹ˆë‹¤!")
        return {}
    
    if USE_EVAL_METRICS:
        # Use shared implementation from eval.py for consistency
        try:
            # Create a temporary DataFrame matching eval.py's expected format
            temp_df = pd.DataFrame({
                'prediction': predictions,
                'reference': references,
            })

            # Use eval.py's calculate_evaluation_metrics
            # Note: we pass a dummy output_dir and timestamp since we only need metrics
            import tempfile
            import time
            with tempfile.TemporaryDirectory() as tmpdir:
                metrics = eval_calculate_metrics(
                    temp_df,
                    output_dir=Path(tmpdir),
                    timestamp=time.strftime('%Y%m%d_%H%M%S'),
                    prefix='temp'
                )

            logging.info("âœ“ ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ (eval.py êµ¬í˜„ ì‚¬ìš©)")
            return metrics

        except Exception as exc:
            import traceback
            logging.error(f"âŒ eval.py ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨, ë¡œì»¬ êµ¬í˜„ìœ¼ë¡œ í´ë°±: {exc}")
            logging.error(f"Traceback: {traceback.format_exc()}")
            # Fall through to local implementation

    # Local fallback implementation (kept for backwards compatibility)
    metrics: Dict[str, float] = {}

    paired = [
        (pred.strip(), ref.strip())
        for pred, ref in zip(predictions, references)
        if ref is not None and str(ref).strip() != "" and pred is not None and str(pred).strip() != ""
    ]

    if not paired:
        logging.warning("âš ï¸ í‰ê°€ ê°€ëŠ¥í•œ ì˜ˆì¸¡-ì •ë‹µ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
        return metrics

    preds = [p for p, _ in paired]
    refs = [r for _, r in paired]
    
    logging.info(f"ğŸ“Š ë¡œì»¬ ë©”íŠ¸ë¦­ ê³„ì‚°: {len(paired)} ìœ íš¨ ìŒ")

    # BLEU-4
    try:
        from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

        smoothing = SmoothingFunction().method1
        ref_tokens = [[r.split()] for r in refs]
        pred_tokens = [p.split() for p in preds]
        if ref_tokens and pred_tokens:
            metrics["bleu4"] = float(
                corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)
            )
    except Exception as exc:
        logging.warning(f"BLEU-4 ê³„ì‚° ì‹¤íŒ¨: {exc}")

    # METEOR
    try:
        import nltk

        try:
            nltk.data.find("corpora/wordnet")
        except LookupError:
            nltk.download("wordnet", quiet=True)
            nltk.download("punkt", quiet=True)

        from nltk.translate.meteor_score import meteor_score

        meteor_scores = []
        for ref, pred in zip(refs, preds):
            if ref and pred:
                meteor_scores.append(meteor_score([ref.split()], pred.split()))
        if meteor_scores:
            metrics["meteor"] = float(np.mean(meteor_scores))
    except Exception as exc:
        logging.warning(f"METEOR ê³„ì‚° ì‹¤íŒ¨: {exc}")

    # ROUGE-L
    try:
        from rouge_score import rouge_scorer

        scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
        rouge_values = []
        for ref, pred in zip(refs, preds):
            if ref and pred:
                rouge = scorer.score(ref, pred)
                rouge_values.append(rouge["rougeL"].fmeasure)
        if rouge_values:
            metrics["rougeL"] = float(np.mean(rouge_values))
    except Exception as exc:
        logging.warning(f"ROUGE-L ê³„ì‚° ì‹¤íŒ¨: {exc}")

    # SPICE
    try:
        from pycocoevalcap.spice.spice import Spice
        spice_scorer = Spice()

        gts = {str(i): [ref] for i, ref in enumerate(refs)}
        res = {str(i): [pred] for i, pred in enumerate(preds)}

        spice_score, _ = spice_scorer.compute_score(gts, res)
        metrics["spice"] = float(spice_score)
        logging.info(f"âœ“ SPICE: {metrics['spice']:.4f}")
    except Exception as exc:
        logging.warning(f"SPICE ê³„ì‚° ì‹¤íŒ¨: {exc}")
        metrics["spice"] = 0.0

    # CIDEr
    try:
        from pycocoevalcap.cider.cider import Cider
        cider_scorer = Cider()

        gts = {str(i): [ref] for i, ref in enumerate(refs)}
        res = {str(i): [pred] for i, pred in enumerate(preds)}

        cider_score, _ = cider_scorer.compute_score(gts, res)
        metrics["cider"] = float(cider_score)
        logging.info(f"âœ“ CIDEr: {metrics['cider']:.4f}")
    except Exception as exc:
        logging.warning(f"CIDEr ê³„ì‚° ì‹¤íŒ¨: {exc}")
        metrics["cider"] = 0.0

    return metrics


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# VLM ëª¨ë¸ í‰ê°€ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class VLMEvaluator:
    def __init__(
        self,
        model_name: str,
        data_csv: str,
        output_dir: str = "eval_results",
        batch_size: int = 1,
        max_samples: Optional[int] = None,
        device: str = "cuda",
        image_size: int = 224,
        max_new_tokens: int = 128,
        image_column: str = "url",
        instruction_column: str = "query",
        response_column: str = "annotation",
    ):
        self.model_name = model_name
        self.data_csv = Path(data_csv)
        # ëª¨ë¸ë³„ í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±: ablation/{ëª¨ë¸ëª…}
        self.output_dir = Path(output_dir) / "ablation" / model_name
        self.batch_size = batch_size
        self.max_samples = max_samples
        self.device = device
        self.image_size = image_size
        self.max_new_tokens = max_new_tokens
        self.image_column = image_column
        self.instruction_column = instruction_column
        self.response_column = response_column

        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        if model_name not in VLM_MODELS:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}. ì‚¬ìš© ê°€ëŠ¥: {list(VLM_MODELS.keys())}")
        
        self.model_config = VLM_MODELS[model_name]
        
        logging.info(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_config['model_id']}")
        self._load_model_and_processor()

    def _load_model_and_processor(self):
        """ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ"""
        from transformers import (
            AutoModel,
            AutoModelForCausalLM,
            AutoProcessor,
            AutoTokenizer,
            Blip2ForConditionalGeneration,
            Blip2Processor,
            Gemma3ForConditionalGeneration,
            InstructBlipForConditionalGeneration,
            InstructBlipProcessor,
            LlavaForConditionalGeneration,
            LlavaNextForConditionalGeneration,
            LlavaNextProcessor,
            LlavaOnevisionForConditionalGeneration,
            LlavaProcessor,
            Qwen2_5_VLForConditionalGeneration,
        )

        model_class_name = self.model_config["model_class"]
        processor_class_name = self.model_config["processor_class"]

        # ëª¨ë¸ í´ë˜ìŠ¤ ë§¤í•‘
        model_classes = {
            "LlavaForConditionalGeneration": LlavaForConditionalGeneration,
            "LlavaNextForConditionalGeneration": LlavaNextForConditionalGeneration,
            "LlavaOnevisionForConditionalGeneration": LlavaOnevisionForConditionalGeneration,
            "Blip2ForConditionalGeneration": Blip2ForConditionalGeneration,
            "InstructBlipForConditionalGeneration": InstructBlipForConditionalGeneration,
            "AutoModelForCausalLM": AutoModelForCausalLM,
            "AutoModel": AutoModel,
            "Qwen2_5_VLForConditionalGeneration": Qwen2_5_VLForConditionalGeneration,
            "Gemma3ForConditionalGeneration": Gemma3ForConditionalGeneration,
        }

        # í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤ ë§¤í•‘
        processor_classes = {
            "LlavaProcessor": LlavaProcessor,
            "LlavaNextProcessor": LlavaNextProcessor,
            "Blip2Processor": Blip2Processor,
            "InstructBlipProcessor": InstructBlipProcessor,
            "AutoProcessor": AutoProcessor,
            "AutoTokenizer": AutoTokenizer,
        }

        model_class = model_classes.get(model_class_name)
        processor_class = processor_classes.get(processor_class_name)

        if model_class is None:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í´ë˜ìŠ¤: {model_class_name}")
        if processor_class is None:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤: {processor_class_name}")

        # ëª¨ë¸ ë¡œë“œ
        # InternVL ëª¨ë¸ì€ bfloat16, ë‚˜ë¨¸ì§€ëŠ” float16
        dtype = torch.bfloat16 if "internvl" in self.model_name else torch.float16

        model_kwargs = {
            "torch_dtype": dtype,  # use standard torch_dtype arg across models
            "device_map": self.device,
            "trust_remote_code": True,
        }
        
        # dtype deprecated ê²½ê³  ë¬´ì‹œë¥¼ ìœ„í•œ ì²˜ë¦¬
        # ì¼ë¶€ ëª¨ë¸ì—ì„œëŠ” ì—¬ì „íˆ dtypeì„ ì‚¬ìš©

        # FlashAttention2ê°€ ì—†ê±°ë‚˜ í˜¸í™˜ì„± ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° eager attention ì‚¬ìš©
        # LLaVA-OneVision ëª¨ë¸ì€ transformersì˜ flash_attn_varlen_funcë¥¼ ìš”êµ¬í•˜ëŠ”ë°
        # ìµœì‹  transformersì—ì„œëŠ” ì´ í•¨ìˆ˜ê°€ ì œê±°ë¨
        if "llava-onevision" in self.model_name or "internvl" in self.model_name:
            model_kwargs["attn_implementation"] = "eager"

        # LLaVA-OneVision ì»¤ìŠ¤í…€ config ë“±ë¡
        # rice_vit, LLaVAOneVision1_5_text ë“± ì»¤ìŠ¤í…€ ì„¤ì •ì´ CONFIG_MAPPINGì— ì—†ìŒ
        if "llava-onevision" in self.model_name:
            try:
                from transformers import (
                    AutoConfig,
                    Qwen2Config,
                    CONFIG_MAPPING
                )
                
                # rice_vit: ì‹¤ì œ rice-vit ëª¨ë¸ì˜ configë¥¼ ê°€ì ¸ì™€ì„œ ë“±ë¡
                if "rice_vit" not in CONFIG_MAPPING:
                    try:
                        # rice-vit ëª¨ë¸ì˜ ì‹¤ì œ config í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜´
                        rice_config = AutoConfig.from_pretrained(
                            "DeepGlint-AI/rice-vit-large-patch14-560",
                            trust_remote_code=True
                        )
                        # rice_vitì˜ config í´ë˜ìŠ¤ë¥¼ ë“±ë¡
                        CONFIG_MAPPING.register("rice_vit", type(rice_config))
                        logging.info(f"âœ“ Registered rice_vit config (model_type: {rice_config.model_type})")
                    except Exception as e:
                        logging.warning(f"âš ï¸ Could not load rice_vit config, trying alternative: {e}")
                        # Fallback: mlcd_vision_modelë„ ë“±ë¡ ì‹œë„
                        try:
                            if "mlcd_vision_model" not in CONFIG_MAPPING:
                                rice_config = AutoConfig.from_pretrained(
                                    "DeepGlint-AI/rice-vit-large-patch14-560",
                                    trust_remote_code=True
                                )
                                CONFIG_MAPPING.register("mlcd_vision_model", type(rice_config))
                                CONFIG_MAPPING.register("rice_vit", type(rice_config))
                                logging.info("âœ“ Registered rice_vit and mlcd_vision_model")
                        except Exception as e2:
                            logging.error(f"âŒ Failed to register rice_vit: {e2}")
                
                # LLaVAOneVision1_5_textë¥¼ Qwen2Configë¡œ ë“±ë¡
                if "LLaVAOneVision1_5_text" not in CONFIG_MAPPING:
                    CONFIG_MAPPING.register("LLaVAOneVision1_5_text", Qwen2Config)
                    logging.info("âœ“ Registered LLaVAOneVision1_5_text as Qwen2Config")
                
            except Exception as e:
                logging.warning(f"âš ï¸ Could not register custom configs: {e}")

        # ë„¤íŠ¸ì›Œí¬ ì´ìŠˆë¡œ ì¸í•œ ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        retry_delay = 10  # seconds

        for attempt in range(max_retries):
            try:
                logging.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt + 1}/{max_retries}...")
                self.model = model_class.from_pretrained(
                    self.model_config["model_id"],
                    **model_kwargs,
                )
                break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
            except (OSError, ConnectionError, TimeoutError) as e:
                if attempt < max_retries - 1:
                    logging.warning(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                    logging.info(f"{retry_delay}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    import time
                    time.sleep(retry_delay)
                    retry_delay *= 2  # exponential backoff
                else:
                    logging.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìµœì¢… ì‹¤íŒ¨: {e}")
                    raise

        self.model.eval()

        # í”„ë¡œì„¸ì„œ ë¡œë“œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        for attempt in range(max_retries):
            try:
                logging.info(f"í”„ë¡œì„¸ì„œ ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt + 1}/{max_retries}...")
                if processor_class_name == "AutoTokenizer":
                    self.processor = processor_class.from_pretrained(
                        self.model_config["processor_id"],
                        trust_remote_code=True,
                    )
                else:
                    self.processor = processor_class.from_pretrained(
                        self.model_config["processor_id"],
                        trust_remote_code=True,
                    )
                break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
            except (OSError, ConnectionError, TimeoutError) as e:
                if attempt < max_retries - 1:
                    logging.warning(f"í”„ë¡œì„¸ì„œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                    logging.info(f"{retry_delay}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    import time
                    time.sleep(retry_delay)
                else:
                    logging.error(f"í”„ë¡œì„¸ì„œ ë‹¤ìš´ë¡œë“œ ìµœì¢… ì‹¤íŒ¨: {e}")
                    raise

        # Tokenizer ì ‘ê·¼
        if hasattr(self.processor, "tokenizer"):
            self.tokenizer = self.processor.tokenizer
        else:
            self.tokenizer = self.processor

        # Padding token ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # ëª¨ë¸ë³„ íŒ¨ë”© ë°©í–¥ ì„¤ì • (ê³µì‹ êµ¬í˜„ ê¸°ì¤€)
        # Decoder-only ëª¨ë¸: left padding (generation ìµœì í™”)
        # Encoder-decoder ëª¨ë¸: right padding (ê¸°ë³¸ê°’)
        padding_side = self._get_padding_side()
        self.tokenizer.padding_side = padding_side
        if hasattr(self.processor, 'tokenizer'):
            self.processor.tokenizer.padding_side = padding_side

        logging.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_config['model_id']}")
        logging.info(f"Padding side: {padding_side}")

    def _get_padding_side(self) -> str:
        """ëª¨ë¸ë³„ ì˜¬ë°”ë¥¸ íŒ¨ë”© ë°©í–¥ ë°˜í™˜ (ê³µì‹ êµ¬í˜„ ê¸°ì¤€)

        Returns:
            "left" for decoder-only models (better for generation)
            "right" for encoder-decoder models (default)
        """
        # Encoder-decoder ëª¨ë¸: right padding
        encoder_decoder_models = [
            "florence",  # Florence-2: encoder-decoder
        ]

        # BLIP2ì˜ ê²½ìš° language modelì— ë”°ë¼ ë‹¤ë¦„
        if "blip2" in self.model_name.lower():
            # Flan-T5 ê¸°ë°˜: encoder-decoder (right padding)
            if "flan" in self.model_config["model_id"].lower() or "t5" in self.model_config["model_id"].lower():
                return "right"
            # OPT ê¸°ë°˜: decoder-only (left padding)
            else:
                return "left"

        # InstructBLIPì˜ ê²½ìš°ë„ language modelì— ë”°ë¼ ë‹¤ë¦„
        if "instructblip" in self.model_name.lower():
            if "flan" in self.model_config["model_id"].lower() or "t5" in self.model_config["model_id"].lower():
                return "right"
            else:
                return "left"

        # Encoder-decoder ëª¨ë¸ ì²´í¬
        for model_type in encoder_decoder_models:
            if model_type in self.model_name.lower():
                return "right"

        # ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ decoder-only: left padding
        # LLaVA ì‹œë¦¬ì¦ˆ, Qwen2.5-VL, PaliGemma, Gemma-3, InternVL ë“±
        return "left"

    def _load_image(self, image_path: str) -> Image.Image:
        """ì´ë¯¸ì§€ ë¡œë“œ (ì „ì²˜ë¦¬ëŠ” processorì— ë§¡ê¹€)"""
        img_path = Path(image_path)
        if not img_path.is_file():
            raise FileNotFoundError(f"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {img_path}")

        # RGBë¡œ ë³€í™˜ë§Œ ìˆ˜í–‰, ë¦¬ì‚¬ì´ì¦ˆëŠ” ê° ëª¨ë¸ì˜ processorê°€ ì²˜ë¦¬
        img = Image.open(img_path).convert("RGB")
        return img

    def _format_prompt(self, instruction: str, image: Optional[Image.Image] = None) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©"""
        # Chat templateì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì˜ ê²½ìš°
        if self.model_config.get("use_chat_template", False):
            return None  # Will be handled differently in evaluate()

        # ì¼ë°˜ í…œí”Œë¦¿ ì‚¬ìš©
        template = self.model_config.get("prompt_template", "{instruction}")
        return template.format(instruction=instruction)

    def _prepare_chat_messages(self, instruction: str, image_path: Optional[str] = None) -> List[Dict]:
        """Chat template í˜•ì‹ì˜ ë©”ì‹œì§€ ì¤€ë¹„ (Gemma3, Qwen2.5-VLìš©)"""
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},  # Will be replaced with actual image
                    {"type": "text", "text": instruction}
                ]
            }
        ]
        return messages

    def evaluate(self) -> Dict[str, Any]:
        """í‰ê°€ ì‹¤í–‰"""
        logging.info(f"{'='*60}")
        logging.info(f"ğŸš€ í‰ê°€ ì‹œì‘: {self.model_name}")
        logging.info(f"{'='*60}")
        logging.info(f"ëª¨ë¸ ID: {self.model_config['model_id']}")
        logging.info(f"Chat template: {self.model_config.get('use_chat_template', False)}")
        logging.info(f"Vision utils: {self.model_config.get('requires_vision_utils', False)}")
        logging.info(f"Padding side: {self.tokenizer.padding_side}")

        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(self.data_csv)
        if self.max_samples is not None:
            df = df.head(self.max_samples)

        logging.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)} ìƒ˜í”Œ")
        logging.info(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}")

        predictions = []
        references = []
        instructions = []
        image_paths = []

        # ë°°ì¹˜ ì²˜ë¦¬
        num_batches = math.ceil(len(df) / self.batch_size)
        logging.info(f"ì´ ë°°ì¹˜ ìˆ˜: {num_batches}")
        logging.info(f"{'='*60}\n")

        with torch.inference_mode():
            for batch_idx in tqdm(range(num_batches), desc=f"Evaluating {self.model_name}"):
                logging.debug(f"\n{'â”€'*60}")
                logging.debug(f"ğŸ“¦ Batch {batch_idx+1}/{num_batches}")
                logging.debug(f"{'â”€'*60}")

                start_idx = batch_idx * self.batch_size
                end_idx = min(start_idx + self.batch_size, len(df))
                batch_df = df.iloc[start_idx:end_idx]

                logging.debug(f"ğŸ” [DEBUG] Processing samples {start_idx}-{end_idx-1}")

                batch_images = []
                batch_prompts = []
                batch_instructions = []
                batch_refs = []
                batch_paths = []

                for _, row in batch_df.iterrows():
                    try:
                        # ì´ë¯¸ì§€ ë¡œë“œ
                        img = self._load_image(row[self.image_column])
                        batch_images.append(img)

                        # í”„ë¡¬í”„íŠ¸/ë©”ì‹œì§€ ìƒì„±
                        instruction = str(row.get(self.instruction_column, "Describe the image."))

                        if self.model_config.get("use_chat_template", False):
                            # Chat template ì‚¬ìš© ëª¨ë¸
                            batch_prompts.append(instruction)  # Store raw instruction
                        else:
                            # ì¼ë°˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
                            prompt = self._format_prompt(instruction)
                            batch_prompts.append(prompt)

                        batch_instructions.append(instruction)
                        batch_refs.append(str(row.get(self.response_column, "")))
                        batch_paths.append(str(row.get(self.image_column, "")))
                    except Exception as e:
                        logging.warning(f"ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨ (idx={start_idx}): {e}")
                        continue

                if not batch_images:
                    continue

                try:
                    # í”„ë¡œì„¸ì„œë¡œ ì…ë ¥ ì¤€ë¹„
                    if self.model_config.get("is_florence", False):
                        # Florence-2: íŠ¹ë³„ ì²˜ë¦¬
                        # Florence-2ëŠ” ë°°ì¹˜ ì²˜ë¦¬ê°€ ì–´ë ¤ìš°ë¯€ë¡œ ê°œë³„ ì²˜ë¦¬
                        for inst, img, ref, path in zip(batch_prompts, batch_images, batch_refs, batch_paths):
                            # Florence-2ëŠ” task tokenì„ ì‚¬ìš©
                            task_prompt = self.model_config.get("prompt_template", "<MORE_DETAILED_CAPTION>")

                            inputs = self.processor(text=task_prompt, images=img, return_tensors="pt")
                            inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}

                            generated_ids = self.model.generate(
                                input_ids=inputs["input_ids"],
                                pixel_values=inputs["pixel_values"],
                                max_new_tokens=self.max_new_tokens,
                                num_beams=3,
                                do_sample=False
                            )
                            generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=False)[0]

                            # Post-process Florence-2 output
                            parsed_answer = self.processor.post_process_generation(
                                generated_text,
                                task=task_prompt,
                                image_size=(img.width, img.height)
                            )

                            # Extract text from parsed answer
                            pred_text = str(parsed_answer.get(task_prompt, ""))

                            predictions.append(pred_text)
                            references.append(ref)
                            instructions.append(inst)
                            image_paths.append(path)

                        continue  # Skip the rest of the batch processing

                    elif self.model_config.get("is_internvl", False):
                        # InternVL3.5: íŠ¹ë³„ ì²˜ë¦¬
                        # InternVL3.5ëŠ” chat ë©”ì„œë“œë¥¼ ì‚¬ìš©
                        for inst, img, ref, path in zip(batch_prompts, batch_images, batch_refs, batch_paths):
                            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                            pixel_values = load_image_internvl(img, input_size=448, max_num=12)
                            pixel_values = pixel_values.to(torch.bfloat16).to(self.device)

                            # í”„ë¡¬í”„íŠ¸ ìƒì„±
                            question = f"<image>\n{inst}"

                            generation_config = {
                                "max_new_tokens": self.max_new_tokens,
                                "do_sample": False,
                            }

                            # InternVLì˜ chat ë©”ì„œë“œ ì‚¬ìš©
                            response = self.model.chat(
                                self.tokenizer,
                                pixel_values,
                                question,
                                generation_config
                            )

                            predictions.append(response)
                            references.append(ref)
                            instructions.append(inst)
                            image_paths.append(path)

                        continue  # Skip the rest of the batch processing

                    elif self.model_config.get("use_chat_template", False):
                        # Chat template ì‚¬ìš© (Gemma3, Qwen2.5-VL, LLaVA-OneVision)
                        if self.model_config.get("requires_vision_utils", False):
                            # Qwen2.5-VL, LLaVA-OneVision: process_vision_info í•„ìš”
                            # ì´ ëª¨ë¸ë“¤ì€ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ì§€ë§Œ ê°œë³„ process_vision_info í˜¸ì¶œì´ í•„ìš”
                            from qwen_vl_utils import process_vision_info

                            logging.debug(f"ğŸ” [DEBUG] Using vision_utils path for {self.model_name}")
                            logging.debug(f"ğŸ” [DEBUG] Processing {len(batch_images)} samples individually")

                            # LLaVA-OneVisionê³¼ Qwen2.5-VLì€ ê°œë³„ ì²˜ë¦¬ê°€ ë” ì•ˆì •ì 
                            for sample_idx, (inst, img, ref, path) in enumerate(zip(batch_prompts, batch_images, batch_refs, batch_paths)):
                                try:
                                    logging.debug(f"ğŸ” [DEBUG] === Sample {sample_idx}/{len(batch_images)} ===")
                                    logging.debug(f"ğŸ” [DEBUG] Instruction: {inst[:80]}...")

                                    messages = [{
                                        "role": "user",
                                        "content": [
                                            {"type": "image", "image": img},
                                            {"type": "text", "text": inst}
                                        ]
                                    }]

                                    # Apply chat template
                                    text = self.processor.apply_chat_template(
                                        messages, tokenize=False, add_generation_prompt=True
                                    )
                                    logging.debug(f"ğŸ” [DEBUG] Chat template output length: {len(text)}")

                                    # Process vision info
                                    image_inputs, video_inputs = process_vision_info(messages)
                                    logging.debug(f"ğŸ” [DEBUG] image_inputs={len(image_inputs) if image_inputs else 0}, video_inputs={len(video_inputs) if video_inputs else 0}")

                                    # Prepare processor inputs
                                    inputs = self.processor(
                                        text=[text],
                                        images=image_inputs if image_inputs else None,
                                        videos=video_inputs if video_inputs else None,
                                        padding=True,
                                        return_tensors="pt",
                                    )
                                    logging.debug(f"ğŸ” [DEBUG] Processor output keys: {inputs.keys()}")
                                    if 'input_ids' in inputs:
                                        logging.debug(f"ğŸ” [DEBUG] input_ids shape: {inputs['input_ids'].shape}")

                                    # GPUë¡œ ì´ë™
                                    inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                                             for k, v in inputs.items()}

                                    # Generation
                                    gen_kwargs = {
                                        "max_new_tokens": self.max_new_tokens,
                                        "do_sample": False,
                                        "num_beams": 1,
                                    }
                                    if self.tokenizer.pad_token_id is not None:
                                        gen_kwargs["pad_token_id"] = self.tokenizer.pad_token_id
                                    if self.tokenizer.eos_token_id is not None:
                                        gen_kwargs["eos_token_id"] = self.tokenizer.eos_token_id

                                    logging.debug(f"ğŸ” [DEBUG] Starting generation...")
                                    outputs = self.model.generate(**inputs, **gen_kwargs)
                                    logging.debug(f"ğŸ” [DEBUG] Generation complete. Output shape: {outputs.shape}")

                                    # Decode
                                    # outputs: [batch_size, seq_len] í˜•íƒœ
                                    if isinstance(outputs, tuple):
                                        outputs = outputs[0]
                                    
                                    # ë°°ì¹˜ í¬ê¸°ê°€ 1ì´ë¯€ë¡œ ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤ ì¶”ì¶œ
                                    output_ids = outputs[0] if len(outputs.shape) > 1 else outputs
                                    
                                    # Prompt ê¸¸ì´ ê³„ì‚° ë° ì œê±°
                                    input_ids = inputs.get("input_ids")
                                    if input_ids is not None:
                                        # input_ids: [1, prompt_len]
                                        prompt_length = input_ids.shape[-1]
                                        logging.debug(f"ğŸ” [DEBUG] Prompt length: {prompt_length} tokens")
                                        logging.debug(f"ğŸ” [DEBUG] Output length: {len(output_ids)} tokens")
                                        
                                        # ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                                        generated_ids = output_ids[prompt_length:]
                                        logging.debug(f"ğŸ” [DEBUG] Generated length: {len(generated_ids)} tokens")
                                    else:
                                        generated_ids = output_ids
                                        logging.warning(f"âš ï¸ [DEBUG] No input_ids found, using full output")

                                    pred_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
                                    logging.debug(f"ğŸ” [DEBUG] Decoded prediction: {pred_text[:100]}...")

                                    predictions.append(pred_text)
                                    references.append(ref)
                                    instructions.append(inst)
                                    image_paths.append(path)

                                    logging.debug(f"ğŸ” [DEBUG] Sample {sample_idx} complete âœ“")

                                except Exception as e:
                                    import traceback
                                    logging.error(f"âŒ ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨ (batch={batch_idx}, sample={sample_idx}, model={self.model_name})")
                                    logging.error(f"   Error: {e}")
                                    logging.error(f"   Image: {path if 'path' in locals() else 'N/A'}")
                                    logging.error(f"   Instruction: {inst[:80] if 'inst' in locals() else 'N/A'}...")
                                    logging.error(f"   Traceback:\n{traceback.format_exc()}")
                                    # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ë¹ˆ ì˜ˆì¸¡ ì¶”ê°€
                                    predictions.append("")
                                    references.append(ref if 'ref' in locals() else "")
                                    instructions.append(inst if 'inst' in locals() else "")
                                    image_paths.append(path if 'path' in locals() else "")
                                    continue

                            continue  # Skip the rest of the batch processing
                        else:
                            # Gemma3: ì¼ë°˜ chat template (vision_utils ë¶ˆí•„ìš”)
                            # pixel_valuesê°€ ê°€ë³€ ê¸¸ì´ì´ë¯€ë¡œ ê°œë³„ ì²˜ë¦¬
                            logging.debug(f"ğŸ” [DEBUG] Using direct chat_template path for {self.model_name}")
                            logging.debug(f"ğŸ” [DEBUG] Processing {len(batch_images)} samples individually")

                            for sample_idx, (inst, img, ref, path) in enumerate(zip(batch_prompts, batch_images, batch_refs, batch_paths)):
                                try:
                                    logging.debug(f"ğŸ” [DEBUG] === Sample {sample_idx}/{len(batch_images)} ===")
                                    logging.debug(f"ğŸ” [DEBUG] Instruction: {inst[:80]}...")
                                    logging.debug(f"ğŸ” [DEBUG] Image size: {img.size}")

                                    messages = [{
                                        "role": "user",
                                        "content": [
                                            {"type": "image", "image": img},
                                            {"type": "text", "text": inst}
                                        ]
                                    }]

                                    # Use processor's apply_chat_template (single sample)
                                    # For Gemma3, apply_chat_template expects a single conversation
                                    logging.debug(f"ğŸ” [DEBUG] Calling apply_chat_template...")
                                    inputs = self.processor.apply_chat_template(
                                        messages,
                                        add_generation_prompt=True,
                                        tokenize=True,
                                        return_dict=True,
                                        return_tensors="pt"
                                    )

                                    logging.debug(f"ğŸ” [DEBUG] Input keys: {inputs.keys()}")
                                    if 'input_ids' in inputs:
                                        logging.debug(f"ğŸ” [DEBUG] input_ids shape: {inputs['input_ids'].shape}")
                                    if 'pixel_values' in inputs:
                                        logging.debug(f"ğŸ” [DEBUG] pixel_values shape: {inputs['pixel_values'].shape}")

                                    # GPUë¡œ ì´ë™
                                    logging.debug(f"ğŸ” [DEBUG] Moving tensors to {self.device}...")
                                    inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                                             for k, v in inputs.items()}

                                    # Generation
                                    gen_kwargs = {
                                        "max_new_tokens": self.max_new_tokens,
                                        "do_sample": False,
                                        "num_beams": 1,
                                    }
                                    if self.tokenizer.pad_token_id is not None:
                                        gen_kwargs["pad_token_id"] = self.tokenizer.pad_token_id
                                    if self.tokenizer.eos_token_id is not None:
                                        gen_kwargs["eos_token_id"] = self.tokenizer.eos_token_id

                                    logging.debug(f"ğŸ” [DEBUG] Generation kwargs: {gen_kwargs}")
                                    logging.debug(f"ğŸ” [DEBUG] Starting generation...")

                                    outputs = self.model.generate(**inputs, **gen_kwargs)

                                    logging.debug(f"ğŸ” [DEBUG] Generation complete. Output shape: {outputs.shape}")

                                    # Decode
                                    if isinstance(outputs, tuple):
                                        logging.debug(f"ğŸ” [DEBUG] Output is tuple, extracting first element")
                                        outputs = outputs[0]

                                    # Prompt ê¸¸ì´ ê³„ì‚°
                                    input_ids = inputs.get("input_ids")
                                    if input_ids is not None:
                                        prompt_length = (input_ids[0] != self.tokenizer.pad_token_id).sum().item()
                                        logging.debug(f"ğŸ” [DEBUG] Calculated prompt_length: {prompt_length} (non-pad tokens)")
                                        logging.debug(f"ğŸ” [DEBUG] Total output length: {outputs.shape[-1]}")
                                    else:
                                        prompt_length = 0
                                        logging.warning(f"âš ï¸ [DEBUG] No input_ids found, using prompt_length=0")

                                    # Prompt ë¶€ë¶„ ì œê±°
                                    generated_tokens = outputs[0][prompt_length:].tolist()
                                    logging.debug(f"ğŸ” [DEBUG] Generated tokens (after trim): {len(generated_tokens)} tokens")

                                    pred_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
                                    logging.debug(f"ğŸ” [DEBUG] Decoded prediction: {pred_text[:100]}...")

                                    predictions.append(pred_text)
                                    references.append(ref)
                                    instructions.append(inst)
                                    image_paths.append(path)

                                    logging.debug(f"ğŸ” [DEBUG] Sample {sample_idx} complete âœ“")

                                except Exception as e:
                                    import traceback
                                    logging.error(f"âŒ ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨ (batch={batch_idx}, sample={sample_idx}, model={self.model_name})")
                                    logging.error(f"   Error: {e}")
                                    logging.error(f"   Image: {path if 'path' in locals() else 'N/A'}")
                                    logging.error(f"   Instruction: {inst[:80] if 'inst' in locals() else 'N/A'}...")
                                    logging.error(f"   Traceback:\n{traceback.format_exc()}")
                                    # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ë¹ˆ ì˜ˆì¸¡ ì¶”ê°€í•˜ì—¬ ë ˆí¼ëŸ°ìŠ¤ì™€ ë§¤ì¹­ ìœ ì§€
                                    predictions.append("")
                                    references.append(ref if 'ref' in locals() else "")
                                    instructions.append(inst if 'inst' in locals() else "")
                                    image_paths.append(path if 'path' in locals() else "")
                                    continue

                            continue  # Skip the rest of the batch processing
                    else:
                        # ì¼ë°˜ ëª¨ë¸: ê¸°ì¡´ ë°©ì‹
                        inputs = self.processor(
                            text=batch_prompts,
                            images=batch_images,
                            return_tensors="pt",
                            padding=True,
                        )

                    # GPUë¡œ ì´ë™ (Qwen2.5-VL, LLaVA-OneVision ë°°ì¹˜ ì²˜ë¦¬)
                    logging.debug(f"ğŸ” [DEBUG] Moving batch tensors to {self.device}...")
                    inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}

                    # Generation
                    gen_kwargs = {
                        "max_new_tokens": self.max_new_tokens,
                        "do_sample": False,
                        "num_beams": 1,
                    }

                    if self.tokenizer.pad_token_id is not None:
                        gen_kwargs["pad_token_id"] = self.tokenizer.pad_token_id
                    if self.tokenizer.eos_token_id is not None:
                        gen_kwargs["eos_token_id"] = self.tokenizer.eos_token_id

                    logging.debug(f"ğŸ” [DEBUG] Generation kwargs: {gen_kwargs}")
                    logging.debug(f"ğŸ” [DEBUG] Starting batch generation for {len(batch_prompts)} samples...")

                    outputs = self.model.generate(**inputs, **gen_kwargs)

                    logging.debug(f"ğŸ” [DEBUG] Batch generation complete. Output shape: {outputs.shape}")

                    # Decode
                    if isinstance(outputs, tuple):
                        logging.debug(f"ğŸ” [DEBUG] Output is tuple, extracting first element")
                        outputs = outputs[0]

                    # Prompt ê¸¸ì´ ê³„ì‚°
                    input_ids = inputs.get("input_ids")
                    if input_ids is not None:
                        prompt_lengths = (input_ids != self.tokenizer.pad_token_id).sum(dim=1).cpu()
                        logging.debug(f"ğŸ” [DEBUG] Prompt lengths: {prompt_lengths.tolist()}")
                    else:
                        prompt_lengths = torch.zeros(len(batch_prompts), dtype=torch.long)
                        logging.warning(f"âš ï¸ [DEBUG] No input_ids found, using zeros for prompt_lengths")

                    # ë°°ì¹˜ ë””ì½”ë”©
                    for i, output in enumerate(outputs):
                        # Prompt ë¶€ë¶„ ì œê±°
                        cut = int(prompt_lengths[i].item()) if i < len(prompt_lengths) else 0
                        generated_tokens = output[cut:].tolist()

                        logging.debug(f"ğŸ” [DEBUG] Sample {i}: cut={cut}, generated_tokens={len(generated_tokens)}")

                        pred_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
                        logging.debug(f"ğŸ” [DEBUG] Sample {i}: prediction={pred_text[:100]}...")

                        predictions.append(pred_text)
                        references.append(batch_refs[i])
                        instructions.append(batch_instructions[i])
                        image_paths.append(batch_paths[i])

                    logging.debug(f"ğŸ” [DEBUG] Batch processing complete âœ“")

                except Exception as e:
                    logging.error(f"ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨ (batch={batch_idx}): {e}")
                    continue

        # í‰ê°€ ì™„ë£Œ ìš”ì•½
        logging.info(f"\n{'='*60}")
        logging.info(f"âœ… í‰ê°€ ì™„ë£Œ: {self.model_name}")
        logging.info(f"{'='*60}")
        logging.info(f"ì´ ì˜ˆì¸¡ ìˆ˜: {len(predictions)}")
        logging.info(f"ì´ ë ˆí¼ëŸ°ìŠ¤ ìˆ˜: {len(references)}")
        
        empty_pred_count = sum(1 for p in predictions if not p.strip())
        logging.info(f"ë¹ˆ ì˜ˆì¸¡ ìˆ˜: {empty_pred_count}")
        
        if empty_pred_count > 0:
            logging.warning(f"âš ï¸ {empty_pred_count}ê°œì˜ ë¹ˆ ì˜ˆì¸¡ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            # ì²˜ìŒ 5ê°œì˜ ë¹ˆ ì˜ˆì¸¡ ìƒ˜í”Œ ì •ë³´ ì¶œë ¥
            empty_indices = [i for i, p in enumerate(predictions) if not p.strip()][:5]
            for idx in empty_indices:
                logging.warning(f"  - ìƒ˜í”Œ {idx}: image={image_paths[idx] if idx < len(image_paths) else 'N/A'}")
                logging.warning(f"    instruction={instructions[idx][:80] if idx < len(instructions) else 'N/A'}...")
        
        logging.info(f"{'='*60}\n")

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        if len(predictions) == 0:
            logging.error("âŒ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤! í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {
                "model_name": self.model_name,
                "model_id": self.model_config["model_id"],
                "num_samples": 0,
                "error": "No predictions generated",
                "metrics": {},
            }
        
        logging.info("ğŸ“Š ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        metrics = compute_text_metrics(predictions, references)

        # ê²°ê³¼ ì €ì¥
        results = {
            "model_name": self.model_name,
            "model_id": self.model_config["model_id"],
            "num_samples": len(predictions),
            "image_size": f"{self.image_size}x{self.image_size}",
            "metrics": metrics,
        }

        logging.info(f"\nğŸ“ˆ ë©”íŠ¸ë¦­ ê²°ê³¼:")
        for metric_name, metric_value in metrics.items():
            if isinstance(metric_value, (int, float)):
                logging.info(f"  {metric_name}: {metric_value:.4f}")

        # ë©”íŠ¸ë¦­ ì €ì¥ (JSONë§Œ)
        metrics_file = self.output_dir / "metrics.json"
        with open(metrics_file, "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        logging.info(f"ë©”íŠ¸ë¦­ ì €ì¥: {metrics_file}")

        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ (CSV)
        predictions_df = pd.DataFrame({
            "image_path": image_paths,
            "instruction": instructions,
            "reference": references,
            "prediction": predictions,
        })
        predictions_file = self.output_dir / "predictions.csv"
        predictions_df.to_csv(predictions_file, index=False, encoding="utf-8")
        logging.info(f"ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {predictions_file}")

        return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(description="VLM ëª¨ë¸ í‰ê°€ (í•™ìŠµ ì—†ì´)")
    parser.add_argument("--data_csv", type=str, required=True, help="í‰ê°€ ë°ì´í„° CSV íŒŒì¼")
    parser.add_argument(
        "--models",
        type=str,
        nargs="+",
        default=["llava-1.5-7b", "blip2-opt-2.7b"],
        help=f"í‰ê°€í•  ëª¨ë¸ ì´ë¦„ë“¤. ì‚¬ìš© ê°€ëŠ¥: {list(VLM_MODELS.keys())}",
    )
    parser.add_argument("--output_dir", type=str, default="eval_results", help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--batch_size", type=int, default=1, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--max_samples", type=int, default=None, help="ìµœëŒ€ í‰ê°€ ìƒ˜í”Œ ìˆ˜ (ë””ë²„ê¹…ìš©)")
    parser.add_argument("--device", type=str, default="cuda", help="ë””ë°”ì´ìŠ¤ (cuda/cpu)")
    parser.add_argument("--image_size", type=int, default=224, help="ì´ë¯¸ì§€ í¬ê¸° (ì •ì‚¬ê°í˜•)")
    parser.add_argument("--max_new_tokens", type=int, default=128, help="ìµœëŒ€ ìƒì„± í† í° ìˆ˜")
    parser.add_argument("--image_column", type=str, default="url", help="ì´ë¯¸ì§€ ê²½ë¡œ ì»¬ëŸ¼ëª…")
    parser.add_argument("--instruction_column", type=str, default="query", help="ì§ˆë¬¸ ì»¬ëŸ¼ëª… (ê¸°ë³¸: query)")
    parser.add_argument("--response_column", type=str, default="annotation", help="ì •ë‹µ ì»¬ëŸ¼ëª… (ê¸°ë³¸: annotation)")
    parser.add_argument("--log_level", type=str, default="INFO", help="ë¡œê·¸ ë ˆë²¨")

    args = parser.parse_args()

    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    # ê° ëª¨ë¸ í‰ê°€
    all_results = []
    for model_name in args.models:
        logging.info(f"\n{'='*60}")
        logging.info(f"ëª¨ë¸ í‰ê°€ ì‹œì‘: {model_name}")
        logging.info(f"{'='*60}\n")

        try:
            evaluator = VLMEvaluator(
                model_name=model_name,
                data_csv=args.data_csv,
                output_dir=args.output_dir,
                batch_size=args.batch_size,
                max_samples=args.max_samples,
                device=args.device,
                image_size=args.image_size,
                max_new_tokens=args.max_new_tokens,
                image_column=args.image_column,
                instruction_column=args.instruction_column,
                response_column=args.response_column,
            )
            
            results = evaluator.evaluate()
            all_results.append(results)

            # ë©”íŠ¸ë¦­ ì¶œë ¥
            logging.info(f"\n{model_name} í‰ê°€ ê²°ê³¼:")
            for key, value in results["metrics"].items():
                logging.info(f"  {key}: {value:.4f}")

        except Exception as e:
            logging.error(f"ëª¨ë¸ {model_name} í‰ê°€ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            continue

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    if all_results:
        summary_file = Path(args.output_dir) / "all_models_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        logging.info(f"\nì „ì²´ ê²°ê³¼ ì €ì¥: {summary_file}")

        # ë¹„êµí‘œ ìƒì„±
        logging.info("\n" + "="*60)
        logging.info("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        logging.info("="*60)
        for result in all_results:
            logging.info(f"\n{result['model_name']}:")
            for key, value in result["metrics"].items():
                if isinstance(value, (int, float)):
                    logging.info(f"  {key}: {value:.4f}")


if __name__ == "__main__":
    main()
