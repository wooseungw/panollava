from pathlib import Path
import torch
from torch.utils.data import DataLoader
from PIL import Image
from transformers import default_data_collator
from panovlm.dataset import ChatPanoDataset, ChatPanoEvalDataset, custom_collate_fn

from panovlm.processors.pano_llava_processor import PanoLLaVAProcessor
from panovlm.processors.image import PanoramaImageProcessor
from panovlm.processors.text import TextTokenizer
from panovlm.model import PanoramaVLM
from train import VLMModule  # LightningModule ë˜í¼ ì‚¬ìš©

print("--- 1. ê°€ìƒ ë°ì´í„° ë° í™˜ê²½ ì„¤ì • ---")
csv_path = "data/quic360/downtest.csv"
if not Path(csv_path).exists():
    raise FileNotFoundError(f"CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {csv_path}")
print("ê°€ìƒ CSV íŒŒì¼ ê²½ë¡œ:", csv_path)
print("\n--- 2. ë°ì´í„° ë¡œë”© íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ---")
VISION_NAME = "google/siglip-base-patch16-224"
LM_NAME = "Qwen/Qwen2.5-0.5B"
BATCH_SIZE = 1
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# DEVICE = "mps" if torch.backends.mps.is_available() else DEVICE  # MPS ì§€ì› ì—¬ë¶€ í™•ì¸

img_proc = PanoramaImageProcessor()
txt_tok = TextTokenizer(LM_NAME)
processor = PanoLLaVAProcessor(img_proc, txt_tok, max_length=128)

dataset = ChatPanoDataset(csv_path, processor, txt_tok.tok)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn)

# --- í…ŒìŠ¤íŠ¸ìš© generate ë°ì´í„°ì…‹ ì¤€ë¹„ ---
test_dataset = ChatPanoEvalDataset(csv_path, processor, txt_tok.tok)
test_sample = test_dataset[0]  # ì²« ìƒ˜í”Œë§Œ ì‚¬ìš©
print(f"ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜: {len(dataset)}, ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")

print("\n--- 3. ëª¨ë¸ í•™ìŠµ ê³¼ì • í…ŒìŠ¤íŠ¸ (VLMModule ë˜í¼ ê¸°ë°˜) ---")
try:
    # VLMModuleì€ LightningModuleì´ì§€ë§Œ, ë‚´ë¶€ì ìœ¼ë¡œ PanoramaVLMì„ ë˜í•‘í•˜ë©° stageë³„ freeze ë¡œì§ì„ í¬í•¨
    model = VLMModule(
        vision_name=VISION_NAME,
        lm_name=LM_NAME,
        resampler="mlp",
        stage="vision",
        lr=1e-5
    )
    model = model.to(DEVICE)
    model.train()

    batch = next(iter(dataloader))
    print(f"ë°°ì¹˜ í¬ê¸°: {batch['pixel_values'].shape}, ì…ë ¥ ID í¬ê¸°: {batch['input_ids'].shape}")
    
    batch = {k: (v.to(DEVICE) if hasattr(v, 'to') else v) for k, v in batch.items()}
    print("=======ì…ë ¥ í…ìŠ¤íŠ¸=======")
    for i in batch["input_text"]:
        print(i[:200])
        print("-----------------------------------")
    print("======================")
    # Vision stage í…ŒìŠ¤íŠ¸
    print("\n=== Vision Stage í…ŒìŠ¤íŠ¸ ===")
    model._freeze_for_stage("vision")
    optimizer_vision = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-5)
    outputs = model.forward(**batch)
    loss = outputs["loss"]
    print(f"âœ… Vision ìˆœì „íŒŒ ì„±ê³µ! Loss: {loss.item():.4f}")
    optimizer_vision.zero_grad()
    loss.backward()
    print("âœ… Vision ì—­ì „íŒŒ ì„±ê³µ! ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì™„ë£Œ.")
    optimizer_vision.step()
    print("âœ… Vision ì˜µí‹°ë§ˆì´ì € ìŠ¤í… ì„±ê³µ!")
    
    # Finetune stage í…ŒìŠ¤íŠ¸ (freeze ìë™ ì ìš©)
    print("\n=== Finetune Stage í…ŒìŠ¤íŠ¸ (LLM & Vision Encoder Frozen) ===")
    model._freeze_for_stage("finetune")
    optimizer_finetune = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-5)
    model._stage_key = "finetune"
    outputs = model.forward(**batch)
    loss = outputs["loss"]
    print(f"âœ… Finetune ìˆœì „íŒŒ ì„±ê³µ! Loss: {loss.item():.4f}")
    optimizer_finetune.zero_grad()
    loss.backward()
    print("âœ… Finetune ì—­ì „íŒŒ ì„±ê³µ! ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì™„ë£Œ.")
    optimizer_finetune.step()
    print("âœ… Finetune ì˜µí‹°ë§ˆì´ì € ìŠ¤í… ì„±ê³µ!")

    # Generation í…ŒìŠ¤íŠ¸ - ì‹¤ì œ ë°ì´í„°ì™€ ë”ë¯¸ ë°ì´í„° ê²°í•©
    print("\n=== Generation Stage í…ŒìŠ¤íŠ¸ ===")
    model.eval()
    
    # === 1. ì‹¤ì œ ë°ì´í„° í…ŒìŠ¤íŠ¸ ===
    print("1. ì‹¤ì œ ë°ì´í„° Generate í…ŒìŠ¤íŠ¸:")
    with torch.no_grad():
        # test_sample ì‚¬ìš© (ChatPanoEvalDatasetì—ì„œ ë¡œë“œëœ ìƒ˜í”Œ)
        pixel_values = test_sample["pixel_values"].unsqueeze(0).to(DEVICE)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        input_ids = test_sample.get("input_ids", None)
        if input_ids is not None:
            input_ids = input_ids.unsqueeze(0).to(DEVICE)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        
        # 1-1. ìº¡ì…”ë‹ í…ŒìŠ¤íŠ¸ (ì§ˆë¬¸ ì—†ìŒ)
        print("   1-1. ìº¡ì…”ë‹ í…ŒìŠ¤íŠ¸:")
        try:
            caption_result = model.model.generate(
                pixel_values=pixel_values,
                max_new_tokens=50,
                temperature=0.7
            )
            print(f"   âœ… ìº¡ì…”ë‹ ì„±ê³µ! í…ìŠ¤íŠ¸: '{caption_result['text'][0][:100]}'")
            
            # ë¹ˆ ë¬¸ìì—´ ì²´í¬
            if len(caption_result['text'][0]) == 0:
                print("   âš ï¸  ê²½ê³ : ë¹ˆ ë¬¸ìì—´ ìƒì„±ë¨")
            else:
                print(f"   âœ… í…ìŠ¤íŠ¸ ê¸¸ì´: {len(caption_result['text'][0])}")
        except Exception as e:
            print(f"   âŒ ìº¡ì…”ë‹ ì‹¤íŒ¨: {e}")
        
        # 1-2. VQA í…ŒìŠ¤íŠ¸ (ì§ˆë¬¸-ë‹µë³€)
        if input_ids is not None:
            print("   1-2. VQA í…ŒìŠ¤íŠ¸:")
            try:
                vqa_result = model.model.generate(
                    pixel_values=pixel_values,
                    input_ids=input_ids,
                    max_new_tokens=64,
                    temperature=0.8
                )
                print(f"   âœ… VQA ì„±ê³µ! ë‹µë³€: '{vqa_result['text'][0][:100]}'")
            except Exception as e:
                print(f"   âŒ VQA ì‹¤íŒ¨: {e}")
    
    # === 2. ë”ë¯¸ ë°ì´í„° í…ŒìŠ¤íŠ¸ (ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤) ===
    print("\n2. ë”ë¯¸ ë°ì´í„° Generate í…ŒìŠ¤íŠ¸:")
    
    # 2-1. ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸
    print("   2-1. ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸:")
    batch_sizes = [1, 2]
    for batch_size in batch_sizes:
        print(f"      ë°°ì¹˜ í¬ê¸° {batch_size}:")
        dummy_pixel_values = torch.randn(batch_size, 6, 3, 224, 224).to(DEVICE)
        
        with torch.no_grad():
            try:
                result = model.model.generate(
                    pixel_values=dummy_pixel_values,
                    max_new_tokens=30,
                    temperature=0.7
                )
                
                for i, text in enumerate(result['text']):
                    print(f"         ìƒ˜í”Œ {i}: '{text[:50]}...' (ê¸¸ì´: {len(text)})")
                
                # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                empty_count = sum(1 for text in result['text'] if len(text) == 0)
                if empty_count == 0:
                    print(f"      âœ… ë°°ì¹˜ {batch_size} ì„±ê³µ! ë¹ˆ ë¬¸ìì—´ ì—†ìŒ")
                else:
                    print(f"      âš ï¸  ë°°ì¹˜ {batch_size}: ë¹ˆ ë¬¸ìì—´ {empty_count}ê°œ ë°œê²¬")
                    
            except Exception as e:
                print(f"      âŒ ë°°ì¹˜ {batch_size} ì‹¤íŒ¨: {e}")
    
    # 2-2. 4D ì´ë¯¸ì§€ ì…ë ¥ í…ŒìŠ¤íŠ¸
    print("   2-2. 4D ì´ë¯¸ì§€ ì…ë ¥ í…ŒìŠ¤íŠ¸:")
    dummy_4d = torch.randn(1, 3, 224, 224).to(DEVICE)  # ë·° ì°¨ì› ì—†ìŒ
    
    with torch.no_grad():
        try:
            result_4d = model.model.generate(
                pixel_values=dummy_4d,
                max_new_tokens=25,
                temperature=0.6
            )
            print(f"      âœ… 4D ì…ë ¥ ì„±ê³µ: '{result_4d['text'][0][:60]}'")
        except Exception as e:
            print(f"      âŒ 4D ì…ë ¥ ì‹¤íŒ¨: {e}")
    
    # 2-3. ê·¹ë‹¨ì  íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸
    print("   2-3. ê·¹ë‹¨ì  íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸:")
    dummy_5d = torch.randn(1, 6, 3, 224, 224).to(DEVICE)
    
    test_params = [
        {"max_new_tokens": 5, "temperature": 0.1, "name": "ì§§ì€+ë‚®ì€ì˜¨ë„"},
        {"max_new_tokens": 100, "temperature": 1.5, "name": "ê¸´+ë†’ì€ì˜¨ë„"},
        {"max_new_tokens": 15, "temperature": 2.0, "name": "ë§¤ìš°ë†’ì€ì˜¨ë„"}
    ]
    
    for params in test_params:
        with torch.no_grad():
            try:
                result_param = model.model.generate(
                    pixel_values=dummy_5d,
                    max_new_tokens=params["max_new_tokens"],
                    temperature=params["temperature"]
                )
                print(f"      âœ… {params['name']}: '{result_param['text'][0][:40]}'")
            except Exception as e:
                print(f"      âŒ {params['name']} ì‹¤íŒ¨: {e}")
    
    # 2-4. ì§ˆë¬¸ê³¼ í•¨ê»˜ ë”ë¯¸ í…ŒìŠ¤íŠ¸
    print("   2-4. ë”ë¯¸ ë°ì´í„° + ì§ˆë¬¸ í…ŒìŠ¤íŠ¸:")
    question = "What can you see in this panoramic image?"
    question_tokens = model.model.tokenizer(question, return_tensors="pt").to(DEVICE)
    
    with torch.no_grad():
        try:
            result_q = model.model.generate(
                pixel_values=dummy_5d,
                input_ids=question_tokens["input_ids"],
                max_new_tokens=40,
                temperature=0.8
            )
            print(f"      âœ… ì§ˆë¬¸+ë”ë¯¸ ì„±ê³µ: '{result_q['text'][0][:60]}'")
        except Exception as e:
            print(f"      âŒ ì§ˆë¬¸+ë”ë¯¸ ì‹¤íŒ¨: {e}")
    
    # === 3. ìƒì„± í’ˆì§ˆ ë¶„ì„ ===
    print("\n3. ìƒì„± í’ˆì§ˆ ë¶„ì„:")
    print("   ğŸ”§ ì£¼ìš” ê°œì„ ì‚¬í•­:")
    improvements = [
        "âœ… ê°•ì œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ë¡œ ë¹ˆ ë¬¸ìì—´ ë°©ì§€",
        "âœ… ë°°ì¹˜ ì°¨ì› ì¼ê´€ì„± í™•ë³´",
        "âœ… ì•ˆì „í•œ ì˜ˆì™¸ ì²˜ë¦¬ ë° fallback",
        "âœ… ìµœì†Œ ê¸¸ì´ ë³´ì¥",
        "âœ… ì¡°ê¸° ì¢…ë£Œ ë°©ì§€",
        "âœ… ê°œì„ ëœ ìƒì„± íŒŒë¼ë¯¸í„°"
    ]
    
    for improvement in improvements:
        print(f"      {improvement}")

    print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼: ë°ì´í„° ë¡œë”© ë° í•™ìŠµ íŒŒì´í”„ë¼ì¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")

except Exception as e:
    import traceback
    print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    traceback.print_exc()
