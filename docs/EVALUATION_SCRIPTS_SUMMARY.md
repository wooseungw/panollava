# í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© ë° ë©”íŠ¸ë¦­ í†µì¼ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” í”„ë¡œì íŠ¸ì˜ ì„¸ ê°€ì§€ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ì˜ ì‚¬ìš©ì²˜ì™€ í‰ê°€ ì§€í‘œ í†µì¼ ì‘ì—…ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“Š í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ê°œìš”

### 1. scripts/eval.py (ë©”ì¸ PanoramaVLM í‰ê°€)

**ëª©ì **: í•™ìŠµëœ PanoramaVLM ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸

**ì‚¬ìš©ì²˜**:
- í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
- `scripts/train.sh`ì—ì„œ ì°¸ì¡°ë¨
- ì„¤ì • íŒŒì¼ ê¸°ë°˜ ìë™ ëª¨ë¸ ë””ë ‰í† ë¦¬ íƒìƒ‰

**í‰ê°€ ì§€í‘œ**:
- âœ… BLEU-4 (corpus-level, smoothing ì ìš©)
- âœ… METEOR (wordnet ê¸°ë°˜ ë™ì˜ì–´ ë§¤ì¹­)
- âœ… ROUGE-L (ìµœì¥ ê³µí†µ ë¶€ë¶„ìˆ˜ì—´)
- âœ… SPICE (íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ + ì˜ë¯¸ ìœ ì‚¬ë„ ëŒ€ì•ˆ)
- âœ… CIDEr (consensus-based ì´ë¯¸ì§€ ì„¤ëª… í‰ê°€)

**ì‚¬ìš© ì˜ˆì‹œ**:
```bash
python scripts/eval.py \
    --config configs/default.yaml \
    --csv-input data/quic360/test.csv
```

**íŠ¹ì§•**:
- ëª¨ë¸ ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰ (`resolve_model_dir`)
- LoRA ê°€ì¤‘ì¹˜ ìë™ ë¡œë“œ
- HF ëª¨ë¸/ì²´í¬í¬ì¸íŠ¸ ëª¨ë‘ ì§€ì›
- ê°€ì¥ ì™„ì „í•˜ê³  ì•ˆì •ì ì¸ ë©”íŠ¸ë¦­ êµ¬í˜„

---

### 2. scripts/evaluate_vlm_models.py (HF VLM ëª¨ë¸ ë¹„êµ)

**ëª©ì **: HuggingFaceì˜ ë‹¤ì–‘í•œ VLM ëª¨ë¸ë“¤ì„ ë™ì¼ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹„êµ í‰ê°€

**ì‚¬ìš©ì²˜**:
- `scripts/run_vlm_ablation.sh` (ablation study)
- `scripts/test_vlm_eval.sh` (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
- `docs/VLM_EVALUATION_GUIDE.md`ì— ë¬¸ì„œí™”ë¨

**ì§€ì› ëª¨ë¸**:
- LLaVA 1.5/1.6
- BLIP2 (OPT/Flan-T5)
- InstructBLIP
- Qwen2.5-VL (3B)
- InternVL2 (2B)
- Gemma 3 (4B)

**í‰ê°€ ì§€í‘œ** (ìˆ˜ì • í›„):
- âœ… **eval.pyì˜ calculate_evaluation_metrics ì¬ì‚¬ìš©** (ìš°ì„ )
- âœ… ë¡œì»¬ êµ¬í˜„ í´ë°± (í˜¸í™˜ì„± ìœ ì§€)
- âœ… ì™„ì „íˆ ë™ì¼í•œ ë©”íŠ¸ë¦­ ë³´ì¥

**ì‚¬ìš© ì˜ˆì‹œ**:
```bash
# ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ
python scripts/evaluate_vlm_models.py \
    --data_csv data/quic360/test.csv \
    --models llava-1.5-7b blip2-opt-2.7b qwen2-vl-2b \
    --output_dir results \
    --batch_size 2

# ë‹¨ì¼ ëª¨ë¸ í‰ê°€
python scripts/evaluate_vlm_models.py \
    --data_csv data/quic360/test.csv \
    --models internvl2-2b \
    --max_samples 50
```

**ë³€ê²½ ì‚¬í•­** (2025-01-XX):
- âœ… `scripts.eval` ëª¨ë“ˆì—ì„œ `calculate_evaluation_metrics` ì„í¬íŠ¸
- âœ… `compute_text_metrics` í•¨ìˆ˜ê°€ eval.py êµ¬í˜„ ìš°ì„  ì‚¬ìš©
- âœ… ì‹¤íŒ¨ ì‹œ ë¡œì»¬ êµ¬í˜„ìœ¼ë¡œ ìë™ í´ë°±
- âœ… ë¡œê·¸ì— ì‚¬ìš©ëœ êµ¬í˜„ ëª…ì‹œ

**ì¶œë ¥**:
```
results/ablation/{model_name}/
â”œâ”€â”€ metrics.json          # í‰ê°€ ë©”íŠ¸ë¦­ (BLEU, METEOR, ROUGE, SPICE, CIDEr)
â””â”€â”€ predictions.csv       # ì˜ˆì¸¡/ì •ë‹µ ë¹„êµ
```

---

### 3. scripts/vlm_evaluate.py (LoRA íŠœë‹ VLM í‰ê°€)

**ëª©ì **: LoRA ì–´ëŒ‘í„°ê°€ ì ìš©ëœ VLM ëª¨ë¸ í‰ê°€

**ì‚¬ìš©ì²˜**:
- `results/vlm_lora_ablation/` ë””ë ‰í† ë¦¬ì˜ LoRA ì‹¤í—˜ í‰ê°€
- ìë™ ì–´ëŒ‘í„° íƒìƒ‰ (lora_adapter/, final/, checkpoints/)

**í‰ê°€ ì§€í‘œ**:
- âœ… **eval.pyì˜ í•¨ìˆ˜ë“¤ì„ ì§ì ‘ ì¬ì‚¬ìš©** (ì´ë¯¸ êµ¬í˜„ë¨)
  ```python
  from scripts.eval import (
      calculate_evaluation_metrics,
      save_and_log_results,
  )
  ```

**ì‚¬ìš© ì˜ˆì‹œ**:
```bash
# LoRA ì‹¤í—˜ í‰ê°€ (ìë™ íƒìƒ‰)
python scripts/vlm_evaluate.py \
    --csv data/quic360/test.csv \
    --run qwen_vl_chat__lora_r16 \
    --results-root results/vlm_lora_ablation

# ëª…ì‹œì  ëª¨ë¸ ì§€ì •
python scripts/vlm_evaluate.py \
    --csv data/quic360/test.csv \
    --model-id Qwen/Qwen2-VL-2B-Instruct \
    --lora-path results/vlm_lora_ablation/qwen2_vl__r8/lora_adapter
```

**íŠ¹ì§•**:
- âœ… ì²˜ìŒë¶€í„° eval.py ì¬ì‚¬ìš©ìœ¼ë¡œ ì„¤ê³„ë¨
- âœ… ë©”íŠ¸ë¦­ ì¼ê´€ì„± ë³´ì¥
- âœ… ì–´ëŒ‘í„° ìë™ íƒìƒ‰
- âœ… ë² ì´ìŠ¤ ëª¨ë¸ ì •ë³´ ìë™ ì¶”ì¶œ

---

## ğŸ”„ ë©”íŠ¸ë¦­ í†µì¼ ì‘ì—… ìš”ì•½

### ë³€ê²½ ì „ ë¬¸ì œì 
- `evaluate_vlm_models.py`ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° ë¡œì§ì„ **ì¬êµ¬í˜„**
- eval.pyì™€ ë¯¸ë¬˜í•œ ì°¨ì´ ê°€ëŠ¥ì„±
- ìœ ì§€ë³´ìˆ˜ ì¤‘ë³µ

### ë³€ê²½ í›„ í•´ê²°
1. **evaluate_vlm_models.py ìˆ˜ì •**:
   - `scripts.eval.calculate_evaluation_metrics` ì„í¬íŠ¸
   - `compute_text_metrics` í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ eval.py êµ¬í˜„ ìš°ì„  ì‚¬ìš©
   - ì„í¬íŠ¸ ì‹¤íŒ¨ ì‹œ ë¡œì»¬ êµ¬í˜„ìœ¼ë¡œ í´ë°± (í•˜ìœ„ í˜¸í™˜ì„±)

2. **vlm_evaluate.py**:
   - ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë¨ (ìˆ˜ì • ë¶ˆí•„ìš”)

### ë©”íŠ¸ë¦­ ê³„ì‚° íë¦„

```
ëª¨ë“  í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
        â†“
scripts/eval.py::calculate_evaluation_metrics()
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  BLEU-4   (corpus_bleu)          â”‚
    â”‚  METEOR   (meteor_score)          â”‚
    â”‚  ROUGE-L  (rouge_scorer)          â”‚
    â”‚  SPICE    (íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ + ëŒ€ì•ˆ)   â”‚
    â”‚  CIDEr    (cider_scorer)          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ í‰ê°€ ì§€í‘œ ìƒì„¸

### BLEU-4
- **ë²”ìœ„**: 0.0 ~ 1.0 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **ì„¤ëª…**: 4-gram precision with smoothing
- **ìš©ë„**: ê¸°ê³„ ë²ˆì—­/ìƒì„± í’ˆì§ˆ ì¸¡ì •

### METEOR
- **ë²”ìœ„**: 0.0 ~ 1.0 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **ì„¤ëª…**: ë™ì˜ì–´/ì–´í˜• ë³€í™” ê³ ë ¤í•œ F1 ì ìˆ˜
- **ìš©ë„**: BLEUë³´ë‹¤ ì¸ê°„ íŒë‹¨ê³¼ ë†’ì€ ìƒê´€ê´€ê³„

### ROUGE-L
- **ë²”ìœ„**: 0.0 ~ 1.0 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **ì„¤ëª…**: ìµœì¥ ê³µí†µ ë¶€ë¶„ìˆ˜ì—´ ê¸°ë°˜ F1
- **ìš©ë„**: ìš”ì•½ í’ˆì§ˆ í‰ê°€

### SPICE
- **ë²”ìœ„**: 0.0 ~ 1.0 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **ì„¤ëª…**: ì˜ë¯¸ ê·¸ë˜í”„ ê¸°ë°˜ ì´ë¯¸ì§€ ìº¡ì…˜ í‰ê°€
- **ìš©ë„**: ì˜ë¯¸ì  ì •í™•ì„± ì¸¡ì •
- **íŠ¹ì§•**: eval.pyëŠ” íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ + ëŒ€ì•ˆ êµ¬í˜„ í¬í•¨

### CIDEr
- **ë²”ìœ„**: 0.0 ~ 10.0 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **ì„¤ëª…**: Consensus-based ì´ë¯¸ì§€ ì„¤ëª… í‰ê°€
- **ìš©ë„**: ì—¬ëŸ¬ ì •ë‹µê³¼ì˜ ì¼ì¹˜ë„

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
```bash
# evaluate_vlm_models.py í…ŒìŠ¤íŠ¸
bash scripts/test_vlm_eval.sh

# ê²°ê³¼ í™•ì¸
cat eval_results/test_run/ablation/blip2-opt-2.7b/metrics.json
```

### ì „ì²´ ablation study
```bash
bash scripts/run_vlm_ablation.sh
```

### ê²°ê³¼ ë¹„êµ
```bash
# ëª¨ë“  ëª¨ë¸ì˜ ë©”íŠ¸ë¦­ ë¹„êµ
python -c "
import json
from pathlib import Path

results_dir = Path('results/ablation')
for model_dir in results_dir.iterdir():
    if model_dir.is_dir():
        metrics_file = model_dir / 'metrics.json'
        if metrics_file.exists():
            with open(metrics_file) as f:
                metrics = json.load(f)
            print(f'{model_dir.name}:')
            for k, v in metrics.items():
                print(f'  {k}: {v:.4f}')
"
```

---

## ğŸ“ ê¶Œì¥ ì‚¬í•­

1. **ìƒˆë¡œìš´ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ì‹œ**:
   - í•­ìƒ `scripts.eval.calculate_evaluation_metrics` ì¬ì‚¬ìš©
   - DataFrame í˜•ì‹ìœ¼ë¡œ ì˜ˆì¸¡/ì •ë‹µ ì „ë‹¬
   ```python
   from scripts.eval import calculate_evaluation_metrics

   df = pd.DataFrame({
       'prediction': predictions,
       'reference': references,
   })

   metrics = calculate_evaluation_metrics(
       df,
       output_dir=Path('eval_results'),
       timestamp='20250101_120000',
       prefix='my_model'
   )
   ```

2. **ë©”íŠ¸ë¦­ ì¶”ê°€/ìˆ˜ì • ì‹œ**:
   - `scripts/eval.py`ì—ì„œë§Œ ìˆ˜ì •
   - ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìë™ìœ¼ë¡œ ìƒˆ ë©”íŠ¸ë¦­ ì‚¬ìš©

3. **ë””ë²„ê¹…**:
   - ë¡œê·¸ì—ì„œ "âœ“ Using shared evaluation metrics from scripts/eval.py" í™•ì¸
   - í´ë°± ê²½ê³  ë°œìƒ ì‹œ ì„í¬íŠ¸ ê²½ë¡œ ì ê²€

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [VLM_EVALUATION_GUIDE.md](./VLM_EVALUATION_GUIDE.md): HF VLM ëª¨ë¸ í‰ê°€ ìƒì„¸ ê°€ì´ë“œ
- [IMPROVED_USAGE.md](./IMPROVED_USAGE.md): ì „ì²´ í•™ìŠµ/í‰ê°€ íŒŒì´í”„ë¼ì¸

---

## ë³€ê²½ ì´ë ¥

- **2025-01-XX**: `evaluate_vlm_models.py` ë©”íŠ¸ë¦­ í†µì¼ ì‘ì—… ì™„ë£Œ
- **2025-01-XX**: ë¬¸ì„œ ì‘ì„±

