# í‰ê°€ ì „ì²˜ë¦¬ ê°€ì´ë“œ

## ğŸ¯ ëª©í‘œ

VLM í‰ê°€ ì‹œ **ê³µì •í•˜ê³  ì¼ê´€ëœ** ë¹„êµë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.

---

## ğŸ“‹ ì „ì²˜ë¦¬ ë ˆë²¨ ì •ì˜

### Level 0: Raw (ì›ë³¸)
**ì ìš©**: ì—†ìŒ
**ëª©ì **: ëª¨ë¸ì˜ ì‹¤ì œ ì¶œë ¥ ê·¸ëŒ€ë¡œ í‰ê°€
**ì‚¬ìš© ì‚¬ë¡€**:
- í”„ë¡œë•ì…˜ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜
- ëª¨ë¸ ê°„ ìˆœìˆ˜ ì¶œë ¥ í’ˆì§ˆ ë¹„êµ

```python
predictions = [model_output]  # ê·¸ëŒ€ë¡œ
references = [ground_truth]    # ê·¸ëŒ€ë¡œ
```

**ì¥ì **: ì‹¤ì œ ì‚¬ìš© í™˜ê²½ê³¼ ì¼ì¹˜
**ë‹¨ì **: í† í°í™”/ëŒ€ì†Œë¬¸ì ì°¨ì´ë¡œ ë©”íŠ¸ë¦­ ë³€ë™ í¼

---

### Level 1: Basic Cleanup (ê¸°ë³¸ ì •ë¦¬) â­ **ê¶Œì¥**
**ì ìš©**:
- íŠ¹ìˆ˜ í† í° ì œê±° (`<image>`, `<|im_start|>` ë“±)
- ì—­í•  íƒœê·¸ ì œê±° (`ASSISTANT:`, `USER:` ë“±)
- í”„ë¡¬í”„íŠ¸ ëˆ„ìˆ˜ ì œê±°
- ê³¼ë„í•œ ê³µë°± ì •ë¦¬

```python
def basic_cleanup(text: str) -> str:
    """Level 1: ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë§Œ ì œê±°"""
    import re

    # 1. íŠ¹ìˆ˜ í† í° ì œê±°
    text = re.sub(r"<\|.*?\|>|<image>|</image>|<img>|</img>", " ", text, flags=re.I)
    text = re.sub(r"<vision_start>|<vision_end>|<image_pad>", " ", text, flags=re.I)

    # 2. ì—­í•  íƒœê·¸ ì œê±°
    text = re.sub(r"^(USER:|ASSISTANT:|Question:|Answer:)\s*", "", text, flags=re.I)

    # 3. ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+", " ", text).strip()

    return text
```

**ì¥ì **:
- ëª¨ë¸ ì¶œë ¥ì˜ ì˜ë¯¸ ë³´ì¡´
- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì•„í‹°íŒ©íŠ¸ë§Œ ì œê±°
- ëŒ€ì†Œë¬¸ì, êµ¬ë‘ì  ë“± ì›ë³¸ ìœ ì§€

**ë‹¨ì **:
- í† í°í™” ì°¨ì´ë¡œ ì¸í•œ BLEU ë³€ë™ ì—¬ì „íˆ ì¡´ì¬

---

### Level 2: Standard Normalization (í‘œì¤€ ì •ê·œí™”)
**ì ìš©**: Level 1 +
- í‘œì¤€ í† í°í™” (Moses/sacrebleu '13a')
- êµ¬ë‘ì  ì •ê·œí™”

```python
import sacrebleu

def standard_normalize(text: str) -> str:
    """Level 2: í‘œì¤€ í† í°í™” ì ìš©"""
    # Level 1 ë¨¼ì € ì ìš©
    text = basic_cleanup(text)

    # sacrebleu í† í¬ë‚˜ì´ì € ì ìš©
    tokenized = sacrebleu.tokenize(text, tokenize="13a")

    return tokenized
```

**ì¥ì **:
- í•™ìˆ  ë²¤ì¹˜ë§ˆí¬(COCO, NoCaps)ì™€ ì¼ê´€ì„±
- í† í°í™” ì°¨ì´ë¡œ ì¸í•œ ë³€ë™ ìµœì†Œí™”

**ë‹¨ì **:
- ëŒ€ì†Œë¬¸ì ì—¬ì „íˆ ë¯¼ê°

---

### Level 3: Aggressive Normalization (ê°•í•œ ì •ê·œí™”) âš ï¸
**ì ìš©**: Level 2 +
- ì†Œë¬¸ìí™”
- êµ¬ë‘ì  ì œê±°

```python
def aggressive_normalize(text: str) -> str:
    """Level 3: ê°•í•œ ì •ê·œí™” (ì£¼ì˜ í•„ìš”)"""
    import re

    # Level 2 ë¨¼ì €
    text = standard_normalize(text)

    # ì†Œë¬¸ìí™”
    text = text.lower()

    # êµ¬ë‘ì  ì œê±° (ì„ íƒì )
    # text = re.sub(r'[^\w\s]', ' ', text)

    return text
```

**âš ï¸ ì£¼ì˜**:
- ê³ ìœ ëª…ì‚¬ ì •ë³´ ì†ì‹¤ ("New York" â†’ "new york")
- ì•½ì–´ ì˜ë¯¸ ë³€ê²½ ("US" â†’ "us")
- ì‹¤ì œ í’ˆì§ˆ ê°€ë¦¼

**ì‚¬ìš© ì‚¬ë¡€**:
- ì˜ë¯¸ì  ìœ ì‚¬ë„ì—ë§Œ ì§‘ì¤‘í•˜ëŠ” ablation study
- ëŒ€ì†Œë¬¸ì/êµ¬ë‘ì ì´ í‰ê°€ ëª©ì ì´ ì•„ë‹Œ ê²½ìš°

---

## ğŸ” BLEU ê³„ì‚° ë°©ì‹ ì„ íƒ

### ì˜µì…˜ 1: NLTK BLEU (í˜„ì¬ ì‚¬ìš©)
```python
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

smoothing = SmoothingFunction().method1  # ë˜ëŠ” method4
ref_tokens = [[ref.split()] for ref in refs]
pred_tokens = [pred.split() for pred in preds]
bleu = corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)
```

**íŠ¹ì§•**:
- ê°„ë‹¨í•œ `split()` í† í°í™”
- Method1: ì•½í•œ ìŠ¤ë¬´ë”© (ì§§ì€ ë¬¸ì¥ì— ë¶ˆë¦¬)
- Method4: ê°•í•œ ìŠ¤ë¬´ë”© (ë” ê´€ëŒ€)

---

### ì˜µì…˜ 2: sacrebleu (í‘œì¤€) â­
```python
import sacrebleu

bleu = sacrebleu.corpus_bleu(
    preds, [refs],
    smooth_method="exp",      # í‘œì¤€ ìŠ¤ë¬´ë”©
    lowercase=False,          # ëŒ€ì†Œë¬¸ì ë³´ì¡´
    tokenize="13a",           # í‘œì¤€ í† í¬ë‚˜ì´ì €
    use_effective_order=True  # ì§§ì€ ë¬¸ì¥ ì•ˆì •í™”
)
score = bleu.score / 100.0  # 0~1 ìŠ¤ì¼€ì¼
```

**íŠ¹ì§•**:
- COCO/NoCaps ë²¤ì¹˜ë§ˆí¬ í‘œì¤€
- ì¬í˜„ì„± ë†’ìŒ
- ë…¼ë¬¸ ì‘ì„± ì‹œ ì‹ ë¢°ë„ â†‘

---

## ğŸ’¼ í”„ë¡œì íŠ¸ë³„ ê¶Œì¥ ì‚¬í•­

### 1. PanoramaVLM (í•™ìŠµëœ ëª¨ë¸) - Level 1 + NLTK
**ì´ìœ **:
- ê¸°ì¡´ ê²°ê³¼ì™€ì˜ ì¼ê´€ì„± ìœ ì§€
- ëª¨ë¸ ê°œì„  íš¨ê³¼ ëª…í™•íˆ ì¸¡ì •

```python
# scripts/eval.py
def calculate_evaluation_metrics(df, output_dir, timestamp, prefix):
    # Level 1 ì •ë¦¬ë§Œ ì ìš©
    df['prediction'] = df['prediction'].apply(basic_cleanup)
    df['reference'] = df['reference'].apply(basic_cleanup)

    # NLTK BLEU (ê¸°ì¡´ ìœ ì§€)
    # ... ê¸°ì¡´ ì½”ë“œ
```

---

### 2. HF VLM ë¹„êµ (evaluate_vlm_models.py) - Level 1 + sacrebleu
**ì´ìœ **:
- ë‹¤ë¥¸ ì—°êµ¬ì™€ ë¹„êµ ê°€ëŠ¥ì„±
- ëª¨ë¸ ê°„ ê³µì •í•œ ë¹„êµ

```python
# scripts/evaluate_vlm_models.py
def compute_text_metrics(predictions, references):
    # Level 1 ì •ë¦¬
    preds = [basic_cleanup(p) for p in predictions]
    refs = [basic_cleanup(r) for r in references]

    # sacrebleu ì‚¬ìš©
    bleu = sacrebleu.corpus_bleu(
        preds, [refs],
        smooth_method="exp",
        lowercase=False,  # ëŒ€ì†Œë¬¸ì ë³´ì¡´
        tokenize="13a"
    )
    metrics["bleu4"] = bleu.score / 100.0
```

---

### 3. Ablation Study - ë‘ ê°€ì§€ ë²„ì „ ì œê³µ
**ì´ìœ **:
- ì›ë³¸(Level 0)ìœ¼ë¡œ ì‹¤ì œ í’ˆì§ˆ í™•ì¸
- ì •ê·œí™”(Level 1-2)ë¡œ ë©”íŠ¸ë¦­ ì•ˆì •í™”

```python
# ë‘ ê°€ì§€ ë©”íŠ¸ë¦­ ì„¸íŠ¸ ì €ì¥
metrics_raw = compute_metrics(preds_raw, refs_raw)         # Level 0
metrics_clean = compute_metrics(preds_clean, refs_clean)   # Level 1

results = {
    "raw": metrics_raw,
    "normalized": metrics_clean
}
```

---

## ğŸ¯ ìµœì¢… ê¶Œì¥: ë‹¨ê³„ì  ì ìš©

### Phase 1: Level 1ë§Œ ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ì— ì ìš© âœ…

**ë³€ê²½ ì‚¬í•­**:
```python
# ëª¨ë“  eval ìŠ¤í¬ë¦½íŠ¸ì— ì¶”ê°€
def basic_cleanup(text: str) -> str:
    """í”„ë¡¬í”„íŠ¸ ì•„í‹°íŒ©íŠ¸ ì œê±° (ì˜ë¯¸ ë³´ì¡´)"""
    import re
    text = re.sub(r"<\|.*?\|>|<image>|</image>|<img>|</img>", " ", text, flags=re.I)
    text = re.sub(r"<vision_start>|<vision_end>|<image_pad>", " ", text, flags=re.I)
    text = re.sub(r"^(USER:|ASSISTANT:|Question:|Answer:)\s*", "", text, flags=re.I)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# ë©”íŠ¸ë¦­ ê³„ì‚° ì „ì— ì ìš©
predictions = [basic_cleanup(p) for p in raw_predictions]
references = [basic_cleanup(r) for r in raw_references]
```

**íš¨ê³¼**:
- âœ… í”„ë¡¬í”„íŠ¸ ëˆ„ìˆ˜ ì œê±°
- âœ… íŠ¹ìˆ˜ í† í° ê°„ì„­ ì œê±°
- âœ… ì˜ë¯¸ ë³´ì¡´ (ëŒ€ì†Œë¬¸ì, êµ¬ë‘ì  ìœ ì§€)
- âœ… ì‹¤ì œ í’ˆì§ˆ ë°˜ì˜

---

### Phase 2: sacrebleu ë„ì… (ì„ íƒì ) ğŸ”„

**ì¡°ê±´ë¶€ ì ìš©**:
- HF VLM ë¹„êµ í‰ê°€: sacrebleu ì‚¬ìš©
- PanoramaVLM í‰ê°€: NLTK ìœ ì§€ (ê¸°ì¡´ ê²°ê³¼ì™€ ë¹„êµ)

```python
# evaluate_vlm_models.pyë§Œ ë³€ê²½
import sacrebleu

def compute_text_metrics_hf(predictions, references):
    # Level 1 ì •ë¦¬
    preds = [basic_cleanup(p) for p in predictions]
    refs = [basic_cleanup(r) for r in references]

    # sacrebleu
    bleu = sacrebleu.corpus_bleu(preds, [refs], smooth_method="exp", lowercase=False, tokenize="13a")
    metrics["bleu4"] = bleu.score / 100.0
    # ...
```

---

### Phase 3: ì†Œë¬¸ìí™”ëŠ” í•˜ì§€ ì•ŠìŒ âŒ

**ì´ìœ **:
- ê³ ìœ ëª…ì‚¬/ì•½ì–´ ì •ë³´ ì†ì‹¤
- ì‹¤ì œ í’ˆì§ˆ ì™œê³¡
- í•™ìˆ  ë²¤ì¹˜ë§ˆí¬ë„ ëŒ€ë¶€ë¶„ case-sensitive

---

## ğŸ“Š ì˜ˆìƒ íš¨ê³¼

### Level 0 (Raw) â†’ Level 1 (Basic Cleanup)
- BLEU-4: **2~5%p ìƒìŠ¹** (í”„ë¡¬í”„íŠ¸ ëˆ„ìˆ˜ ì œê±° íš¨ê³¼)
- ì˜ë¯¸ ë³´ì¡´: **100%**
- ì‹¤ì œ í’ˆì§ˆ ë°˜ì˜: **ë†’ìŒ**

### Level 1 â†’ Level 2 (sacrebleu)
- BLEU-4: **1~3%p ìƒìŠ¹** (í† í°í™” ì•ˆì •í™”)
- ë²¤ì¹˜ë§ˆí¬ ë¹„êµ: **ê°€ëŠ¥**
- ì‹¤ì œ í’ˆì§ˆ ë°˜ì˜: **ë†’ìŒ**

### Level 2 â†’ Level 3 (lowercase)
- BLEU-4: **3~8%p ìƒìŠ¹** âš ï¸ (ì¸ìœ„ì )
- ì •ë³´ ì†ì‹¤: **ì¤‘ê°„**
- ì‹¤ì œ í’ˆì§ˆ ë°˜ì˜: **ë‚®ìŒ**

---

## ğŸ”— êµ¬í˜„ ìš°ì„ ìˆœìœ„

1. **ì¦‰ì‹œ ì ìš©**: `basic_cleanup()` í•¨ìˆ˜ë¥¼ ëª¨ë“  eval ìŠ¤í¬ë¦½íŠ¸ì— ì¶”ê°€
2. **ë‹¨ê¸° ì ìš©**: HF VLM ë¹„êµì—ë§Œ sacrebleu ë„ì…
3. **ì¥ê¸° ê²€í† **: ë©€í‹° ë ˆí¼ëŸ°ìŠ¤ ë°ì´í„°ì…‹ êµ¬ì¶• (BLEU ì•ˆì •í™”)
4. **í•˜ì§€ ì•ŠìŒ**: ì†Œë¬¸ìí™”, êµ¬ë‘ì  ì œê±°

---

## ë³€ê²½ ì´ë ¥

- **2025-01-XX**: ì´ˆì•ˆ ì‘ì„±
- **2025-01-XX**: Level 1 ê¶Œì¥ì•ˆ í™•ì •

