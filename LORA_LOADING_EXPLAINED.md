# LoRA ν‚¤ "λ„λ½" λ΅κ·Έ μ„¤λ… - μ •μƒ μ‘λ™ μ¤‘μ…λ‹λ‹¤!

## β… κ²°λ΅ : μ •μƒ μ‘λ™

**"λ„λ½λ ν‚¤ 311κ°" λ΅κ·Έλ” μ •μƒμ μΈ ν„μƒμ…λ‹λ‹¤!**

### μ”μ•½
- μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ‹ 311κ° λ„λ½ ν‚¤κ°€ λ³΄μ΄μ§€λ§ **λ¬Έμ  μ•„λ‹**
- LoRAλ” `lora_weights` ν΄λ”μ—μ„ λ³„λ„λ΅ λ΅λ“λ¨ β“
- μµμΆ…μ μΌλ΅ LoRAκ°€ μ¬λ°”λ¥΄κ² μ μ©λ¨ β“

---

## μ™ "λ„λ½λ ν‚¤" λ΅κ·Έκ°€ λ‚νƒ€λ‚λ”κ°€?

### 1. ν•™μµ μ‹ μ €μ¥λλ” κµ¬μ΅° (PeftModel)

```python
# ν•™μµ μ¤‘ PanoramaVLMμ€ PeftModelμ„ ν¬ν•¨
state_dict = {
    'model.language_model.base_model.model.model.layers.0.*.lora_A.default.weight',
    'model.language_model.base_model.model.model.layers.0.*.lora_B.default.weight',
    'model.language_model.base_model.model.lm_head.weight',
    ...
}
```

**ν”„λ¦¬ν”½μ¤**: `model.language_model.base_model.model.*` (PeftModel κµ¬μ΅°)

### 2. λ΅λ”© μ‹ κΈ°λ€ν•λ” κµ¬μ΅° (μΌλ° λ¨λΈ)

```python
# μƒλ΅ μ΄κΈ°ν™”λ PanoramaVLMμ€ μΌλ° λ¨λΈ
expected_keys = {
    'language_model.model.layers.0.self_attn.q_proj.weight',
    'language_model.model.layers.0.self_attn.k_proj.weight',
    'language_model.model.embed_tokens.weight',
    ...
}
```

**ν”„λ¦¬ν”½μ¤**: `language_model.model.*` (μΌλ° λ¨λΈ κµ¬μ΅°)

### 3. ν”„λ¦¬ν”½μ¤ λ¶μΌμΉ β†’ λ„λ½ ν‚¤ λ°μƒ

```
μ²΄ν¬ν¬μΈνΈ ν‚¤: model.language_model.base_model.model.model.layers.X.*
λ¨λΈ κΈ°λ€ ν‚¤:                language_model.model.layers.X.*
                           β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘β†‘
                           λ¶μΌμΉ! β†’ λ„λ½μΌλ΅ λ¶„λ¥
```

**λ΅κ·Έ κ²°κ³Ό**:
```
β™οΈ  κ°€μ¤‘μΉ λ΅λ”© μ¤‘...
   - λ΅λ“λ ν‚¤: 944 (vision, resampler, projector λ“±)
   - λ„λ½λ ν‚¤: 311 (language_modelμ LoRA ν‚¤)
   - μμƒμΉ λ»ν• ν‚¤: 703 (μ²΄ν¬ν¬μΈνΈμ base_model.* ν”„λ¦¬ν”½μ¤ ν‚¤)
```

### 4. κ·Έλμ„ λ³„λ„λ΅ LoRA ν΄λ”μ—μ„ λ΅λ“

```python
# model.py:1098-1110
π” LoRA κ°€μ¤‘μΉ μλ™ κ°μ§€: runs/.../lora_weights
π”§ LoRA κ°€μ¤‘μΉ λ΅λ”©: runs/.../lora_weights

# PeftModel.from_pretrained() μ‹¤ν–‰
Converting to PeftModel.from_pretrained...
β“ LoRA weights loaded via PeftModel
   β… LoRA λ΅λ”© μ„±κ³µ - Rank: 32, Alpha: 64
```

---

## μ‹¤μ  μ‘λ™ ν™•μΈ

### μ²΄ν¬ν¬μΈνΈ λ΅λ”© κ³Όμ •

```
1λ‹¨κ³„: μ²΄ν¬ν¬μΈνΈ λ΅λ“ (.ckpt νμΌ)
β”β”€ Vision encoder β“
β”β”€ Resampler β“
β”β”€ Projector β“
β””β”€ Language model β†’ 311κ° λ„λ½ (ν”„λ¦¬ν”½μ¤ λ¶μΌμΉ, μ •μƒ)

2λ‹¨κ³„: LoRA μλ™ κ°μ§€
β””β”€ lora_weights ν΄λ” λ°κ²¬ β“

3λ‹¨κ³„: LoRA λ³„λ„ λ΅λ“ (PeftModel)
β””β”€ β… LoRA μ μ© μ„±κ³µ (r=32, alpha=64)
```

### LoRA μ •λ³΄ ν™•μΈ

```python
lora_info = model.get_lora_info()
# {
#   'is_lora_enabled': True,      β“
#   'lora_r': 32,                  β“
#   'lora_alpha': 64,              β“
#   'target_modules': {...}        β“
# }
```

---

## νμΌ κµ¬μ΅°

### μ²΄ν¬ν¬μΈνΈ νμΌ (.ckpt)
```
μ΄ 1255κ° ν‚¤:
β”β”€ vision/resampler/projector: 552κ°
β”‚  β””β”€ β“ λ΅λ“ μ„±κ³µ
β”‚
β””β”€ language_model: 703κ°
   β”β”€ model.language_model.base_model.model.lm_head.weight
   β”β”€ model.language_model.base_model.model.model.layers.X.*.lora_A.default.weight
   β””β”€ model.language_model.base_model.model.model.layers.X.*.lora_B.default.weight
      β””β”€ β— ν”„λ¦¬ν”½μ¤ λ¶μΌμΉλ΅ λ„λ½ (μ •μƒ - λ³„λ„ λ΅λ“λ¨)
```

### lora_weights ν΄λ” (adapter_model.bin)
```
μ΄ 394κ° ν‚¤:
β”β”€ LoRA adapter: 392κ°
β”‚  β”β”€ base_model.model.model.layers.X.*.lora_A.weight
β”‚  β””β”€ base_model.model.model.layers.X.*.lora_B.weight
β”‚     β””β”€ β“ PeftModel.from_pretrained()λ΅ λ΅λ“
β”‚
β””β”€ modules_to_save: 2κ°
   β”β”€ base_model.model.model.embed_tokens.weight
   β””β”€ base_model.model.lm_head.weight
      β””β”€ β“ ν•¨κ» λ΅λ“
```

---

## μ½”λ“ νλ¦„ (μ •μƒ)

```
eval.py:1153
β””β”€> load_model_and_lora()
    β””β”€> PanoramaVLM.from_checkpoint()
        β”β”€ 1λ‹¨κ³„: μ²΄ν¬ν¬μΈνΈ λ΅λ“
        β”‚   β””β”€ load_state_dict(strict=False)
        β”‚       β”β”€ μ„±κ³µ: vision, resampler, projector (944κ°)
        β”‚       β””β”€ λ„λ½: language_model (311κ°) β† ν”„λ¦¬ν”½μ¤ λ¶μΌμΉ (μ •μƒ)
        β”‚
        β”β”€ 2λ‹¨κ³„: LoRA μλ™ κ°μ§€
        β”‚   β””β”€ lora_weights ν΄λ” λ°κ²¬
        β”‚
        β””β”€ 3λ‹¨κ³„: LoRA λ΅λ“
            β””β”€ PeftModel.from_pretrained()
                β””β”€ β… LoRA μ μ© μ„±κ³µ (r=32, alpha=64)
```

**μµμΆ… κ²°κ³Ό**:
- Vision encoder: μ²΄ν¬ν¬μΈνΈμ—μ„ λ΅λ“ β“
- Resampler: μ²΄ν¬ν¬μΈνΈμ—μ„ λ΅λ“ β“
- Projector: μ²΄ν¬ν¬μΈνΈμ—μ„ λ΅λ“ β“
- Language model: lora_weightsμ—μ„ λ΅λ“ β“

---

## μ§„λ‹¨ λ°©λ²•

### 1. LoRA λ΅λ”© ν™•μΈ
```bash
python scripts/eval.py --config configs/default.yaml --csv-input data/test.csv 2>&1 | grep "LoRA"
```

**μ„±κ³µ μ‹ (μ •μƒ):**
```
π” LoRA κ°€μ¤‘μΉ μλ™ κ°μ§€: runs/.../lora_weights
π”§ LoRA κ°€μ¤‘μΉ λ΅λ”©: runs/.../lora_weights
β“ LoRA weights loaded via PeftModel
   β… LoRA λ΅λ”© μ„±κ³µ - Rank: 32, Alpha: 64
```

**μ‹¤ν¨ μ‹ (λ¬Έμ ):**
```
Warning: PEFT not available. Cannot load LoRA weights.
β LoRA λ΅λ”© μ‹¤ν¨
```

### 2. LoRA μ •λ³΄ ν™•μΈ (Python)
```python
from src.panovlm.models.model import PanoramaVLM

model = PanoramaVLM.from_checkpoint(
    'runs/.../checkpoint.ckpt',
    device='cuda'
)

lora_info = model.get_lora_info()
print(lora_info)
# {'is_lora_enabled': True, 'lora_r': 32, 'lora_alpha': 64, ...}
```

### 3. PEFT μ„¤μΉ ν™•μΈ
```bash
source /data/3_lib/miniconda3/bin/activate pano
python -c "import peft; print(f'PEFT {peft.__version__} installed')"
# PEFT 0.17.1 installed
```

---

## FAQ

### Q1: "λ„λ½λ ν‚¤ 311κ°"κ°€ μ •μƒμΈκ°€μ”?
**A**: λ„¤, μ •μƒμ…λ‹λ‹¤. μ²΄ν¬ν¬μΈνΈμ LoRA ν‚¤λ” ν”„λ¦¬ν”½μ¤κ°€ λ‹¤λ¥΄κΈ° λ•λ¬Έμ— λ„λ½λμ§€λ§,
λ³„λ„μ `lora_weights` ν΄λ”μ—μ„ μ¬λ°”λ¥΄κ² λ΅λ“λ©λ‹λ‹¤.

### Q2: LoRA μ—†μ΄ ν‰κ°€ν•λ ¤λ©΄?
**A**: `lora_weights` ν΄λ”λ¥Ό μ κ±°ν•κ±°λ‚ λ‹¤λ¥Έ κ³³μΌλ΅ μ΄λ™ν•λ©΄ λ©λ‹λ‹¤.
```bash
mv runs/.../lora_weights runs/.../lora_weights.bak
```

### Q3: μ²΄ν¬ν¬μΈνΈμ— LoRAκ°€ ν¬ν•¨λλ” μ΄μ λ”?
**A**: PyTorch Lightningμ΄ μ „μ²΄ λ¨λΈμ„ μ €μ¥ν•  λ• PeftModelμ κµ¬μ΅°κΉμ§€ ν¬ν•¨λκΈ° λ•λ¬Έμ…λ‹λ‹¤.
ν•μ§€λ§ λ΅λ”© μ‹μ—λ” ν”„λ¦¬ν”½μ¤ λ¶μΌμΉλ΅ μ‚¬μ©λμ§€ μ•μµλ‹λ‹¤.

### Q4: lora_weights ν΄λ”κ°€ μ—†μΌλ©΄?
**A**: μλ™ κ°μ§€κ°€ μ‹¤ν¨ν•κ³  LoRA μ—†μ΄ ν‰κ°€λ©λ‹λ‹¤.
λ΅κ·Έμ—μ„ `β οΈ  LoRA κ²½λ΅κ°€ μ΅΄μ¬ν•μ§€ μ•μµλ‹λ‹¤` λ©”μ‹μ§€λ¥Ό ν™•μΈν•  μ μμµλ‹λ‹¤.

### Q5: λ®μ–΄μ”μΈ λ• λ¬Έμ κ°€ λλ‚μ”?
**A**: μ•„λ‹μ”, λ¬Έμ μ—†μµλ‹λ‹¤. μ²΄ν¬ν¬μΈνΈμ LoRA ν‚¤λ” ν”„λ¦¬ν”½μ¤ λ¶μΌμΉλ΅ λ¬΄μ‹λκ³ ,
`lora_weights` ν΄λ”μ LoRAκ°€ `PeftModel.from_pretrained()`λ¥Ό ν†µν•΄ μ¬λ°”λ¥΄κ² μ μ©λ©λ‹λ‹¤.
λ‘ μ†μ¤κ°€ μ¶©λν•μ§€ μ•μµλ‹λ‹¤.

---

## μ”μ•½ ν‘

| λ΅κ·Έ λ©”μ‹μ§€ | μλ―Έ | μ •μƒ? |
|------------|------|-------|
| "λ„λ½λ ν‚¤: 311" | μ²΄ν¬ν¬μΈνΈμ LoRA ν‚¤ ν”„λ¦¬ν”½μ¤ λ¶μΌμΉ | β… μ •μƒ |
| "μμƒμΉ λ»ν• ν‚¤: 703" | μ²΄ν¬ν¬μΈνΈμ base_model ν”„λ¦¬ν”½μ¤ | β… μ •μƒ |
| "π” LoRA κ°€μ¤‘μΉ μλ™ κ°μ§€" | lora_weights ν΄λ” λ°κ²¬ | β… μ •μƒ |
| "β… LoRA λ΅λ”© μ„±κ³µ" | LoRAκ°€ μ¬λ°”λ¥΄κ² μ μ©λ¨ | β… μ •μƒ |
| "β LoRA λ΅λ”© μ‹¤ν¨" | PEFT μ—†κ±°λ‚ νμΌ μ¤λ¥ | β λ¬Έμ  |

---

## κ²°λ΅ 

### ν•µμ‹¬ ν¬μΈνΈ

1. **"λ„λ½λ ν‚¤" λ΅κ·Έλ” λ¬΄μ‹ν•΄λ„ λ©λ‹λ‹¤**
   - ν”„λ¦¬ν”½μ¤ λ¶μΌμΉλ΅ μΈν• μ •μƒμ μΈ ν„μƒ

2. **"LoRA λ΅λ”© μ„±κ³µ" λ©”μ‹μ§€κ°€ λ‚μ¤λ©΄ μ •μƒ μ‘λ™ μ¤‘μ…λ‹λ‹¤**
   - lora_weightsμ—μ„ μ¬λ°”λ¥΄κ² λ΅λ“λ¨

3. **ν‰κ°€ κ²°κ³Όλ” ν•™μµλ LoRAκ°€ μ μ©λ μƒνƒμ…λ‹λ‹¤**
   - μ²΄ν¬ν¬μΈνΈ LoRAμ™€ lora_weights LoRAκ°€ μ¶©λν•μ§€ μ•μ

4. **λ®μ–΄μ”μ°κΈ° λ¬Έμ  μ—†μµλ‹λ‹¤**
   - μ²΄ν¬ν¬μΈνΈ: ν”„λ¦¬ν”½μ¤ λ¶μΌμΉλ΅ λ¬΄μ‹
   - lora_weights: PeftModelλ΅ μ¬λ°”λ¥΄κ² μ μ©
   - λ‘ μ†μ¤κ°€ λ…λ¦½μ μΌλ΅ μ²λ¦¬λ¨

### ν™•μΈ λ°©λ²•

ν‰κ°€ μ‹¤ν–‰ μ‹ λ‹¤μ λ΅κ·Έκ°€ λ‚μ¤λ©΄ μ •μƒμ…λ‹λ‹¤:
```
β… LoRA λ΅λ”© μ„±κ³µ - Rank: 32, Alpha: 64
```

μ΄ λ©”μ‹μ§€κ°€ λ³΄μ΄λ©΄ LoRAκ°€ μ¬λ°”λ¥΄κ² μ μ©λ μƒνƒλ΅ ν‰κ°€κ°€ μ§„ν–‰λ©λ‹λ‹¤!
