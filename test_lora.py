#!/usr/bin/env python3
"""
LoRA μ§€μ› μ—¬λ¶€λ¥Ό ν…μ¤νΈν•λ” μ¤ν¬λ¦½νΈ
"""

import os
import sys
import argparse
from pathlib import Path

# ν”„λ΅μ νΈ λ£¨νΈλ¥Ό κ²½λ΅μ— μ¶”κ°€
sys.path.insert(0, str(Path(__file__).parent))

from panovlm.model import PanoramaVLM

def test_lora_support():
    """LoRA μ§€μ› κΈ°λ¥μ„ ν…μ¤νΈν•©λ‹λ‹¤."""
    
    print("=== PanoLLaVA LoRA μ§€μ› ν…μ¤νΈ ===\n")
    
    # 1. PEFT λΌμ΄λΈλ¬λ¦¬ κ°€μ©μ„± ν™•μΈ
    try:
        from peft import LoraConfig, get_peft_model, TaskType
        print("β“ PEFT λΌμ΄λΈλ¬λ¦¬κ°€ μ„¤μΉλμ–΄ μμµλ‹λ‹¤.")
    except ImportError as e:
        print(f"β— PEFT λΌμ΄λΈλ¬λ¦¬κ°€ μ„¤μΉλμ§€ μ•μ•μµλ‹λ‹¤: {e}")
        print("  μ„¤μΉ λ…λ Ή: pip install peft")
        return False
    
    # 2. λ¨λΈ μ΄κΈ°ν™” ν…μ¤νΈ
    try:
        print("\nπ“¦ λ¨λΈ μ΄κΈ°ν™” μ¤‘...")
        model = PanoramaVLM(
            vision_model_name="google/siglip-base-patch16-224",
            language_model_name="Qwen/Qwen2.5-0.5B",  # λ” κ°€λ²Όμ΄ λ¨λΈλ΅ ν…μ¤νΈ
            resampler_type="mlp",
            max_text_length=64
        )
        print("β“ PanoramaVLM λ¨λΈμ΄ μ„±κ³µμ μΌλ΅ μ΄κΈ°ν™”λμ—μµλ‹λ‹¤.")
    except Exception as e:
        print(f"β— λ¨λΈ μ΄κΈ°ν™” μ‹¤ν¨: {e}")
        return False
    
    # 3. LoRA μ„¤μ • ν…μ¤νΈ
    try:
        print("\nπ”§ LoRA μ„¤μ • μ¤‘...")
        success = model.setup_lora_for_finetune(
            lora_r=8,
            lora_alpha=16,
            lora_dropout=0.1
        )
        
        if success:
            print("β“ LoRA μ„¤μ •μ΄ μ„±κ³µμ μΌλ΅ μ™„λ£λμ—μµλ‹λ‹¤.")
        else:
            print("β— LoRA μ„¤μ •μ— μ‹¤ν¨ν–μµλ‹λ‹¤.")
            return False
            
    except Exception as e:
        print(f"β— LoRA μ„¤μ • μ¤‘ μ¤λ¥ λ°μƒ: {e}")
        return False
    
    # 4. LoRA μ •λ³΄ ν™•μΈ
    try:
        print("\nπ“ LoRA μ •λ³΄ ν™•μΈ μ¤‘...")
        lora_info = model.get_lora_info()
        
        print(f"  - PEFT μ‚¬μ© κ°€λ¥: {lora_info.get('peft_available', False)}")
        print(f"  - LoRA ν™μ„±ν™”: {lora_info.get('is_lora_enabled', False)}")
        
        if lora_info.get('is_lora_enabled', False):
            print(f"  - LoRA Rank: {lora_info.get('lora_r', 'N/A')}")
            print(f"  - LoRA Alpha: {lora_info.get('lora_alpha', 'N/A')}")
            print(f"  - LoRA Dropout: {lora_info.get('lora_dropout', 'N/A')}")
            print(f"  - Target Modules: {lora_info.get('target_modules', 'N/A')}")
            
    except Exception as e:
        print(f"β— LoRA μ •λ³΄ ν™•μΈ μ¤‘ μ¤λ¥ λ°μƒ: {e}")
        return False
    
    # 5. νλΌλ―Έν„° μΉ΄μ΄νΈ ν™•μΈ
    try:
        print("\nπ“ νλΌλ―Έν„° μ ν™•μΈ μ¤‘...")
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"  - μ΄ νλΌλ―Έν„°: {total_params:,}")
        print(f"  - ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°: {trainable_params:,}")
        print(f"  - ν›λ ¨ λΉ„μ¨: {trainable_params/total_params*100:.2f}%")
        
        if trainable_params < total_params:
            print("β“ LoRAλ¥Ό ν†µν• νλΌλ―Έν„° ν¨μ¨μ  ν•™μµμ΄ μ„¤μ •λμ—μµλ‹λ‹¤.")
        
    except Exception as e:
        print(f"β— νλΌλ―Έν„° ν™•μΈ μ¤‘ μ¤λ¥ λ°μƒ: {e}")
        return False
    
    print("\nπ‰ λ¨λ“  LoRA ν…μ¤νΈκ°€ μ„±κ³µμ μΌλ΅ μ™„λ£λμ—μµλ‹λ‹¤!")
    print("\n3λ‹¨κ³„ finetuneμ—μ„ LoRAλ¥Ό μ‚¬μ©ν•λ ¤λ©΄:")
    print("python train.py --stage finetune --use-lora --lora-rank 16 --lora-alpha 32")
    
    return True

def main():
    parser = argparse.ArgumentParser(description="LoRA μ§€μ› μ—¬λ¶€ ν…μ¤νΈ")
    parser.add_argument("--verbose", "-v", action="store_true", help="μƒμ„Έν• μ¶λ ¥")
    args = parser.parse_args()
    
    if args.verbose:
        import logging
        logging.basicConfig(level=logging.DEBUG)
    
    success = test_lora_support()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
