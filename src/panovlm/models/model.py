# coding: utf-8

from typing import Any, Dict, Optional

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer

from .utils import safe_save_pretrained
from .language_fusion import LanguageFusion
from ..losses import VicRegLoss
from ..losses.vicreg_overlap import compute_vicreg_overlap_loss
from ..losses.vicreg_projector import VICRegProjector
from .vision import VisionBackbone, PanoramaProjector, ResamplerModule
from .vision.utils import resolve_module_dtype

# LoRA ì§€ì›ì„ ìœ„í•œ PEFT import (ì„ íƒì )
try:
    from peft import LoraConfig, get_peft_model, TaskType
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    print("Warning: PEFT not available. LoRA fine-tuning will not be supported.")

# ---------------------------------------------------------------------------
# â€£ VICReg Loss (moved to panovlm/utils/losses.py)
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# â€£ PanoramaVLM (VICReg + AR)
# ---------------------------------------------------------------------------
class PanoramaVLM(nn.Module):
    """
    3ë‹¨ê³„ í•™ìŠµ:
    1) stage="vision": VICReg (ì¸ì ‘ ë·° ê²¹ì¹˜ëŠ” íŒ¨ì¹˜ êµ¬ê°„ë§Œ)
       - NEW: VICReg projectorë¡œ ì°¨ì› ì¶•ì†Œ/ì •ê·œí™” í›„ ì†ì‹¤ ê³„ì‚°
    2) stage="resampler": Resampler + LM AR-loss (resamplerì™€ projë§Œ í•™ìŠµ ê°€ëŠ¥)
    3) stage="finetune": ì „ì²´ ë¯¸ì„¸ì¡°ì •
    """
    def __init__(self, config=None, **kwargs):
        super().__init__()

        # ì„¤ì • ------------------------------------------------
        if config is not None:
            self.config = config
        else:
            # ê°„ë‹¨í•œ dict ê¸°ë°˜ ì„¤ì • í´ë°±
            class _Cfg:
                def __init__(self, **kw):
                    for k, v in kw.items():
                        setattr(self, k, v)
            self.config = _Cfg(**kwargs)

        # ë¹„ì „ ì¸ì½”ë” ------------------------------------------
        vision_name = getattr(self.config, 'vision_name', 'google/siglip-base-patch16-224')
        use_vicreg_norm = bool(getattr(self.config, 'use_vicreg_norm', False))
        self.vision_backbone = VisionBackbone(vision_name=vision_name, use_vicreg_norm=use_vicreg_norm)
        vision_hidden_size = self.vision_backbone.hidden_size

        # NEW: VICReg ì „ìš© Projector ---------------------------
        self.use_vicreg_projector = getattr(self.config, 'use_vicreg_projector', True)
        self.vicreg_projector_dim = int(getattr(self.config, 'vicreg_projector_dim', 768))
        self.vicreg_projector_depth = int(getattr(self.config, 'vicreg_projector_depth', 2))
        self.vicreg_projector_hidden = int(getattr(self.config, 'vicreg_projector_hidden', self.vicreg_projector_dim))
        self.vicreg_projector_ln = bool(getattr(self.config, 'vicreg_projector_ln', True))

        if self.use_vicreg_projector:
            self.vicreg_projector = VICRegProjector(
                in_dim=vision_hidden_size,
                out_dim=self.vicreg_projector_dim,
                hidden_dim=self.vicreg_projector_hidden,
                depth=self.vicreg_projector_depth,
                use_ln=self.vicreg_projector_ln,
            )
            self._vicreg_feat_dim = self.vicreg_projector_dim
        else:
            self.vicreg_projector = nn.Identity()
            self._vicreg_feat_dim = vision_hidden_size

        # ë¦¬ìƒ˜í”ŒëŸ¬ ----------------------------------------------
        self.resampler_module = ResamplerModule(self.config, vision_hidden_size)
        latent_dimension = self.resampler_module.output_dim
        self.resampler = self.resampler_module.resampler  # Backward compatibility alias

        # ì–¸ì–´ ëª¨ë¸ ë° íˆ¬ì˜ ------------------------------------
        lm_name = getattr(self.config, 'language_model_name', 'Qwen/Qwen2.5-0.5B-Instruct')
        self.language_model = AutoModelForCausalLM.from_pretrained(
            lm_name,
            attn_implementation="sdpa",
        )
        # Reduce GPU footprint during training
        if hasattr(self.language_model, "config"):
            self.language_model.config.use_cache = False
        self._gradient_checkpointing = False

        # í”„ë¡œì í„° ----------------------------------------------
        self.projector = PanoramaProjector(self.config, latent_dimension, self.language_model.config.hidden_size)
        self.vision_to_language_projection = self.projector.linear  # Backward compatibility alias

        # í…ìŠ¤íŠ¸ í”„ë¡œì í„° (2ë‹¨ê³„ í•™ìŠµìš©) -------------------------
        self.use_text_projection = getattr(self.config, 'use_text_projection', False)
        if self.use_text_projection:
            # ìë™ìœ¼ë¡œ LLMì˜ ì…ë ¥ ì°¨ì›ìœ¼ë¡œ projection (í•­ìƒ ë™ì¼í•œ ì°¨ì› ìœ ì§€)
            llm_hidden_size = self.language_model.config.hidden_size
            self.text_projection = nn.Linear(llm_hidden_size, llm_hidden_size)
            print(f"âœ“ Text projection enabled: {llm_hidden_size} -> {llm_hidden_size} (auto-aligned with LLM input)")
        else:
            self.text_projection = nn.Identity()

        # í† í¬ë‚˜ì´ì € -------------------------------------------
        self.tokenizer = AutoTokenizer.from_pretrained(lm_name)
        self._setup_tokenizer()
        self.vision_token_id = self.tokenizer.convert_tokens_to_ids("<|vision|>")
        if self.vision_token_id == self.tokenizer.unk_token_id:
            self.vision_token_id = self.tokenizer.bos_token_id

        # VICReg ì†ì‹¤ ë° í•˜ì´í¼ --------------------------------
        self.vicreg_loss = VicRegLoss(
            similarity_weight=float(getattr(self.config, 'vicreg_similarity_weight', 25.0)),
            variance_weight=float(getattr(self.config, 'vicreg_variance_weight', 25.0)),
            covariance_weight=float(getattr(self.config, 'vicreg_covariance_weight', 1.0)),
            gamma=float(getattr(self.config, 'vicreg_gamma', 1.0)),
            use_ddp_gather=bool(getattr(self.config, 'vicreg_use_ddp_gather', False)),
        )
        self.vicreg_loss_weight = float(getattr(self.config, 'vicreg_loss_weight', 1.0))
        self.overlap_ratio = float(getattr(self.config, 'overlap_ratio', 0.5))

        self.skip_first_view_in_vision_stage = bool(getattr(self.config, 'skip_first_view_in_vision_stage', False))
        expected_views_cfg = getattr(self.config, 'vision_stage_expected_views', None)
        if expected_views_cfg is None:
            expected_views_cfg = getattr(self.config, 'resampler_num_views', None)
        try:
            self.vision_stage_expected_views = int(expected_views_cfg) if expected_views_cfg is not None else None
        except (TypeError, ValueError):
            self.vision_stage_expected_views = None
        if self.vision_stage_expected_views is not None and self.vision_stage_expected_views <= 0:
            self.vision_stage_expected_views = None

        # í…ìŠ¤íŠ¸ ì„¤ì • ------------------------------------------
        self.max_text_length = int(getattr(self.config, 'max_text_length', 512))
        self.ignore_index = -100
        system_prompt = getattr(self.config, 'system_prompt', None)
        default_generation_prompt = getattr(self.config, 'default_generation_prompt', None)
        fallback_insert_position = getattr(self.config, 'vision_token_fallback_position', 'prefix')
        if isinstance(fallback_insert_position, str):
            fallback_insert_position = fallback_insert_position.lower().strip()
        else:
            fallback_insert_position = 'prefix'

        self.language_fusion = LanguageFusion(
            language_model=self.language_model,
            tokenizer=self.tokenizer,
            vision_token_id=self.vision_token_id,
            ignore_index=self.ignore_index,
            max_text_length=self.max_text_length,
            system_prompt=system_prompt,
            default_generation_prompt=default_generation_prompt,
            fallback_insert_position=fallback_insert_position,
        )
        self.language_fusion.update_vision_token_id(self.vision_token_id)
        self.language_fusion.update_max_text_length(self.max_text_length)

        # ë””ë²„ê·¸ í”Œë˜ê·¸
        self._warned_single_view = False
        self._debug_loss_verification = False
        self._cached_module_dtypes: Dict[str, torch.dtype] = {}
        self._vision_stage_trim_warned = False

    def _setup_tokenizer(self):
        tokens_added = False
        if self.tokenizer.pad_token is None:
            if self.tokenizer.eos_token is not None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                print(f"[Tokenizer Setup] Set pad_token = eos_token: '{self.tokenizer.eos_token}'")
            else:
                new_tokens = {'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}
                self.tokenizer.add_special_tokens(new_tokens)
                tokens_added = True
                print(f"[Tokenizer Setup] Added eos_token and pad_token: '<|endoftext|>'")
        special_tokens_to_add = []
        vision_token = '<|vision|>'
        if not any(vision_token in str(token) for token in self.tokenizer.additional_special_tokens):
            special_tokens_to_add.append(vision_token)
        if self.tokenizer.eos_token != '<|endoftext|>':
            if not any('<|endoftext|>' in str(token) for token in self.tokenizer.additional_special_tokens):
                special_tokens_to_add.append('<|endoftext|>')
        if special_tokens_to_add:
            added_tokens = self.tokenizer.add_special_tokens({'additional_special_tokens': special_tokens_to_add})
            if added_tokens > 0:
                tokens_added = True
                print(f"[Tokenizer Setup] Added {added_tokens} special tokens: {special_tokens_to_add}")
        if tokens_added:
            old_embeddings = self.language_model.get_input_embeddings().weight.size(0)
            self.language_model.resize_token_embeddings(len(self.tokenizer))
            new_embeddings = self.language_model.get_input_embeddings().weight.size(0)
            print(f"[Tokenizer Setup] Resized embeddings: {old_embeddings} -> {new_embeddings}")
        self.tokenizer.padding_side = "right"
        self.pad_token_id = self.tokenizer.pad_token_id
        self.eos_token_id = self.tokenizer.eos_token_id
        self.bos_token_id = self.tokenizer.bos_token_id
        if self.pad_token_id == self.eos_token_id:
            print(f"[Tokenizer Setup] âœ“ pad_token_id == eos_token_id (consistent with loss masking)")

    # ==================== ê³µí†µ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ====================
    def _process_vision_encoder(self, pixel_values: torch.Tensor) -> Dict[str, Any]:
        """
        ì´ë¯¸ì§€ B,V,3,H,Wë¥¼ B*V,3,H,Wë¡œ ë³€í™˜í•˜ì—¬ ë¹„ì „ ì¸ì½”ë”ë¥¼ í†µê³¼ì‹œí‚´
        
        Returns:
            Dict with vision_features: [B*V, S, D_vision], batch_size, num_views
        """
        target_dtype = resolve_module_dtype(
            self._cached_module_dtypes,
            "vision_encoder",
            getattr(self.vision_backbone, 'encoder', None),
            pixel_values.dtype,
        )
        batch_size, num_views, normalized_pixels = self.vision_backbone.normalize_inputs(pixel_values, target_dtype)
        vision_features = self.vision_backbone.extract_features(normalized_pixels)  # [B*V, S, D_vision]

        return {
            "vision_features": vision_features,
            "batch_size": batch_size,
            "num_views": num_views,
            "device": normalized_pixels.device
        }
    
    def _process_resampler(self, vision_features: torch.Tensor, batch_size: int, num_views: int) -> torch.Tensor:
        """
        Resamplerë¥¼ í†µí•œ vision feature ë³€í™˜

        Args:
            vision_features: [B*V, S, D_vision] - Vision encoderì˜ ì¶œë ¥

        Returns:
            resampled_features: [B*V, S, D_latent] - Resampler ì¶œë ¥
        """
        target_dtype = resolve_module_dtype(
            self._cached_module_dtypes,
            "resampler",
            getattr(self.resampler_module, 'resampler', None),
            vision_features.dtype,
        )
        return self.resampler_module(vision_features, target_dtype)
    
    def _process_projection_layer(self, resampled_features: torch.Tensor, batch_size: int, num_views: int) -> torch.Tensor:
        """
        Projection layer ì²˜ë¦¬: B*V,L,D -> B,V*L,D ë³€í™˜
        
        Args:
            resampled_features: [B*V, S, D_latent] - Resampler ì¶œë ¥
            
        Returns:
            projected_features: [B, V*S, D_lm] - íˆ¬ì˜ëœ íŠ¹ì§•
        """
        return self.projector(resampled_features, batch_size, num_views, self._cached_module_dtypes, self.language_model)
    
    
    def _fuse_text_image_embeddings(self, vision_tokens: torch.Tensor, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, labels: Optional[torch.Tensor] = None, stage: str = "finetune") -> Dict[str, torch.Tensor]:
        """
        í…ìŠ¤íŠ¸ í† í°ì„ ì„ë² ë”©í•˜ê³  ì´ë¯¸ì§€ í† í° ìë¦¬ì— vision tokens ì¶”ê°€
        
        Args:
            vision_tokens: [B, V*S, D_lm] - íˆ¬ì˜ëœ vision tokens
            input_ids: [B, L] - í…ìŠ¤íŠ¸ í† í° ID
            
        Returns:
            Dict with inputs_embeds: [B, L'-1+V*S, D_lm], attention_mask, labels
        """
        text_inputs = self._prepare_text_inputs(input_ids, attention_mask, labels, stage)
        return self._create_combined_inputs(vision_tokens, text_inputs=text_inputs)
    
    # ==================== ë©”ì¸ ìˆœì „íŒŒ ë° ìƒì„± ====================
    def forward(
        self,
        pixel_values: torch.Tensor,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        stage: str = "vision",
        **kwargs
    ):
        """
        Args:
            pixel_values: [B,V,C,H,W] ë˜ëŠ” [B,C,H,W]
            stage: "vision" | "resampler" | "finetune"
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°ë“¤ (ë¯¸ë˜ í™•ì¥ì„±ì„ ìœ„í•´ ë³´ì¡´)
        """
        # kwargsì—ì„œ ì¶”ê°€ ì„¤ì • ì¶”ì¶œ (í•„ìš”ì‹œ)
        debug_mode = kwargs.get('debug_mode', False)
        if debug_mode:
            print(f"[DEBUG] Forward pass - stage: {stage}, batch_size: {pixel_values.size(0)}")
        if stage == "vision":
            # Vision-only stage uses raw vision hidden states (no resampler/projection)
            batch_size, num_views, normalized_pixels = self._normalize_pixel_values(pixel_values)
            batch_size, num_views, normalized_pixels = self._maybe_trim_views_for_stage(
                normalized_pixels, stage, batch_size, num_views
            )
            vision_hidden_states = self._extract_vision_features(normalized_pixels, batch_size, num_views)
            if num_views <= 1 and not self._warned_single_view:
                print("[VICReg] Warning: num_views <= 1, VICReg ì†ì‹¤ì´ 0ì´ ë©ë‹ˆë‹¤.")
                self._warned_single_view = True

            if self.vicreg_loss_weight == 0.0:
                if not hasattr(self, '_warned_zero_vicreg_weight'):
                    print("[VICReg] Warning: VICReg ê°€ì¤‘ì¹˜ 0.0 â†’ ê³„ì‚° ìŠ¤í‚µ")
                    self._warned_zero_vicreg_weight = True
                zero = torch.zeros((), device=vision_hidden_states.device)
                return {"loss": zero, "vicreg_loss": zero, "vicreg_raw": zero, "vicreg_weight": 0.0}

            # NEW: VICReg projector ì ìš© (vision stageì—ì„œë§Œ)
            vicreg_feats = self.vicreg_projector(vision_hidden_states)

            vicreg_raw = self._compute_vicreg_overlap_loss(
                vicreg_feats, batch_size, num_views, overlap_ratio=self.overlap_ratio
            )
            vicreg_loss = vicreg_raw * self.vicreg_loss_weight
            total_loss = vicreg_loss
            return {
                "loss": total_loss,
                "vicreg_loss": vicreg_loss,
                "vicreg_raw": vicreg_raw.detach(),
                "vicreg_weight": self.vicreg_loss_weight,
                "vicreg_dim": self._vicreg_feat_dim,
            }

        # vision ì™¸ ë‹¨ê³„: ê³µí†µ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        if stage in ("resampler", "finetune"):
            if input_ids is None or labels is None:
                raise ValueError(f"'{stage}' ë‹¨ê³„ì—ì„œëŠ” input_idsì™€ labelsê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # 1. Vision encoder ì²˜ë¦¬
            vision_result = self._process_vision_encoder(pixel_values)
            vision_features = vision_result["vision_features"]  # [B*V, S, D_vision]
            batch_size = vision_result["batch_size"]
            num_views = vision_result["num_views"]
            
            # 2. Resampler ì²˜ë¦¬
            resampled_features = self._process_resampler(vision_features, batch_size, num_views)  # [B*V, S, D_latent]
            
            # 3. Projection layer ì²˜ë¦¬
            vision_tokens = self._process_projection_layer(resampled_features, batch_size, num_views)  # [B, V*S, D_lm]
            
            # 4. í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìœµí•© ë° LM forward
            return self._compute_autoregressive_loss(
                vision_tokens, input_ids, attention_mask, labels, stage
            )

        raise ValueError("stageëŠ” 'vision', 'resampler', 'finetune' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    @torch.inference_mode()
    def generate(self, pixel_values: torch.Tensor, input_ids: Optional[torch.Tensor] = None,
                 attention_mask: Optional[torch.Tensor] = None,
                 max_new_tokens: int = 32, temperature: float = 0.7, **kwargs):
        """forwardì™€ ë™ì¼í•œ ì²˜ë¦¬ ê³¼ì •ì„ ì‚¬ìš©í•˜ëŠ” ìƒì„± í•¨ìˆ˜"""
        self.eval()
        try:
            # 1. Vision encoder ì²˜ë¦¬ (forwardì™€ ë™ì¼)
            vision_result = self._process_vision_encoder(pixel_values)
            vision_features = vision_result["vision_features"]  # [B*V, S, D_vision]
            batch_size = vision_result["batch_size"]
            num_views = vision_result["num_views"]
            device = vision_result["device"]
            
            # 2. Resampler ì²˜ë¦¬ (forwardì™€ ë™ì¼)
            resampled_features = self._process_resampler(vision_features, batch_size, num_views)  # [B*V, S, D_latent]
            
            # 3. Projection layer ì²˜ë¦¬ (forwardì™€ ë™ì¼)
            vision_tokens = self._process_projection_layer(resampled_features, batch_size, num_views)  # [B, V*S, D_lm]
            
            # 4. ìƒì„±ìš© ì…ë ¥ ì¤€ë¹„
            input_ids, attention_mask = self._prepare_generation_inputs(
                input_ids, attention_mask, batch_size, device
            )
            
            # 5. í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìœµí•© (forwardì™€ ë™ì¼, ë‹¨ labels ì—†ìŒ)
            combined_inputs = self._fuse_text_image_embeddings(
                vision_tokens, input_ids=input_ids, attention_mask=attention_mask, stage="finetune"
            )
            
            # 6. LM generate í˜¸ì¶œ
            generation_kwargs = self._build_lm_generate_kwargs(combined_inputs, max_new_tokens, temperature, **kwargs)
            generated_ids = self.language_model.generate(**generation_kwargs)
            return self._decode_generated_text(generated_ids)
        except Exception as e:
            print(f"[Generate] Error in generation: {e}")
            import traceback; traceback.print_exc()
            return self._fallback_generation_output(pixel_values.size(0) if pixel_values.ndim in (4,5) else 1,
                                                        pixel_values.device if isinstance(pixel_values, torch.Tensor) else 'cpu')

    # ==================== ê¸°ë³¸ ìœ í‹¸ë¦¬í‹° í—¬í¼ ====================
    def _normalize_pixel_values(self, pixel_values):
        target_dtype = resolve_module_dtype(
            self._cached_module_dtypes,
            "vision_encoder",
            getattr(self.vision_backbone, 'encoder', None),
            pixel_values.dtype,
        )
        return self.vision_backbone.normalize_inputs(pixel_values, target_dtype)

    def _maybe_trim_views_for_stage(
        self,
        pixel_values: torch.Tensor,
        stage: str,
        batch_size: int,
        num_views: int,
    ) -> tuple[int, int, torch.Tensor]:
        if stage != "vision":
            return batch_size, num_views, pixel_values
        if pixel_values.ndim != 5 or num_views <= 0:
            return batch_size, num_views, pixel_values

        drop_count = 0
        if self.skip_first_view_in_vision_stage:
            drop_count = 1
        else:
            expected = getattr(self, "vision_stage_expected_views", None)
            if expected is not None and expected > 0 and num_views > expected:
                drop_count = num_views - int(expected)

        if drop_count <= 0:
            return batch_size, num_views, pixel_values

        drop_count = min(drop_count, num_views - 1)
        if drop_count <= 0:
            return batch_size, num_views, pixel_values

        trimmed = pixel_values[:, drop_count:, ...].contiguous()
        if not getattr(self, "_vision_stage_trim_warned", False):
            try:
                print(f"[Vision Stage] Dropping {drop_count} leading view(s) prior to VICReg ({num_views}->{trimmed.size(1)})")
            except Exception:
                pass
            self._vision_stage_trim_warned = True

        return batch_size, trimmed.size(1), trimmed

    def _extract_vision_features(self, pixel_values, batch_size, num_views):
        return self.vision_backbone.extract_features(pixel_values)

    def _prepare_generation_inputs(self, input_ids, attention_mask, batch_size, device):
        if input_ids is None:
            print("[Generate] Warning: input_ids not provided, creating default prompt for captioning")
        return self.language_fusion.prepare_generation_inputs(input_ids, attention_mask, batch_size, device)

    def _create_default_prompt(self, batch_size, device):
        return self.language_fusion.create_default_prompt(batch_size, device)

    def _adjust_batch_size(self, input_ids, attention_mask, target_batch_size):
        return self.language_fusion.adjust_batch_size(input_ids, attention_mask, target_batch_size)

    def _get_tokenizer_info(self):
        try:
            if hasattr(self.language_model, 'config'):
                config = self.language_model.config
            elif hasattr(self.language_model, 'base_model') and hasattr(self.language_model.base_model, 'config'):
                config = self.language_model.base_model.config
            else:
                config = None
            if config:
                return {
                    'pad_token_id': getattr(config, 'pad_token_id', None),
                    'eos_token_id': getattr(config, 'eos_token_id', None),
                    'bos_token_id': getattr(config, 'bos_token_id', None),
                }
            return {}
        except Exception:
            return {}

    def _build_generation_kwargs(self, combined_inputs, max_new_tokens, temperature, **kwargs):
        temperature = max(0.1, min(temperature, 1.0))
        MIN_NEW = 5
        DEFAULTS = dict(top_p=0.9, top_k=50, repetition_penalty=1.1, length_penalty=1.0)
        tk = self._get_tokenizer_info()
        out = {
            **combined_inputs,
            'max_new_tokens': max_new_tokens,
            'min_new_tokens': kwargs.get('min_new_tokens', MIN_NEW),
            'temperature': temperature,
            'do_sample': temperature > 0.1,
            'top_p': kwargs.get('top_p', DEFAULTS['top_p']),
            'top_k': kwargs.get('top_k', DEFAULTS['top_k']),
            'repetition_penalty': kwargs.get('repetition_penalty', DEFAULTS['repetition_penalty']),
            'length_penalty': kwargs.get('length_penalty', DEFAULTS['length_penalty']),
        }
        if tk.get('pad_token_id') is not None: out['pad_token_id'] = tk['pad_token_id']
        if tk.get('eos_token_id') is not None: out['eos_token_id'] = tk['eos_token_id']
        for key in ['pad_token_id', 'eos_token_id', 'bos_token_id']:
            if key in kwargs:
                out[key] = kwargs[key]
        return out

    def _build_lm_generate_kwargs(self, combined_inputs, max_new_tokens, temperature, **kwargs):
        return self._build_generation_kwargs(combined_inputs, max_new_tokens, temperature, **kwargs)

    def _postprocess_generated_text(self, generated_ids):
        generated_texts = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
        cleaned_texts = [t.strip() for t in generated_texts]
        return {"generated_ids": generated_ids, "text": cleaned_texts}

    def _decode_generated_text(self, generated_ids):
        return self._postprocess_generated_text(generated_ids)

    def _get_fallback_generation_result(self, batch_size, device):
        fallback_text = ["a panoramic view"] * batch_size
        fallback_ids = torch.ones((batch_size, 5), dtype=torch.long, device=device)
        return {"generated_ids": fallback_ids, "text": fallback_text}

    def _fallback_generation_output(self, batch_size, device):
        return self._get_fallback_generation_result(batch_size, device)

    # ==================== VICReg & AR í—¬í¼ ====================
    def _compute_vicreg_overlap_loss(
        self,
        vision_output: torch.Tensor,   # [B*V, S, D_vicreg]
        batch_size: int,
        num_views: int,
        overlap_ratio: float = 0.5,
        pair_chunk: Optional[int] = 8,  # ë©”ëª¨ë¦¬ ì ˆì•½ìš© ì²­í‚¹ (Noneì´ë©´ full-batch)
    ):
        """
        ë²¡í„°í™”ëœ VICReg overlap loss ê³„ì‚°
        - ì›ë˜ êµ¬í˜„ê³¼ ë™ì¼í•˜ê²Œ (v, v+1 mod V) í˜ì–´ë³„ë¡œ VICRegì„ êµ¬í•´ í‰ê· ëƒ„
        - íŒŒì´ì¬ ë£¨í”„ ì œê±° â†’ í° í­ì˜ ì†ë„ ê°œì„ 
        - ê³µë¶„ì‚° í•­ì€ [P, D, D]ë¥¼ ë§Œë“¤ê¸° ë•Œë¬¸ì— Dê°€ ì•„ì£¼ í° ê²½ìš° pair_chunkë¡œ ë‚˜ëˆ  ê³„ì‚° ê¶Œì¥

        Args:
            pair_chunk: í•œ ë²ˆì— ì²˜ë¦¬í•  í˜ì–´ ìˆ˜(P=B*V). ë©”ëª¨ë¦¬ í”¼í¬ ë‚®ì¶”ê³  ì‹¶ì„ ë•Œ ì„¤ì •.
        """
        if num_views <= 1:
            return torch.zeros((), device=vision_output.device)

        return compute_vicreg_overlap_loss(
            vision_output,
            batch_size=batch_size,
            num_views=num_views,
            overlap_ratio=overlap_ratio,
            pair_chunk=pair_chunk,
            vicreg_loss_module=self.vicreg_loss,
            has_cls_token=self.vision_backbone.has_cls_token(vision_output),
        )


    def _prepare_text_inputs(self, input_ids, attention_mask, labels=None, stage="finetune"):
        """í…ìŠ¤íŠ¸ ì…ë ¥ ì¤€ë¹„ (projection ì ìš© í¬í•¨)"""
        text_inputs = self.language_fusion.prepare_text_inputs(input_ids, attention_mask, labels)
        
        # 2ë‹¨ê³„(resampler) í•™ìŠµì—ì„œë§Œ í…ìŠ¤íŠ¸ projection ì ìš©
        if self.use_text_projection and hasattr(self, 'text_projection') and stage == "resampler":
            text_inputs["inputs_embeds"] = self.text_projection(text_inputs["inputs_embeds"])
        
        return text_inputs

    def _create_combined_inputs(self, vision_tokens, input_ids=None, attention_mask=None, labels=None, text_inputs=None):
        """
        í•™ìŠµ ì‹œ ê²°í•© ë¡œì§ì„ LanguageFusion ì„œë¸Œëª¨ë“ˆì— ìœ„ì„í•©ë‹ˆë‹¤.
        """
        return self.language_fusion.fuse(
            vision_tokens,
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            text_inputs=text_inputs,
        )

    def _compose_multimodal_embeddings(self, vision_tokens, input_ids=None, attention_mask=None, labels=None, text_inputs=None):
        return self._create_combined_inputs(vision_tokens, input_ids=input_ids, attention_mask=attention_mask, labels=labels, text_inputs=text_inputs)

    # Gradient checkpointing utilities -------------------------------------------------
    def gradient_checkpointing_enable(self, use_reentrant: bool | None = None):
        """Enable gradient checkpointing for submodules that support it."""
        if hasattr(self.language_model, "gradient_checkpointing_enable"):
            try:
                kwargs = {}
                if use_reentrant is not None:
                    kwargs["use_reentrant"] = use_reentrant
                self.language_model.gradient_checkpointing_enable(**kwargs)
            except TypeError:
                self.language_model.gradient_checkpointing_enable()
        encoder = getattr(self.vision_backbone, 'encoder', None)
        if hasattr(encoder, "gradient_checkpointing_enable"):
            try:
                encoder.gradient_checkpointing_enable()
            except TypeError:
                encoder.gradient_checkpointing_enable()
        self._gradient_checkpointing = True

    def gradient_checkpointing_disable(self):
        if hasattr(self.language_model, "gradient_checkpointing_disable"):
            self.language_model.gradient_checkpointing_disable()
        encoder = getattr(self.vision_backbone, 'encoder', None)
        if hasattr(encoder, "gradient_checkpointing_disable"):
            encoder.gradient_checkpointing_disable()
        self._gradient_checkpointing = False

    def _compute_autoregressive_loss(self, vision_tokens, input_ids, attention_mask, labels, stage="finetune"):
        text_inputs = self._prepare_text_inputs(input_ids, attention_mask, labels, stage)
        combined_inputs = self._create_combined_inputs_for_training(vision_tokens, text_inputs)
        try:
            outputs = self.language_model(
                inputs_embeds=combined_inputs['inputs_embeds'],
                attention_mask=combined_inputs['attention_mask'],
                labels=combined_inputs['labels'],
                return_dict=True
            )
            if not torch.isfinite(outputs.loss):
                manual = self._compute_manual_cross_entropy_loss(outputs.logits, combined_inputs['labels'])
                manual.setdefault('ar_loss', manual['loss'])
                return manual
            if hasattr(self, '_debug_loss_verification') and self._debug_loss_verification:
                manual = self._compute_manual_cross_entropy_loss(outputs.logits, combined_inputs['labels'])
                diff = abs(outputs.loss.item() - manual['loss'].item())
                if diff > 0.01:
                    print(f"[Loss Debug] HF loss: {outputs.loss.item():.6f}, Manual loss: {manual['loss'].item():.6f}, Diff: {diff:.6f}")
            return {"loss": outputs.loss, "ar_loss": outputs.loss, "logits": outputs.logits}
        except Exception:
            fallback_loss = torch.tensor(0.0, device=vision_tokens.device, requires_grad=True)
            return {"loss": fallback_loss, "ar_loss": fallback_loss,
                    "logits": torch.zeros((text_inputs['batch_size'], combined_inputs['inputs_embeds'].size(1),
                                           self.language_model.config.vocab_size), device=vision_tokens.device)}

    def _create_combined_inputs_for_training(self, vision_tokens, text_inputs):
        return self._create_combined_inputs(vision_tokens, text_inputs=text_inputs)

    def _compute_manual_cross_entropy_loss(self, logits, labels):
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.ignore_index)
        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        if not hasattr(self, '_debug_shift_logged'):
            valid_tokens = (shift_labels != self.ignore_index).sum()
            total_tokens = shift_labels.numel()
            print(f"[Loss Debug] logits: {logits.shape} -> {shift_logits.shape}, labels: {labels.shape} -> {shift_labels.shape}")
            print(f"[Loss Debug] Valid tokens: {valid_tokens.item()}/{total_tokens} ({valid_tokens.item()/total_tokens*100:.1f}%)")
            self._debug_shift_logged = True
        return {"loss": loss, "ar_loss": loss, "perplexity": torch.exp(loss) if torch.isfinite(loss) else torch.tensor(float('inf'))}


    # ==================== LoRA ìœ í‹¸ (ìƒëµ ì—†ì´ ìœ ì§€) ====================
    def setup_lora_for_finetune(self, lora_r: int = 16, lora_alpha: int = 32, lora_dropout: float = 0.1, target_modules: Optional[list] = None) -> bool:
        if not PEFT_AVAILABLE:
            print("Warning: PEFT not available. LoRA setup skipped.")
            return False
        try:
            if target_modules is None:
                target_modules = ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
            lora_config = LoraConfig(r=lora_r, lora_alpha=lora_alpha, target_modules=target_modules,
                                     lora_dropout=lora_dropout, bias="none", task_type=TaskType.CAUSAL_LM)
            self.language_model = get_peft_model(self.language_model, lora_config)
            if hasattr(self.language_model, 'enable_input_require_grads'):
                self.language_model.enable_input_require_grads()
            trainable_params = sum(p.numel() for p in self.language_model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.language_model.parameters())
            print(f"âœ“ LoRA setup: trainable {trainable_params:,}/{total_params:,} ({trainable_params/total_params*100:.2f}%)")
            return True
        except Exception as e:
            print(f"Error setting up LoRA: {e}")
            return False

    def load_lora_weights(self, load_path: str):
        if not PEFT_AVAILABLE:
            print("Warning: PEFT not available. Cannot load LoRA weights.")
            return False
        try:
            from peft import PeftModel
            is_already_peft = hasattr(self.language_model, 'peft_config')
            if is_already_peft:
                print("Model already PEFT-enabled. Trying to load adapter...")
                if hasattr(self.language_model, 'load_adapter'):
                    adapter_name = "eval_adapter"
                    try:
                        self.language_model.load_adapter(load_path, adapter_name=adapter_name)
                        self.language_model.set_adapter(adapter_name)
                        print(f"âœ“ LoRA adapter '{adapter_name}' loaded")
                        return True
                    except Exception as e:
                        print(f"load_adapter failed, fallback to manual state_dict load: {e}")
                import os
                state_dict = None
                # safetensors ìš°ì„  -> ì‹¤íŒ¨ ì‹œ PyTorch bin ì‹œë„
                st_path = os.path.join(load_path, "adapter_model.safetensors")
                if os.path.exists(st_path):
                    try:
                        from safetensors.torch import load_file as load_sft
                        state_dict = load_sft(st_path)
                    except Exception as _e:
                        print(f"Warning: failed to load safetensors adapter: {_e}")
                        state_dict = None
                if state_dict is None:
                    bin_path = os.path.join(load_path, "adapter_model.bin")
                    if os.path.exists(bin_path):
                        try:
                            import torch
                            state_dict = torch.load(bin_path, map_location='cpu')
                        except Exception as _e:
                            print(f"Warning: failed to load bin adapter: {_e}")
                            state_dict = None
                if state_dict is None:
                    print(f"Error: No adapter weights found in {load_path} (adapter_model.safetensors or adapter_model.bin)")
                    return False
                cleaned = {}
                for k, v in state_dict.items():
                    ck = k
                    for prefix in ('base_model.model.', 'base_model.'):
                        if ck.startswith(prefix):
                            ck = ck[len(prefix):]
                            break
                    cleaned[ck] = v
                missing, unexpected = self.language_model.load_state_dict(cleaned, strict=False)
                loaded_cnt = len(cleaned) - len(missing)
                if loaded_cnt == 0:
                    print("Error: No LoRA weights matched.")
                    return False
                print(f"âœ“ LoRA weights loaded: {loaded_cnt}/{len(cleaned)} (missing {len(missing)}, unexpected {len(unexpected)})")
                return True
            else:
                print("Converting to PeftModel.from_pretrained...")
                self.language_model = PeftModel.from_pretrained(self.language_model, load_path, is_trainable=False)
                print("âœ“ LoRA weights loaded via PeftModel")
                return True
        except Exception as e:
            import traceback
            print(f"Error loading LoRA weights: {e}")
            print(f"Detailed error: {traceback.format_exc()}")
            return False

    def merge_lora_weights(self):
        if not PEFT_AVAILABLE:
            print("Warning: PEFT not available. Cannot merge LoRA weights.")
            return False
        if hasattr(self.language_model, 'merge_and_unload'):
            try:
                self.language_model = self.language_model.merge_and_unload()
                print("âœ“ LoRA weights merged into base model")
                return True
            except Exception as e:
                print(f"Error merging LoRA weights: {e}")
                return False
        else:
            print("Warning: Language model does not support LoRA weight merging.")
            return False

    def get_lora_info(self) -> Dict[str, Any]:
        if not PEFT_AVAILABLE:
            return {"peft_available": False}
        info = {"peft_available": True}
        if hasattr(self.language_model, 'peft_config'):
            peft_config = getattr(self.language_model, 'peft_config', {})
            if peft_config:
                adapter_name = list(peft_config.keys())[0]
                config = peft_config.get(adapter_name, None)
                if config is not None:
                    info.update({
                        "is_lora_enabled": True,
                        "lora_r": getattr(config, 'r', None),
                        "lora_alpha": getattr(config, 'lora_alpha', None),
                        "lora_dropout": getattr(config, 'lora_dropout', None),
                        "target_modules": getattr(config, 'target_modules', None),
                        "adapter_name": adapter_name
                    })
                else:
                    info["is_lora_enabled"] = False
            else:
                info["is_lora_enabled"] = False
        else:
            info["is_lora_enabled"] = False
        return info

    def save_lora_weights(self, save_path: str) -> bool:
        if not PEFT_AVAILABLE:
            print("Warning: PEFT not available. Cannot save LoRA weights.")
            return False
        try:
            from pathlib import Path
            lora_info = self.get_lora_info()
            if not lora_info.get("is_lora_enabled", False):
                print("Warning: LoRA is not enabled. No weights to save.")
                return False
            save_dir = Path(save_path)
            save_dir.mkdir(parents=True, exist_ok=True)
            # safetensorsë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ safe_serialization=Falseë¡œ ê°•ì œ
            if safe_save_pretrained(self.language_model, str(save_dir), safe_serialization=False):
                print(f"âœ“ LoRA weights saved: {save_dir}")
                return True
            else:
                print("Error: Failed to save LoRA weights")
                return False
        except Exception as e:
            print(f"Error saving LoRA weights: {e}")
            import traceback; traceback.print_exc()
            return False

    # (ì €ì¥ ë©”ì„œë“œ ì‚­ì œ: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì€ Lightningì˜ ModelCheckpointì— ìœ„ì„)

    @classmethod
    def from_checkpoint(cls, 
                       checkpoint_path: str, 
                       lora_weights_path: Optional[str] = None,
                       device: str = "auto",
                       auto_detect_lora: bool = True,
                       strict_loading: bool = False,
                       **model_kwargs) -> 'PanoramaVLM':
        """
        í†µí•©ëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë©”ì„œë“œ - Lightning ì²´í¬í¬ì¸íŠ¸ì™€ LoRA ê°€ì¤‘ì¹˜ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬
        
        Args:
            checkpoint_path (str): Lightning ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ (.ckpt)
            lora_weights_path (str, optional): LoRA ê°€ì¤‘ì¹˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            device (str): ëª¨ë¸ì„ ë¡œë“œí•  ë””ë°”ì´ìŠ¤ ("auto", "cuda", "cpu")
            auto_detect_lora (bool): LoRA ê°€ì¤‘ì¹˜ ìë™ ê°ì§€ ì—¬ë¶€
            strict_loading (bool): ì—„ê²©í•œ ê°€ì¤‘ì¹˜ ë¡œë”© ì—¬ë¶€
            **model_kwargs: ëª¨ë¸ ìƒì„±ì— í•„ìš”í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°ë“¤
            
        Returns:
            PanoramaVLM: ë¡œë“œëœ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
            
        Example:
            # ê¸°ë³¸ ì‚¬ìš©ë²•
            model = PanoramaVLM.from_checkpoint("runs/best.ckpt")
            
            # LoRA ê²½ë¡œ ì§ì ‘ ì§€ì •
            model = PanoramaVLM.from_checkpoint(
                "runs/best.ckpt", 
                lora_weights_path="runs/lora_weights"
            )
        """
        import torch
        from pathlib import Path
        
        print(f"ğŸš€ PanoramaVLM ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        device_obj = torch.device(device)
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint_path = Path(checkpoint_path)
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        
        print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤‘...")
        try:
            checkpoint = torch.load(checkpoint_path, map_location=device_obj, weights_only=False)
        except Exception as e:
            raise RuntimeError(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        hparams = checkpoint.get('hyper_parameters', {})
        model_state_dict = checkpoint.get('state_dict', {})

        # ì²´í¬í¬ì¸íŠ¸ë¡œë¶€í„° íˆ¬ì˜ ë ˆì´ì–´ ì¶œë ¥ ì°¨ì› ì¶”ì • (LM hidden size ìœ ë„)
        proj_out_dim = None
        try:
            for k in [
                'projector.linear.weight',
                'model.projector.linear.weight',
                'model.vision_to_language_projection.weight',
                'vision_to_language_projection.weight'
            ]:
                if k in model_state_dict:
                    w = model_state_dict[k]
                    # torch.Size([out_features, in_features])
                    if hasattr(w, 'shape') and len(w.shape) == 2:
                        proj_out_dim = int(w.shape[0])
                        break
        except Exception:
            proj_out_dim = None

        # Qwen2.5 ê³„ì—´ì˜ hidden_size ì¶”ì • â†’ ëª¨ë¸ ì´ë¦„ ì¶”ë¡  (í•„ìš” ì‹œë§Œ ì‚¬ìš©)
        def _infer_qwen_from_hidden_size(hs: Optional[int]) -> Optional[str]:
            if hs is None:
                return None
            # ìµœì†Œí•œì˜ ë§¤í•‘ë§Œ ì œê³µ (í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ë“¤)
            mapping = {
                896:  'Qwen/Qwen2.5-0.5B-Instruct',
                1536: 'Qwen/Qwen2.5-1.5B-Instruct',
            }
            return mapping.get(int(hs))
        
        # ì„¤ì • ì‹œìŠ¤í…œì„ í™œìš©í•œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ê²°ì •
        try:
            # 1. ì„¤ì • íŒŒì¼ ìë™ ê°ì§€ ì‹œë„
            from .config import ConfigManager, ModelConfig
            detected_config = ConfigManager.auto_detect_config(checkpoint_path)
            
            if detected_config:
                print(f"ğŸ” ì„¤ì • íŒŒì¼ ìë™ ê°ì§€ ì„±ê³µ")
                model_config = detected_config
            else:
                print(f"ğŸ” ì„¤ì • íŒŒì¼ ê°ì§€ ì‹¤íŒ¨ - í•˜ì´í¼íŒŒë¼ë¯¸í„°ì—ì„œ ìƒì„±")
                # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„°ì—ì„œ ì„¤ì • ìƒì„±
                config_dict = {
                    'vision_name': hparams.get('vision_name', 'google/siglip-base-patch16-224'),
                    'language_model_name': hparams.get('language_model_name', 'Qwen/Qwen2.5-0.5B-Instruct'),
                    'resampler_type': hparams.get('resampler_type', 'mlp'),
                    'latent_dimension': hparams.get('latent_dimension', 768),
                    'vicreg_loss_weight': hparams.get('vicreg_loss_weight', 1.0),
                    'overlap_ratio': hparams.get('overlap_ratio', 0.5),
                    'max_text_length': hparams.get('max_text_length', 512),
                }
                model_config = ModelConfig.from_dict(config_dict)
            
            # 2.5 ì²´í¬í¬ì¸íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìš°ì„  ì ìš©í•˜ì—¬ êµ¬ì¡°ì  ë¶ˆì¼ì¹˜ ìµœì†Œí™”
            #     (auto_detect_configê°€ ì „ì—­ config.jsonì„ ì¡ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, hparamsë¡œ í•µì‹¬ ê°’ ë™ê¸°í™”)
            if isinstance(model_config, ModelConfig) and isinstance(hparams, dict) and len(hparams) > 0:
                override_keys = [
                    'vision_name', 'language_model_name', 'resampler_type',
                    'latent_dimension', 'max_text_length', 'vicreg_loss_weight', 'overlap_ratio',
                    'resampler_depth', 'resampler_hidden_dim', 'resampler_use_ln',
                    'resampler_num_latents', 'resampler_heads', 'resampler_dropout'
                ]
                hp_overrides = {k: v for k, v in hparams.items() if k in override_keys}
                if hp_overrides:
                    try:
                        model_config = model_config.update(**hp_overrides)
                        print(f"ğŸ§© ì²´í¬í¬ì¸íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•µì‹¬ ì„¤ì • ë™ê¸°í™”: {list(hp_overrides.keys())[:5]}{'...' if len(hp_overrides)>5 else ''}")
                    except Exception as _e:
                        print(f"[from_checkpoint] Warning: failed to merge hparams into config: {_e}")

            # íˆ¬ì˜ ë ˆì´ì–´ë¡œë¶€í„° ìœ ë„ëœ LM ì´ë¦„ì„ ìš°ì„  ê³ ë ¤ (ì²´í¬í¬ì¸íŠ¸ì™€ êµ¬ì¡° ì¼ì¹˜ ë³´ì¥)
            inferred_lm_name = _infer_qwen_from_hidden_size(proj_out_dim)
            if inferred_lm_name:
                try:
                    model_config = model_config.update(language_model_name=inferred_lm_name)
                    print(f"ğŸ§­ ì²´í¬í¬ì¸íŠ¸ íˆ¬ì˜ ì°¨ì›({proj_out_dim})ì— ë§ì¶° LM ê³ ì •: {inferred_lm_name}")
                except Exception as _e:
                    print(f"[from_checkpoint] Warning: failed to enforce LM from projection dim: {_e}")

            # 3. ì‚¬ìš©ì ì§€ì • íŒŒë¼ë¯¸í„°ë¡œ ì˜¤ë²„ë¼ì´ë“œ (í—ˆìš©ëœ í‚¤ë§Œ)
            if model_kwargs:
                try:
                    allowed = set(ModelConfig().__dict__.keys())
                except Exception:
                    allowed = set()
                filtered = {k: v for k, v in model_kwargs.items() if k in allowed}
                # ë§Œì•½ ì²´í¬í¬ì¸íŠ¸ì—ì„œ LM ì°¨ì›ì„ ìœ ë„í–ˆìœ¼ë©´, ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ì¸í•œ êµ¬ì¡° ë¶ˆì¼ì¹˜ ë°©ì§€
                if inferred_lm_name and 'language_model_name' in filtered and filtered['language_model_name'] != inferred_lm_name:
                    print(f"âš ï¸  ì‚¬ìš©ì LM({filtered['language_model_name']})ê°€ ì²´í¬í¬ì¸íŠ¸ íˆ¬ì˜ ì°¨ì›({proj_out_dim})ê³¼ ë¶ˆì¼ì¹˜ â†’ ë¬´ì‹œí•˜ê³  {inferred_lm_name} ì‚¬ìš©")
                    filtered.pop('language_model_name', None)
                if filtered:
                    print(f"ğŸ› ï¸  ì‚¬ìš©ì íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ: {list(filtered.keys())}")
                    try:
                        model_config = model_config.update(**filtered)
                    except Exception as _e:
                        print(f"[from_checkpoint] Warning: failed to apply user kwargs: {_e}")
            
            # 4. ëª¨ë¸ ìƒì„±ìš© íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            model_params = model_config.get_model_kwargs()
            model_params['config'] = model_config  # config ê°ì²´ë„ ì „ë‹¬

        except Exception as e:
            print(f"âš ï¸ ì„¤ì • ì‹œìŠ¤í…œ ì‚¬ìš© ì‹¤íŒ¨ ({e}) - ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹
            model_params = {
                'vision_name': 'google/siglip-base-patch16-224',
                'language_model_name': 'Qwen/Qwen2.5-0.5B-Instruct',
                'resampler_type': 'mlp',
                'latent_dimension': 768,
                'vicreg_loss_weight': 1.0,
                'overlap_ratio': 0.5,
                'max_text_length': 512,
            }
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„°ì—ì„œ ì—…ë°ì´íŠ¸
            for key in model_params.keys():
                if key in hparams:
                    model_params[key] = hparams[key]
            
            # ì‚¬ìš©ì ì§€ì • íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ì—…ë°ì´íŠ¸
            model_params.update(model_kwargs)
        
        print(f"ğŸ› ï¸  ëª¨ë¸ íŒŒë¼ë¯¸í„°:")
        preview = {k: v for k, v in model_params.items() if k != 'config'}
        for i, (key, value) in enumerate(preview.items()):
            if i >= 12:
                print("   - ...")
                break
            print(f"   - {key}: {value}")
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        print(f"ğŸ—ï¸  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...")
        model = cls(**model_params)
        
        # Lightning wrapperì—ì„œ ì‹¤ì œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
        print(f"âš™ï¸  ê°€ì¤‘ì¹˜ ë¡œë”© ì¤‘...")
        model_weights = {}
        for key, value in model_state_dict.items():
            if key.startswith('model.'):
                # 'model.' ì ‘ë‘ì–´ ì œê±°
                clean_key = key[6:]  # len('model.') = 6
                model_weights[clean_key] = value
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        if model_weights:
            missing_keys, unexpected_keys = model.load_state_dict(model_weights, strict=strict_loading)
            loaded_cnt = len(model_weights) - len(missing_keys)
            print(f"   - ë¡œë“œëœ í‚¤: {loaded_cnt}")
            if missing_keys:
                print(f"   - ëˆ„ë½ëœ í‚¤: {len(missing_keys)} (ì˜ˆ: {missing_keys[:3]}{'...' if len(missing_keys) > 3 else ''})")
            if unexpected_keys:
                print(f"   - ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)} (ì˜ˆ: {unexpected_keys[:3]}{'...' if len(unexpected_keys) > 3 else ''})")
        else:
            print("   âš ï¸  ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì´ˆê¸°í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # LoRA ê°€ì¤‘ì¹˜ ì²˜ë¦¬
        if auto_detect_lora and lora_weights_path is None:
            # ìë™ ê°ì§€: ì²´í¬í¬ì¸íŠ¸ì™€ ê°™ì€ ë””ë ‰í† ë¦¬ì—ì„œ lora_weights í´ë” ì°¾ê¸°
            checkpoint_dir = checkpoint_path.parent
            potential_lora_path = checkpoint_dir / "lora_weights"
            if potential_lora_path.exists() and potential_lora_path.is_dir():
                lora_weights_path = str(potential_lora_path)
                print(f"ğŸ” LoRA ê°€ì¤‘ì¹˜ ìë™ ê°ì§€: {lora_weights_path}")
        
        if lora_weights_path:
            lora_path = Path(lora_weights_path)
            if lora_path.exists():
                print(f"ğŸ”§ LoRA ê°€ì¤‘ì¹˜ ë¡œë”©: {lora_weights_path}")
                success = model.load_lora_weights(str(lora_path))
                if success:
                    lora_info = model.get_lora_info()
                    if lora_info.get("is_lora_enabled", False):
                        print(f"   âœ… LoRA ë¡œë”© ì„±ê³µ - Rank: {lora_info.get('lora_r')}, Alpha: {lora_info.get('lora_alpha')}")
                    else:
                        print(f"   âš ï¸  LoRA ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
                else:
                    print(f"   âŒ LoRA ë¡œë”© ì‹¤íŒ¨")
            else:
                print(f"   âš ï¸  LoRA ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {lora_weights_path}")
        
        # ëª¨ë¸ì„ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        model = model.to(device_obj)
        model.eval()  # ê¸°ë³¸ì ìœ¼ë¡œ í‰ê°€ ëª¨ë“œ
        
        # í† í¬ë‚˜ì´ì € ì •ë³´ ì¶”ê°€ (eval.py í˜¸í™˜ì„±)
        if not hasattr(model, 'tokenizer'):
            try:
                from transformers import AutoTokenizer
                tokenizer_name = model_params.get('language_model_name', 'Qwen/Qwen2.5-0.5B-Instruct')
                model.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
                print(f"   âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ: {tokenizer_name}")
            except Exception as e:
                print(f"   âš ï¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ - Device: {device}")
        return model

    @classmethod
    def from_pretrained_dir(
        cls,
        pretrained_dir: str,
        device: str = "auto",
        strict_loading: bool = False,
        **model_kwargs
    ) -> 'PanoramaVLM':
        """
        HuggingFace-styleë¡œ ì €ì¥ëœ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ ë¡œë“œ.
        - save_pretrainedë¡œ ì €ì¥ëœ í´ë” êµ¬ì¡°ë¥¼ ê¸°ëŒ€ (pytorch_model.bin, config.json)
        - config.jsonì´ ìˆìœ¼ë©´ íŒŒë¼ë¯¸í„° ì¶”ì¶œ, ì „ë‹¬ëœ model_kwargsê°€ ìš°ì„  ì ìš©
        """
        from pathlib import Path
        import json

        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        device_obj = torch.device(device)

        p = Path(pretrained_dir)
        if not p.exists() or not p.is_dir():
            raise FileNotFoundError(f"Pretrained directory not found: {pretrained_dir}")

        # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        params = {
            'vision_name': 'google/siglip-base-patch16-224',
            'language_model_name': 'Qwen/Qwen2.5-0.5B-Instruct',
            'resampler_type': 'mlp',
            'latent_dimension': 768,
            'vicreg_loss_weight': 1.0,
            'overlap_ratio': 0.5,
            'max_text_length': 512,
        }

        # config.jsonì—ì„œ ë³´ì •
        cfg_path = p / "config.json"
        if cfg_path.exists():
            try:
                with open(cfg_path, 'r', encoding='utf-8') as f:
                    saved_cfg = json.load(f)
                params.update({
                    'vision_name': saved_cfg.get('vision_name', params['vision_name']),
                    'language_model_name': saved_cfg.get('language_model_name', params['language_model_name']),
                    'latent_dimension': saved_cfg.get('latent_dimension', params['latent_dimension']),
                    'max_text_length': saved_cfg.get('max_text_length', params['max_text_length']),
                    'vicreg_loss_weight': saved_cfg.get('vicreg_loss_weight', params['vicreg_loss_weight']),
                    'overlap_ratio': saved_cfg.get('overlap_ratio', params['overlap_ratio']),
                })
            except Exception as e:
                print(f"[from_pretrained_dir] Warning: failed to parse config.json: {e}")

        # ì™¸ë¶€ ì „ë‹¬ ì¸ì ìš°ì„ í•˜ë˜ 'config'/'model_config' í‚¤ëŠ” ë¬´ì‹œí•˜ì—¬ ì €ì¥ëœ ì„¤ì •ê³¼ì˜ êµ¬ì¡° ë¶ˆì¼ì¹˜ë¥¼ ë°©ì§€
        if model_kwargs:
            filtered = {k: v for k, v in model_kwargs.items() if k not in ("config", "model_config")}
            if filtered:
                params.update(filtered)

        print(f"ğŸ—ï¸  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±(from_pretrained_dir): {pretrained_dir}")
        model = cls(**params)

        # ê°€ì¤‘ì¹˜ íŒŒì¼ ì°¾ê¸° (PyTorch binë§Œ ì§€ì›)
        state_path = None
        if (p/"pytorch_model.bin").exists():
            state_path = p/"pytorch_model.bin"
            try:
                state = torch.load(str(state_path), map_location='cpu')
            except Exception as e:
                print(f"[from_pretrained_dir] Failed to load torch model: {e}")
                state = None
        else:
            state = None

        if state is not None:
            try:
                missing_keys, unexpected_keys = model.load_state_dict(state, strict=strict_loading)
                print(f"   âœ… Weights loaded from {state_path}")
                if missing_keys:
                    print(f"   âš ï¸ Missing keys: {len(missing_keys)} (showing first 5) {missing_keys[:5]}")
                if unexpected_keys:
                    print(f"   âš ï¸ Unexpected keys: {len(unexpected_keys)} (showing first 5) {unexpected_keys[:5]}")
            except Exception as e:
                print(f"   âŒ Failed to load state dict: {e}")
        else:
            print(f"   âš ï¸ No state dict found in {pretrained_dir}. Using randomly initialized weights.")

        model = model.to(device_obj)
        model.eval()
        # í† í¬ë‚˜ì´ì € ë³´ì¥
        if not hasattr(model, 'tokenizer'):
            try:
                model.tokenizer = AutoTokenizer.from_pretrained(params.get('language_model_name', 'Qwen/Qwen2.5-0.5B-Instruct'))
            except Exception:
                pass
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ - Device: {device}")
        return model

# -------------------- í¸ì˜: combined inputs ìƒì„± (ê³µìš©) --------------------
# def _stack_pad_mask(mask_list):  # ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
#     return torch.cat(mask_list, dim=1)

# í´ë˜ìŠ¤ ë©”ì„œë“œì— ë‘ê¸° ì• ë§¤í•œ ë³´ì¡° í•¨ìˆ˜ë“¤ ë³´ì™„
def _create_combined_inputs_for_training(self, vision_tokens, text_inputs):
    return self._create_combined_inputs(vision_tokens, text_inputs=text_inputs)

# ë°”ì¸ë”©
PanoramaVLM._create_combined_inputs_for_training = _create_combined_inputs_for_training
