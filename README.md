# PanoLLaVA: Panoramic Large Vision-Language Assistant

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

PanoLLaVAëŠ” íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ë¥¼ ì´í•´í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆëŠ” ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.
í—ˆê¹…í˜ì´ìŠ¤ì˜ Vision Encoderì™€ Language Modelì„ ê²°í•©í•˜ì—¬ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ì— íŠ¹í™”ëœ ë©€í‹°ëª¨ë‹¬ AIë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

## âœ¨ Features

- **íŒŒë…¸ë¼ë§ˆ íŠ¹í™”**: 360Â° íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ì— ìµœì í™”ëœ ëª¨ë¸ ì•„í‚¤í…ì²˜
- **Flash Attention 2 ì§€ì›**: ë©”ëª¨ë¦¬ ~30% ì ˆê°, ì†ë„ ~2ë°° í–¥ìƒ (ìë™ ê°ì§€)
- **LoRA ì§€ì›**: Parameter-Efficient Fine-Tuning (PEFT)ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ë¯¸ì„¸ì¡°ì •
- **ëª¨ë“ˆí™” ì„¤ê³„**: Vision, Language, Resampler ì»´í¬ë„ŒíŠ¸ì˜ ìœ ì—°í•œ ì¡°í•©
- **ë‹¤ì–‘í•œ ë°±ë³¸**: SigLIP, CLIP, DINOv2 ë“± ìµœì‹  ë¹„ì „ ëª¨ë¸ ì§€ì›
- **í™•ì¥ì„±**: Qwen, Llama, Gemma ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ ì§€ì›

## ğŸš€ Quick Start

### 1. ì„¤ì¹˜

```bash
# ê°œë°œ í™˜ê²½ ì„¤ì • (ê¶Œì¥)
python setup_dev.py

# ë˜ëŠ” ìˆ˜ë™ ì„¤ì¹˜
pip install -e .
```

### 2. í™˜ê²½ í™•ì¸

```bash
python check_env.py
```

### 3. Flash Attention 2 ì„¤ì¹˜ (ì„ íƒì , ê¶Œì¥)

```bash
# ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ Flash Attention 2 ì„¤ì¹˜
pip install flash-attn --no-build-isolation

# ì„¤ì¹˜ í™•ì¸ ë° ë²¤ì¹˜ë§ˆí¬
python scripts/test_flash_attention.py
```

**Flash Attention 2 íš¨ê³¼**:
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~30% ê°ì†Œ
- í›ˆë ¨ ì†ë„: ~2ë°° í–¥ìƒ
- Inference ì†ë„: ~2.2ë°° í–¥ìƒ

ìì„¸í•œ ë‚´ìš©ì€ [Flash Attention ê°€ì´ë“œ](docs/FLASH_ATTENTION_GUIDE.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### 4. ëª¨ë¸ ì„ íƒ ë° LoRA íŒŒì¸íŠœë‹

#### ê°„ë‹¨í•œ ë‹¨ì¼ ëª¨ë¸ íŒŒì¸íŠœë‹
```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
python select_model.py --list

# Qwen 0.5B ëª¨ë¸ë¡œ ë¹ ë¥¸ LoRA íŒŒì¸íŠœë‹
python quick_lora_train.py --model qwen_0.5b --lora-r 16 --lora-alpha 32
```

#### ëŒ€ê·œëª¨ Ablation Study
```bash
# ëŒ€í™”í˜• ëª¨ë¸ ì„ íƒ
python select_model.py --interactive

# Ablation study ì‹¤í–‰
python lora_ablation_study.py --config configs/custom_lora_ablation.yaml
```

## ğŸ“Š LoRA Ablation Study

PanoLLaVAëŠ” ì²´ê³„ì ì¸ LoRA ablation studyë¥¼ ì§€ì›í•©ë‹ˆë‹¤:

### ì§€ì› ëª¨ë¸ë“¤

**ì–¸ì–´ ëª¨ë¸:**
- Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B
- Llama-3.2: 1B, 3B
- Gemma-2: 2B, 9B

**ë¹„ì „ ëª¨ë¸:**
- SigLIP: Base (86M), Large (427M)
- CLIP: Base (151M), Large (427M)
- DINOv2: Base (86M), Large (307M)

### LoRA ì„¤ì • ì˜ˆì‹œ

```yaml
# configs/lora_ablation.yaml
experiment_name: "my_ablation_study"
models:
  - name: "qwen_0.5b"
    vision_name: "google/siglip-base-patch16-224"
    language_model_name: "Qwen/Qwen2.5-0.5B-Instruct"
    latent_dimension: 768

lora_configs:
  - lora_r: 16
    lora_alpha: 32
    lora_dropout: 0.1
  - lora_r: 32
    lora_alpha: 64
    lora_dropout: 0.1
```

## ğŸ—ï¸ Project Structure

```
panollava/
â”œâ”€â”€ src/panovlm/           # ë©”ì¸ íŒ¨í‚¤ì§€
â”‚   â”œâ”€â”€ models/           # ëª¨ë¸ ì•„í‚¤í…ì²˜
â”‚   â”œâ”€â”€ data/             # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ training/         # í•™ìŠµ ê´€ë ¨
â”‚   â””â”€â”€ evaluation/       # í‰ê°€ ë„êµ¬
â”œâ”€â”€ configs/              # ì„¤ì • íŒŒì¼ë“¤
â”œâ”€â”€ scripts/              # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ë“¤
â”œâ”€â”€ tests/                # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”œâ”€â”€ docs/                 # ë¬¸ì„œ
â”œâ”€â”€ notebooks/            # ì˜ˆì œ ë…¸íŠ¸ë¶
â””â”€â”€ results/              # ì‹¤í—˜ ê²°ê³¼
```

## ğŸ§­ Workflow Package

`workflow/` ë””ë ‰í† ë¦¬ëŠ” í•™ìŠµ/í‰ê°€/ì²´í¬í¬ì¸íŠ¸/ë©”íŠ¸ë¦­ ê´€ë ¨ ë¡œì§ì„ í•˜ë‚˜ë¡œ ë¬¶ì€ ê²½ëŸ‰ ë˜í¼ì…ë‹ˆë‹¤.

- `workflow.configuration.WorkflowConfig`: YAMLì„ ê°ì²´ë¡œ ë˜í•‘í•˜ê³  Stage ìˆœì„œë¥¼ ê°•ì œí•˜ê±°ë‚˜ ë°ì´í„°ì…‹ CSVë¥¼ ë³µì œí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `workflow.checkpointing.CheckpointResolver`: prefix/crop/stage ì¡°í•©ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ê³ , ì›í•˜ëŠ” `epoch`/`step` ë²ˆí˜¸ë¥¼ ì§€ì •í•´ í•´ë‹¹ ì‹œì ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë°”ë¡œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `workflow.training.ThreeStageTrainer`: Vision â†’ Resampler â†’ Finetuneì˜ 3ë‹¨ê³„ í•™ìŠµì„ ì½”ë“œ ì‹¤í–‰ë§Œìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- `workflow.evaluation.EvaluationRunner`: ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ ëª¨ë¸ê³¼ YAML/ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•œ ë’¤ í‰ê°€í•©ë‹ˆë‹¤.
- `workflow.metrics.compute_text_metrics`: ì˜ˆì¸¡/ì •ë‹µ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë§Œìœ¼ë¡œ BLEU, METEOR, ROUGE-L, SPICE, CIDErë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

ì¦‰ì‹œ ì‹¤í–‰í˜• íŒŒì´í”„ë¼ì¸ì´ í•„ìš”í•˜ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# ê¸°ë³¸ configë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
python workflow/runner.py
```

ë˜ëŠ” Python ëª¨ë“ˆë¡œ ì„ë² ë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from pathlib import Path
from workflow import WorkflowConfig, ThreeStageTrainer, EvaluationRunner

cfg = WorkflowConfig.load("configs/default.yaml")
trainer = ThreeStageTrainer(cfg)
final_ckpt = trainer.run()

checkpoint_dir = Path(final_ckpt).parent if final_ckpt else None
evaluator = EvaluationRunner(cfg)
metrics = evaluator.run(checkpoint_dir=checkpoint_dir)
```

## ğŸ”§ Development

### ë¹Œë“œ ë° í…ŒìŠ¤íŠ¸

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
make test

# ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
make lint

# Docker ë¹Œë“œ
make docker-build
```

### ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€

1. `src/panovlm/models/`ì— ëª¨ë¸ í´ë˜ìŠ¤ êµ¬í˜„
2. `src/panovlm/config/schema.py`ì— ì„¤ì • ì¶”ê°€
3. `select_model.py`ì— ëª¨ë¸ ì •ë³´ ì¶”ê°€
4. í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±

## ğŸ“ˆ Evaluation

PanoLLaVAëŠ” ë‹¤ì–‘í•œ í‰ê°€ ë©”íŠ¸ë¦­ì„ ì§€ì›í•©ë‹ˆë‹¤:

- **DINO Similarity**: ë¹„ì „ í”¼ì²˜ ìœ ì‚¬ë„ ë¶„ì„
- **Perplexity**: ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
- **BLEU Score**: í…ìŠ¤íŠ¸ ìƒì„± í’ˆì§ˆ í‰ê°€
- **Custom Metrics**: ì‚¬ìš©ìì˜ í‰ê°€ ì½”ë“œ í†µí•©

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```markdown

## ğŸ“Š í‰ê°€ (Evaluation)

### ìë™ í‰ê°€ ë©”íŠ¸ë¦­

PanoLLaVAëŠ” **5ê°€ì§€ ê³µì‹ í‰ê°€ ë©”íŠ¸ë¦­**ì„ ì§€ì›í•©ë‹ˆë‹¤:

| ë©”íŠ¸ë¦­ | ì„¤ëª… | ê³µì‹ êµ¬í˜„ | ì¶”ì²œ |
|--------|------|---------|------|
| **BLEU-4** | n-gram ì •í™•ë„ ê¸°ë°˜ | [sacrebleu](https://github.com/mjpost/sacrebleu) | â­ |
| **METEOR** | ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ | [NLTK](https://www.nltk.org/) | â­â­ |
| **ROUGE-L** | ìµœëŒ€ê³µí†µë¶€ë¶„ìˆ˜ì—´ | [rouge-score](https://github.com/google-research/rouge) | â­ |
| **SPICE** | ì˜ë¯¸ì  ëª…ì œ ë¶„ì„ | [pycocoevalcap](https://github.com/salaniz/pycocoevalcap) | â­â­â­ |
| **CIDEr** | TF-IDF ê¸°ë°˜ í‰ê°€ | [pycocoevalcap](https://github.com/salaniz/pycocoevalcap) | â­â­â­ |

### ì„¤ì¹˜

```bash
# ìë™ ì„¤ì¹˜ (ê¶Œì¥)
./install_eval_metrics.sh

# ë˜ëŠ” ìˆ˜ë™ ì„¤ì¹˜
pip install sacrebleu nltk rouge-score
pip install git+https://github.com/salaniz/pycocoevalcap.git
pip install sentence-transformers  # SPICE í´ë°±ìš©
```

### ì‚¬ìš© ë°©ë²•

```bash
# CSV ê¸°ë°˜ í‰ê°€
python scripts/eval.py --csv-input predictions.csv

# ëª¨ë¸ê³¼ í•¨ê»˜ í‰ê°€
python scripts/eval.py --checkpoint-dir runs/my_model/ \
                       --csv-input data/quic360/test.csv

# ì „ì²´ íŒŒì´í”„ë¼ì¸
python scripts/eval.py --config configs/default.yaml
```

### CSV í˜•ì‹

```csv
image_path,original_query,prediction,reference
path/to/img1.jpg,What is in the image?,Generated text,Reference text
path/to/img2.jpg,Describe the scene,Generated text,Reference text
...
```

### ê²°ê³¼ ì˜ˆì‹œ

```
ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­ ê²°ê³¼ (ê³µì‹ ë ˆí¬ì§€í† ë¦¬ ê¸°ë°˜):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ BLEU-4      (â†‘):   0.007838  | sacrebleu
âœ“ METEOR      (â†‘):   0.195023  | NLTK
âœ“ ROUGE-L     (â†‘):   0.146450  | rouge-score
âœ“ SPICE       (â†‘):   0.412910  | pycocoevalcap
âœ“ CIDEr       (â†‘):   0.004784  | pycocoevalcap
```

### ìì„¸í•œ ê°€ì´ë“œ

í‰ê°€ ë©”íŠ¸ë¦­ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ëŠ” [EVAL_METRICS_OFFICIAL_REPOS.md](docs/EVAL_METRICS_OFFICIAL_REPOS.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

---

## ğŸ™ Acknowledgments

- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [PyTorch Lightning](https://github.com/Lightning-AI/lightning)
- [PEFT](https://github.com/huggingface/peft)
- [SigLIP](https://arxiv.org/abs/2303.15343)
- [LLaVA](https://arxiv.org/abs/2304.08485)

## ğŸ“ Contact

- **Issues**: [GitHub Issues](https://github.com/wooseungw/panollava/issues)
- **Discussions**: [GitHub Discussions](https://github.com/wooseungw/panollava/discussions)

---

**PanoLLaVA**: íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ì˜ ìƒˆë¡œìš´ ì§€í‰ì„ ì—´ë‹¤! ğŸŒ…
