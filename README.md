# PanoLLaVA ğŸŒğŸ¦™

í—ˆê¹…í˜ì´ìŠ¤ì˜ Image Encoderì™€ LLM ëª¨ë¸ì„ ì¡°í•©í•˜ì—¬ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ì— ëŒ€í•œ í•™ìŠµ, ê²€ì¦, ì‹œê°í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ AI í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

PanoLLaVAëŠ” 360ë„ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ë¥¼ ì´í•´í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ì…ë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- ğŸ–¼ï¸ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ ì¸ì½”ë”© ë° íŠ¹ì§• ì¶”ì¶œ
- ğŸ¤– ëŒ€í™”í˜• íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ ë¶„ì„
- ğŸ“Š ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦
- ğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„
- ğŸ”§ ë‹¤ì–‘í•œ í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ê³¼ì˜ í˜¸í™˜ì„±

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

### ë©€í‹°ëª¨ë‹¬ íŒŒë…¸ë¼ë§ˆ ì´í•´
- 360ë„ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ ì²˜ë¦¬
- ê³µê°„ì  ê´€ê³„ ì´í•´
- ê°ì²´ ê°ì§€ ë° ë¶„í• 
- ì¥ë©´ ì„¤ëª… ìƒì„±

### ëŒ€í™”í˜• AI
- íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆì˜ì‘ë‹µ
- ìì—°ì–´ ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ì„
- ë©€í‹°í„´ ëŒ€í™” ì§€ì›

### í•™ìŠµ ë° í‰ê°€
- ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í•™ìŠµ
- ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
- ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
panollava/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model_config.yaml
â”‚   â””â”€â”€ training_config.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ image_encoder.py
â”‚   â”‚   â”œâ”€â”€ llm_model.py
â”‚   â”‚   â””â”€â”€ panollava_model.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py
â”‚   â”‚   â””â”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â””â”€â”€ loss.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ visualization.py
â”‚       â””â”€â”€ metrics.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ demo.ipynb
â”‚   â””â”€â”€ analysis.ipynb
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_data.py
â”‚   â””â”€â”€ test_training.py
â””â”€â”€ examples/
    â”œâ”€â”€ basic_usage.py
    â””â”€â”€ custom_training.py
```

## ğŸ› ï¸ ì„¤ì¹˜

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
- Python >= 3.8
- PyTorch >= 1.12.0
- transformers >= 4.20.0
- CUDA (GPU ì‚¬ìš© ì‹œ)

### ì„¤ì¹˜ ë°©ë²•

```bash
# ë ˆí¬ì§€í† ë¦¬ í´ë¡ 
git clone https://github.com/your-username/panollava.git
cd panollava

# ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ë˜ëŠ” venv\Scripts\activate  # Windows

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt # For core functionality

# ê°œë°œ ë° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì¶”ê°€ ì˜ì¡´ì„± ì„¤ì¹˜ (ì„ íƒ ì‚¬í•­)
pip install -r requirements-dev.txt

# ê°œë°œ ëª¨ë“œ ì„¤ì¹˜
pip install -e .
```

## ğŸ¯ ë¹ ë¥¸ ì‹œì‘

### ê¸°ë³¸ ì¶”ë¡ 

```python
from src.models.panollava_model import PanoLLaVAModel
from PIL import Image

# ëª¨ë¸ ë¡œë“œ
model = PanoLLaVAModel.from_pretrained("your-model-path")

# íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ ë¡œë“œ
pano_image = Image.open("path/to/panorama.jpg")

# ì§ˆì˜ì‘ë‹µ
question = "ì´ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ì—ì„œ ì–´ë–¤ ê°ì²´ë“¤ì„ ë³¼ ìˆ˜ ìˆë‚˜ìš”?"
response = model.generate(pano_image, question)
print(response)
```

### ëª¨ë¸ í•™ìŠµ

```python
from src.training.trainer import PanoLLaVATrainer
from src.data.dataset import PanoDataset

# ë°ì´í„°ì…‹ ì¤€ë¹„
train_dataset = PanoDataset("path/to/train_data")
val_dataset = PanoDataset("path/to/val_data")

# íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
trainer = PanoLLaVATrainer(
    model=model,
    train_dataset=train_dataset,
    val_dataset=val_dataset
)

# í•™ìŠµ ì‹œì‘
trainer.train()
```

### CLI ì‚¬ìš©

#### 3ë‹¨ê³„ ì „ì²´ í•™ìŠµ (ê¸°ë³¸)

```bash
# ì „ì²´ ìŠ¤í…Œì´ì§€ í•™ìŠµ (vision â†’ resampler â†’ finetune)
bash scripts/train_all_stages.sh

# íŠ¹ì • ìŠ¤í…Œì´ì§€ë§Œ í•™ìŠµ
python train.py --stage vision --epochs 3 --batch-size 16
python train.py --stage resampler --epochs 1 --batch-size 4
python train.py --stage finetune --epochs 1 --batch-size 4
```

#### LoRAë¥¼ ì‚¬ìš©í•œ íš¨ìœ¨ì  Finetune í•™ìŠµ

PanoLLaVAëŠ” ë§ˆì§€ë§‰ finetune ë‹¨ê³„ì—ì„œ LoRA(Low-Rank Adaptation)ë¥¼ ì§€ì›í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```bash
# LoRAë¥¼ ì‚¬ìš©í•œ finetune ë‹¨ê³„ í•™ìŠµ
bash scripts/stage3_finetune_lora_train.sh

# ë˜ëŠ” ì§ì ‘ íŒŒë¼ë¯¸í„° ì§€ì •
python train.py \
    --stage finetune \
    --use-lora \
    --lora-rank 16 \
    --lora-alpha 32 \
    --lora-dropout 0.1 \
    --batch-size 4 \
    --epochs 1
```

**LoRA í•™ìŠµì˜ ì¥ì :**
- ğŸ”¥ **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì „ì²´ì˜ 1-5%ë¡œ ê°ì†Œ
- âš¡ **ë¹ ë¥¸ í•™ìŠµ**: ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ì¸í•œ ë¹ ë¥¸ í•™ìŠµ ì†ë„
- ğŸ’¾ **ì‘ì€ ëª¨ë¸ í¬ê¸°**: LoRA ê°€ì¤‘ì¹˜ë§Œ ì €ì¥í•˜ë©´ ìš©ëŸ‰ ì ˆì•½
- ğŸ”„ **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë³„ LoRA ì–´ëŒ‘í„° ìƒì„± ê°€ëŠ¥

**LoRA íŒŒë¼ë¯¸í„° ì„¤ëª…:**
- `--lora-rank`: LoRAì˜ rank (16-64 ê¶Œì¥, ë‚®ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ì ìŒ)
- `--lora-alpha`: LoRA alpha ê°’ (ì¼ë°˜ì ìœ¼ë¡œ rankì˜ 2ë°°)
- `--lora-dropout`: LoRA dropout rate (ê³¼ì í•© ë°©ì§€)
- `--save-lora-only`: LoRA ê°€ì¤‘ì¹˜ë§Œ ì €ì¥ (ê¸°ë³¸ ëª¨ë¸ ì œì™¸)

#### LoRA ëª¨ë¸ ë³‘í•© ë° ë°°í¬

```python
from panovlm.model import PanoramaVLM

# ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
model = PanoramaVLM(...)

# LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
model.load_lora_weights("./runs/e2p_finetune_mlp/lora_weights")

# LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•© (ë°°í¬ìš©)
model.merge_lora_weights()

# ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), "merged_model.pth")
```

```bash
# í•™ìŠµ
python scripts/train.py --config config/training_config.yaml

# í‰ê°€
python scripts/evaluate.py --model-path checkpoints/best_model --test-data data/test

# ì¶”ë¡ 
python scripts/inference.py --image panorama.jpg --question "Describe this scene"
```

## ğŸ“Š ì§€ì›í•˜ëŠ” ëª¨ë¸

### Image Encoders
- **CLIP**: `openai/clip-vit-base-patch32`
- **DINOv2**: `facebook/dinov2-base`
- **SigLIP**: `google/siglip-base-patch16-224`

### Language Models
- **LLaMA**: `meta-llama/Llama-2-7b-chat-hf`
- **Vicuna**: `lmsys/vicuna-7b-v1.5`
- **Mistral**: `mistralai/Mistral-7B-Instruct-v0.1`

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| ëª¨ë¸ ì¡°í•© | íŒŒë…¸ë¼ë§ˆ QA ì •í™•ë„ | ê°ì²´ ê°ì§€ mAP | ì¶”ë¡  ì†ë„ (ms) |
|-----------|-------------------|---------------|----------------|
| CLIP + LLaMA-7B | 85.2% | 72.4% | 1,250 |
| DINOv2 + Vicuna-7B | 87.1% | 75.8% | 1,180 |
| SigLIP + Mistral-7B | 88.3% | 74.2% | 1,050 |

## ğŸ”§ ì„¤ì •

### ëª¨ë¸ ì„¤ì • (`config/model_config.yaml`)

```yaml
model:
  image_encoder:
    name: "openai/clip-vit-base-patch32"
    freeze: false
  llm:
    name: "meta-llama/Llama-2-7b-chat-hf"
    freeze_layers: 20
  
panorama:
  resolution: [512, 1024]  # height, width
  projection: "equirectangular"
  crop_strategy: "adaptive"
```

### í•™ìŠµ ì„¤ì • (`config/training_config.yaml`)

```yaml
training:
  batch_size: 8
  learning_rate: 1e-4
  num_epochs: 10
  warmup_steps: 1000
  
data:
  train_data_path: "data/train"
  val_data_path: "data/val"
  augmentation: true
  
output:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
```

## ğŸ“š ë°ì´í„°ì…‹ í˜•ì‹

### íŒŒë…¸ë¼ë§ˆ QA ë°ì´í„°ì…‹

```json
{
  "image_id": "pano_001",
  "image_path": "images/panorama_001.jpg",
  "conversations": [
    {
      "human": "ì´ íŒŒë…¸ë¼ë§ˆì—ì„œ ë³´ì´ëŠ” ê±´ë¬¼ì˜ íŠ¹ì§•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
      "assistant": "ì´ íŒŒë…¸ë¼ë§ˆì—ì„œëŠ” í˜„ëŒ€ì ì¸ ê³ ì¸µ ë¹Œë”©ë“¤ì´ ë³´ì…ë‹ˆë‹¤..."
    }
  ],
  "metadata": {
    "location": "Seoul, Korea",
    "camera_height": 1.7,
    "timestamp": "2024-01-15T14:30:00Z"
  }
}
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest tests/

# íŠ¹ì • í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest tests/test_models.py

# ì»¤ë²„ë¦¬ì§€ í¬í•¨ í…ŒìŠ¤íŠ¸
python -m pytest tests/ --cov=src/
```

## ğŸ“– ì˜ˆì œ

### Jupyter Notebook ë°ëª¨
- `notebooks/demo.ipynb`: ê¸°ë³¸ ì‚¬ìš©ë²•ê³¼ ì‹œê°í™”
- `notebooks/analysis.ipynb`: ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„

### Python ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì œ
- `examples/basic_usage.py`: ê¸°ë³¸ ì¶”ë¡  ì˜ˆì œ
- `examples/custom_training.py`: ì»¤ìŠ¤í…€ í•™ìŠµ ì˜ˆì œ

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. í¬í¬ (Fork) ìƒì„±
2. ê¸°ëŠ¥ ë¸Œëœì¹˜ ìƒì„± (`git checkout -b feature/amazing-feature`)
3. ë³€ê²½ì‚¬í•­ ì»¤ë°‹ (`git commit -m 'Add amazing feature'`)
4. ë¸Œëœì¹˜ì— í‘¸ì‹œ (`git push origin feature/amazing-feature`)
5. Pull Request ìƒì„±

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” [MIT ë¼ì´ì„ ìŠ¤](LICENSE) í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

## ğŸ“ ë¬¸ì˜

- **ì €ì**: Your Name
- **ì´ë©”ì¼**: your.email@example.com
- **í”„ë¡œì íŠ¸ ë§í¬**: https://github.com/your-username/panollava

## ğŸ™ ê°ì‚¬ì˜ ë§

- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [CLIP](https://github.com/openai/CLIP)
- PyTorch íŒ€

## ğŸ“ ì—…ë°ì´íŠ¸ ë¡œê·¸

### v1.0.0 (2024-07-10)
- ì´ˆê¸° ë¦´ë¦¬ìŠ¤
- ê¸°ë³¸ íŒŒë…¸ë¼ë§ˆ ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ëŠ¥
- ë©€í‹°ëª¨ë‹¬ QA ì‹œìŠ¤í…œ
- í•™ìŠµ ë° í‰ê°€ íŒŒì´í”„ë¼ì¸

---

â­ ì´ í”„ë¡œì íŠ¸ê°€ ìœ ìš©í•˜ë‹¤ë©´ ë³„í‘œë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!
