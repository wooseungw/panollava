# PanoLLaVA Hyperparameter-Tuned 3-Stage Config
# Baseline: BLEU=0.02, METEOR=0.16, ROUGE-L=0.19, CIDEr=0.10
# GPU: RTX 3090 24GB x2
# NOTE: vision encoder receives lr * 0.1 automatically (hardcoded in module.py)
# NOTE: Architecture changed → MUST NOT resume from old checkpoints

experiment:
  name: "hp_tuned_3stage"
  description: "Hyperparameter-tuned 3-stage training"
  version: "2.0"

environment:
  wandb_project: "panollava-training"

paths:
  runs_dir: "runs"
  csv_train: "train.csv"
  csv_val: "val.csv"
  csv_test: "val.csv"

models:
  vision_name: "google/siglip2-so400m-patch16-256"
  vision_backbone_type: "hf"
  vision_input_key: "pixel_values"
  vision_output_key: "last_hidden_state"
  vision_hidden_size: null
  use_vicreg_norm: false

  language_model_name: "Qwen/Qwen3-0.6B"

  resampler_type: "bimamba"
  latent_dimension: 1024          # 768 → 1024 (LLM hidden_size 매칭, Projector identity-like)
  resampler_config:
    hidden_dim: 1024
    num_layers: 5                  # 4 → 5 (6은 OOM 위험)
    expand: 2.0
    d_state: 128                   # 64 → 128
    d_conv: 4
    dropout: 0.03                  # 0.05 → 0.03
    norm_first: true

  use_projection_positional_encoding: true
  pe_view_encoding_type: "sinusoidal"
  pe_spatial_encoding_type: "sinusoidal"
  pe_enable_continuity: true

  use_vicreg_projector: true
  vicreg_projector_dim: 1024     # null → 1024
  vicreg_projector_depth: 2      # 2 유지 (depth=3은 speculative)
  vicreg_projector_ln: true

  use_text_projection: false

image_processing:
  crop_strategy: "anyres_e2p"
  image_size: [256, 256]         # 명시적 지정 (SigLIP2 네이티브 해상도)
  overlap_ratio: 0.5
  stitching_mode: "concat"
  stitch_target_to_view_width: true
  stitch_interp: "linear"
  fov_deg: 90.0
  use_vision_processor: true
  anyres_max_patches: 9
  normalize: true

training:
  stages: ["vision", "resampler", "finetune"]
  num_workers: 8
  eval_batch_size: 2
  system_msg: "You are a helpful assistant. Describe the panorama image."
  wandb_project: "panollava-training"
  cache_cleanup_interval: 500

  auto_eval:
    enabled: true
    stage: "finetune"
    checkpoint: "best"
    csv: "val.csv"

  deepspeed:
    enabled: false
    strategy:
      stage: 2

  stage_configs:
    vision:
      epochs: 5                           # 3 → 5
      lr: 3e-4                            # 1e-4 → 3e-4 (vision encoder는 자동으로 3e-5)
      batch_size: 4                       # 8 → 4 (BiMamba 확장으로 메모리 절약)
      accumulate_grad_batches: 4          # effective bs = 16
      vicreg_loss_weight: 1.0
      vicreg_mode: "pairwise"
      vision_trainable_blocks: 3          # 2 → 3
      vicreg_similarity_weight: 25.0
      vicreg_variance_weight: 25.0
      vicreg_covariance_weight: 1.0
      debug_vicreg_loss: false
      max_text_length: 32
      data:
        csv_train:
          - "train.csv"
        csv_val:
          - "val.csv"

    resampler:
      epochs: 3                           # 1 → 3
      lr: 1e-4
      batch_size: 1
      accumulate_grad_batches: 16         # effective bs = 16
      vicreg_loss_weight: 0.0
      vision_trainable_blocks: 0
      max_text_length: 256                # 128 → 256

    finetune:
      epochs: 3                           # 1 → 3
      lr: 5e-5                            # 2e-6 → 5e-5
      batch_size: 1
      accumulate_grad_batches: 16         # effective bs = 16
      vicreg_loss_weight: 0.0
      vision_trainable_blocks: 2          # 0 → 2
      max_text_length: 256                # 128 → 256

generation:
  max_new_tokens: 256                     # 128 → 256
  temperature: 0.6
  top_p: 0.9
  do_sample: true
  repetition_penalty: 1.15

data:
  train: ["train.csv"]
  val: ["val.csv"]
  csv_test: "val.csv"
  max_text_length: "auto"
  auto_max_text_length_cap: 512           # 256 → 512

system_messages:
  default: "You are a helpful assistant. Describe the panorama image."

lora:
  use_lora: true
  rank: 64                               # 32 → 64
  alpha: 128                             # 64 → 128
  dropout: 0.05                          # 0.1 → 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  save_lora_only: false
