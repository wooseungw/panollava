experiment_name: "native_gemma3-4b"
output_dir: "runs/baseline"
max_new_tokens: 128

model:
  name: "native_gemma3-4b"
  hf_model_id: "google/gemma-3-4b-it"
  model_type: "gemma3"
  dtype: "bfloat16"
  image_size: 896

lora:
  r: 32
  alpha: 64
  dropout: 0.1

training:
  num_epochs: 1
  batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 5e-5
  warmup_ratio: 0.03
  weight_decay: 0.01
  max_grad_norm: 1.0
  max_length: 3072
  seed: 42
  gradient_checkpointing: true
  mixed_precision: "bf16"
  logging_steps: 10
  save_strategy: "epoch"
  save_total_limit: 2
  dataloader_num_workers: 4

data:
  image_column: "url"
  instruction_column: "instruction"
  response_column: "response"

data_train_csv: "/data/1_personal/4_SWWOO/panollava/runs/baseline/_shared_data/train.csv"
data_test_csv: "/data/1_personal/4_SWWOO/panollava/runs/baseline/_shared_data/test.csv"

wandb_enabled: false
