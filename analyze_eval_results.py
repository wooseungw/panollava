#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PanoLLaVA í‰ê°€ ê²°ê³¼ ë¶„ì„ ë„êµ¬
================================
CSV í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ë‹¤ì–‘í•œ ì§€í‘œë¥¼ ê³„ì‚°í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python analyze_eval_results.py --csv-file lora_finetune_eval_results/finetune_detailed_results_20250803_090307.csv
    python analyze_eval_results.py --csv-file results.csv --save-plots --output-dir analysis_output
"""

import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib) - ì„œë²„ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •
try:
    plt.rcParams['font.family'] = ['DejaVu Sans']
except:
    plt.rcParams['font.family'] = ['sans-serif']
plt.rcParams['axes.unicode_minus'] = False

class EvaluationAnalyzer:
    """í‰ê°€ ê²°ê³¼ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, csv_file: str):
        """
        Args:
            csv_file: í‰ê°€ ê²°ê³¼ CSV íŒŒì¼ ê²½ë¡œ
        """
        self.csv_file = Path(csv_file)
        self.df = None
        self.metrics = {}
        self.load_data()
    
    def load_data(self):
        """CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            self.df = pd.read_csv(self.csv_file)
            print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)} ìƒ˜í”Œ")
            print(f"  ì»¬ëŸ¼: {list(self.df.columns)}")
            
            # ê¸°ë³¸ í†µê³„
            print(f"\n=== ê¸°ë³¸ í†µê³„ ===")
            print(f"ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(self.df)}")
            print(f"ìŠ¤í…Œì´ì§€: {self.df['stage'].unique()}")
            print(f"ë¹ˆ ì˜ˆì¸¡: {self.df['is_empty_prediction'].sum()}")
            print(f"ê°œë³„ ì²˜ë¦¬: {self.df['individual_processing'].sum()}")
            
            # ë°ì´í„° ì •ë¦¬
            self.df['prediction'] = self.df['prediction'].fillna('')
            self.df['reference'] = self.df['reference'].fillna('')
            
        except Exception as e:
            raise ValueError(f"CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def compute_text_metrics(self) -> Dict[str, float]:
        """í…ìŠ¤íŠ¸ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print(f"\n=== í…ìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚° ===")
        
        metrics = {}
        valid_samples = []
        valid_predictions = []
        
        # ìœ íš¨í•œ ìƒ˜í”Œ í•„í„°ë§ (ì°¸ì¡° í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°)
        for idx, row in self.df.iterrows():
            pred = str(row['prediction']).strip()
            ref = str(row['reference']).strip()
            
            if pred and ref:  # ë‘˜ ë‹¤ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°
                valid_samples.append((pred, ref))
            elif pred:  # ì˜ˆì¸¡ë§Œ ìˆëŠ” ê²½ìš° (ì°¸ì¡° ì—†ëŠ” ë©”íŠ¸ë¦­ìš©)
                valid_predictions.append(pred)
        
        print(f"ì°¸ì¡°ê°€ ìˆëŠ” ìœ íš¨í•œ ìƒ˜í”Œ: {len(valid_samples)}/{len(self.df)}")
        print(f"ì˜ˆì¸¡ë§Œ ìˆëŠ” ìƒ˜í”Œ: {len(valid_predictions)}/{len(self.df)}")
        
        # ì°¸ì¡°ê°€ ìˆëŠ” ê²½ìš° BLEU, ROUGE ë“± ê³„ì‚°
        if len(valid_samples) > 0:
            predictions, references = zip(*valid_samples)
        
        # ì°¸ì¡°ê°€ ìˆëŠ” ê²½ìš° BLEU, ROUGE ë“± ê³„ì‚°
        if len(valid_samples) > 0:
            predictions, references = zip(*valid_samples)
            
            # BLEU ì ìˆ˜ ê³„ì‚°
            try:
                import nltk
                from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
                
                nltk.download('punkt', quiet=True)
                
                ref_tokens = [[ref.split()] for ref in references]
                pred_tokens = [pred.split() for pred in predictions]
                smoothing = SmoothingFunction().method1
                
                metrics.update({
                    'bleu1': corpus_bleu(ref_tokens, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing),
                    'bleu2': corpus_bleu(ref_tokens, pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing),
                    'bleu3': corpus_bleu(ref_tokens, pred_tokens, weights=(1/3, 1/3, 1/3, 0), smoothing_function=smoothing),
                    'bleu4': corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)
                })
                print("âœ“ BLEU ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ")
                
            except ImportError:
                print("âœ— NLTK ì—†ìŒ - BLEU ê±´ë„ˆëœ€")
            except Exception as e:
                print(f"âœ— BLEU ê³„ì‚° ì˜¤ë¥˜: {e}")
            
            # ROUGE ì ìˆ˜ ê³„ì‚°
            try:
                from rouge_score import rouge_scorer
                
                scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
                rouge1_scores, rouge2_scores, rougeL_scores = [], [], []
                
                for ref, pred in zip(references, predictions):
                    scores = scorer.score(ref, pred)
                    rouge1_scores.append(scores['rouge1'].fmeasure)
                    rouge2_scores.append(scores['rouge2'].fmeasure)
                    rougeL_scores.append(scores['rougeL'].fmeasure)
                
                metrics.update({
                    'rouge1': np.mean(rouge1_scores),
                    'rouge2': np.mean(rouge2_scores),
                    'rougeL': np.mean(rougeL_scores)
                })
                print("âœ“ ROUGE ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ")
                
            except ImportError:
                print("âœ— rouge-score ì—†ìŒ - ROUGE ê±´ë„ˆëœ€")
            except Exception as e:
                print(f"âœ— ROUGE ê³„ì‚° ì˜¤ë¥˜: {e}")
            
            # METEOR ì ìˆ˜ ê³„ì‚°
            try:
                from nltk.translate.meteor_score import meteor_score
                
                meteor_scores = []
                for ref, pred in zip(references, predictions):
                    try:
                        score = meteor_score([ref.split()], pred.split())
                        meteor_scores.append(score)
                    except:
                        meteor_scores.append(0.0)
                
                metrics['meteor'] = np.mean(meteor_scores)
                print("âœ“ METEOR ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ")
                
            except ImportError:
                print("âœ— NLTK METEOR ì—†ìŒ - METEOR ê±´ë„ˆëœ€")
            except Exception as e:
                print(f"âœ— METEOR ê³„ì‚° ì˜¤ë¥˜: {e}")
        else:
            print("ğŸ“Œ ì°¸ì¡° í…ìŠ¤íŠ¸ê°€ ì—†ì–´ BLEU/ROUGE/METEOR ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        # CLIP Score ê³„ì‚° (ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„)
        if len(valid_predictions) > 0:
            print("ğŸ“Œ CLIP Score ê³„ì‚° ì‹œë„...")
            try:
                import torch
                import clip
                from PIL import Image
                
                # CLIP ëª¨ë¸ ë¡œë“œ
                device = "cuda" if torch.cuda.is_available() else "cpu"
                model, preprocess = clip.load("ViT-B/32", device=device)
                
                clip_scores = []
                processed_images = 0
                
                for idx, row in self.df.iterrows():
                    if str(row['prediction']).strip():
                        try:
                            # ì´ë¯¸ì§€ ë¡œë“œ
                            image_path = row['image_path']
                            if not Path(image_path).exists():
                                # ìƒëŒ€ ê²½ë¡œë¡œ ë‹¤ì‹œ ì‹œë„
                                image_path = Path("/data/1_personal/4_SWWOO/panollava") / image_path
                            
                            if Path(image_path).exists():
                                image = Image.open(image_path).convert('RGB')
                                
                                # ì „ì²˜ë¦¬
                                image_input = preprocess(image).unsqueeze(0).to(device)
                                text_input = clip.tokenize([row['prediction']], truncate=True).to(device)
                                
                                # CLIP ìŠ¤ì½”ì–´ ê³„ì‚°
                                with torch.no_grad():
                                    image_features = model.encode_image(image_input)
                                    text_features = model.encode_text(text_input)
                                    
                                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                                    similarity = torch.cosine_similarity(image_features, text_features).item()
                                    clip_scores.append(similarity)
                                    processed_images += 1
                            
                            # ë„ˆë¬´ ë§ìœ¼ë©´ ìƒ˜í”Œë§ (ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•)
                            if processed_images >= 100:
                                break
                                
                        except Exception as e:
                            continue
                
                if clip_scores:
                    metrics['clip_score'] = np.mean(clip_scores)
                    metrics['clip_score_std'] = np.std(clip_scores)
                    print(f"âœ“ CLIP Score ê³„ì‚° ì™„ë£Œ: {processed_images}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬")
                else:
                    print("âœ— CLIP Score ê³„ì‚° ì‹¤íŒ¨: ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì—†ìŒ")
                
            except ImportError:
                print("âœ— CLIP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - CLIP Score ê±´ë„ˆëœ€")
                print("  ì„¤ì¹˜: pip install git+https://github.com/openai/CLIP.git")
            except Exception as e:
                print(f"âœ— CLIP Score ê³„ì‚° ì˜¤ë¥˜: {e}")

        # ì°¸ì¡° ì—†ëŠ” í…ìŠ¤íŠ¸ í’ˆì§ˆ ë©”íŠ¸ë¦­
        if len(valid_predictions) > 0:
            print("ğŸ“Œ ì°¸ì¡° ì—†ëŠ” í…ìŠ¤íŠ¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°...")
            
            # ì–´íœ˜ ë‹¤ì–‘ì„± (Vocabulary Diversity)
            all_words = []
            for pred in valid_predictions:
                all_words.extend(pred.lower().split())
            
            if all_words:
                unique_words = set(all_words)
                metrics['vocabulary_diversity'] = len(unique_words) / len(all_words)
                metrics['total_unique_words'] = len(unique_words)
                metrics['total_words'] = len(all_words)
            
            # í‰ê·  ë¬¸ì¥ ê¸¸ì´ì™€ ë³µì¡ì„±
            sentence_lengths = [len(pred.split()) for pred in valid_predictions]
            metrics['avg_sentence_length'] = np.mean(sentence_lengths)
            metrics['sentence_length_std'] = np.std(sentence_lengths)
            
            # ë°˜ë³µì„± ì¸¡ì • (ê°™ì€ êµ¬ë¬¸ì˜ ë°˜ë³µ)
            pred_set = set(valid_predictions)
            metrics['prediction_uniqueness'] = len(pred_set) / len(valid_predictions)
            
        
        # ê¸°ë³¸ í†µê³„ (ì°¸ì¡°ê°€ ìˆëŠ” ê²½ìš°)
        if len(valid_samples) > 0:
            predictions_with_ref, references = zip(*valid_samples)
            ref_lengths = [len(ref.split()) for ref in references]
            pred_lengths_with_ref = [len(pred.split()) for pred in predictions_with_ref]
            
            metrics.update({
                'avg_ref_length': np.mean(ref_lengths),
                'avg_pred_length_with_ref': np.mean(pred_lengths_with_ref),
                'length_ratio': np.mean(pred_lengths_with_ref) / np.mean(ref_lengths) if np.mean(ref_lengths) > 0 else 0,
            })
        
        # ì „ì²´ ì˜ˆì¸¡ì— ëŒ€í•œ ê¸°ë³¸ í†µê³„
        all_pred_lengths = [len(str(pred).split()) for pred in self.df['prediction'] if str(pred).strip()]
        if all_pred_lengths:
            metrics.update({
                'total_predictions': len(all_pred_lengths),
                'avg_pred_length_all': np.mean(all_pred_lengths),
                'empty_predictions_ratio': self.df['is_empty_prediction'].mean()
            })
        
        self.metrics = metrics
        return metrics
    
    def _convert_numpy_types(self, obj):
        """numpy íƒ€ì…ì„ JSON í˜¸í™˜ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        if isinstance(obj, dict):
            return {k: self._convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(v) for v in obj]
        elif isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj
    
    def _json_serializer(self, obj):
        """JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ serializer"""
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif hasattr(obj, 'item'):  # numpy scalar
            return obj.item()
        else:
            return str(obj)
    
    def _deep_convert_to_json_compatible(self, obj):
        """ê¹Šì€ ë³€í™˜ìœ¼ë¡œ ëª¨ë“  numpy íƒ€ì…ì„ JSON í˜¸í™˜ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        if isinstance(obj, dict):
            return {str(k): self._deep_convert_to_json_compatible(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._deep_convert_to_json_compatible(v) for v in obj]
        elif isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif hasattr(obj, 'item'):  # numpy scalar
            return obj.item()
        elif obj is None:
            return None
        else:
            try:
                # JSONìœ¼ë¡œ ì§ë ¬í™” ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸
                json.dumps(obj)
                return obj
            except:
                return str(obj)
    
    def analyze_length_distribution(self) -> Dict[str, any]:
        """í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ ë¶„ì„"""
        print(f"\n=== ê¸¸ì´ ë¶„í¬ ë¶„ì„ ===")
        
        analysis = {}
        
        # ì˜ˆì¸¡ ê¸¸ì´ ë¶„ì„
        pred_lengths = self.df['prediction_length'].values
        ref_lengths = self.df['reference_length'].values
        
        analysis['prediction_lengths'] = {
            'mean': np.mean(pred_lengths),
            'std': np.std(pred_lengths),
            'min': np.min(pred_lengths),
            'max': np.max(pred_lengths),
            'median': np.median(pred_lengths),
            'q25': np.percentile(pred_lengths, 25),
            'q75': np.percentile(pred_lengths, 75)
        }
        
        analysis['reference_lengths'] = {
            'mean': np.mean(ref_lengths),
            'std': np.std(ref_lengths),
            'min': np.min(ref_lengths),
            'max': np.max(ref_lengths),
            'median': np.median(ref_lengths),
            'q25': np.percentile(ref_lengths, 25),
            'q75': np.percentile(ref_lengths, 75)
        }
        
        print(f"ì˜ˆì¸¡ ê¸¸ì´: í‰ê·  {analysis['prediction_lengths']['mean']:.1f} Â± {analysis['prediction_lengths']['std']:.1f}")
        print(f"ì°¸ì¡° ê¸¸ì´: í‰ê·  {analysis['reference_lengths']['mean']:.1f} Â± {analysis['reference_lengths']['std']:.1f}")
        
        return analysis
    
    def analyze_error_patterns(self) -> Dict[str, any]:
        """ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„"""
        print(f"\n=== ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ===")
        
        analysis = {}
        
        # ë¹ˆ ì˜ˆì¸¡ ë¶„ì„
        empty_pred_ratio = self.df['is_empty_prediction'].mean()
        analysis['empty_predictions'] = {
            'count': self.df['is_empty_prediction'].sum(),
            'ratio': empty_pred_ratio,
            'percentage': empty_pred_ratio * 100
        }
        
        # ê°œë³„ ì²˜ë¦¬ ë¶„ì„
        individual_proc_ratio = self.df['individual_processing'].mean()
        analysis['individual_processing'] = {
            'count': self.df['individual_processing'].sum(),
            'ratio': individual_proc_ratio,
            'percentage': individual_proc_ratio * 100
        }
        
        # ë°°ì¹˜ë³„ ë¶„ì„
        batch_analysis = self.df.groupby('batch_idx').agg({
            'is_empty_prediction': 'mean',
            'individual_processing': 'mean',
            'prediction_length': 'mean'
        }).reset_index()
        
        analysis['batch_performance'] = {
            'total_batches': len(batch_analysis),
            'avg_empty_per_batch': batch_analysis['is_empty_prediction'].mean(),
            'avg_individual_per_batch': batch_analysis['individual_processing'].mean()
        }
        
        print(f"ë¹ˆ ì˜ˆì¸¡: {analysis['empty_predictions']['count']} ({analysis['empty_predictions']['percentage']:.1f}%)")
        print(f"ê°œë³„ ì²˜ë¦¬: {analysis['individual_processing']['count']} ({analysis['individual_processing']['percentage']:.1f}%)")
        
        return analysis
    
    def create_visualizations(self, output_dir: Optional[Path] = None):
        """ì‹œê°í™” ìƒì„±"""
        print(f"\n=== ì‹œê°í™” ìƒì„± ===")
        
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True)
        
        try:
            # 1. ê¸¸ì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('Text Length Distribution Analysis', fontsize=16)
            
            # ì˜ˆì¸¡ ê¸¸ì´ ë¶„í¬
            pred_lengths = self.df['prediction_length'].dropna()
            if len(pred_lengths) > 0:
                axes[0, 0].hist(pred_lengths, bins=min(30, len(pred_lengths)//2), alpha=0.7, color='blue')
                axes[0, 0].set_title('Prediction Length Distribution')
                axes[0, 0].set_xlabel('Length (words)')
                axes[0, 0].set_ylabel('Frequency')
            
            # ì°¸ì¡° ê¸¸ì´ ë¶„í¬
            ref_lengths = self.df['reference_length'].dropna()
            if len(ref_lengths) > 0 and ref_lengths.max() > 0:
                axes[0, 1].hist(ref_lengths, bins=min(30, len(ref_lengths)//2), alpha=0.7, color='green')
                axes[0, 1].set_title('Reference Length Distribution')
                axes[0, 1].set_xlabel('Length (words)')
                axes[0, 1].set_ylabel('Frequency')
            else:
                axes[0, 1].text(0.5, 0.5, 'No reference data\n(all lengths = 0)', 
                              ha='center', va='center', transform=axes[0, 1].transAxes)
                axes[0, 1].set_title('Reference Length Distribution')
            
            # ê¸¸ì´ ë¹„êµ ì‚°ì ë„
            valid_refs = self.df[self.df['reference_length'] > 0]
            if len(valid_refs) > 0:
                axes[1, 0].scatter(valid_refs['reference_length'], valid_refs['prediction_length'], alpha=0.5)
                max_len = max(valid_refs['reference_length'].max(), valid_refs['prediction_length'].max())
                axes[1, 0].plot([0, max_len], [0, max_len], 'r--')
                axes[1, 0].set_title('Prediction vs Reference Length')
                axes[1, 0].set_xlabel('Reference Length')
                axes[1, 0].set_ylabel('Prediction Length')
            else:
                # ì°¸ì¡° ê¸¸ì´ê°€ ëª¨ë‘ 0ì¸ ê²½ìš° - ì˜ˆì¸¡ ê¸¸ì´ë§Œ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ í‘œì‹œ
                axes[1, 0].hist(self.df['prediction_length'], bins=20, alpha=0.7, color='orange')
                axes[1, 0].set_title('Prediction Length Only (No Reference)')
                axes[1, 0].set_xlabel('Prediction Length')
                axes[1, 0].set_ylabel('Frequency')
            
            # ë°°ì¹˜ë³„ ì„±ëŠ¥
            batch_stats = self.df.groupby('batch_idx')['prediction_length'].mean()
            if len(batch_stats) > 0:
                axes[1, 1].plot(batch_stats.index, batch_stats.values, 'o-')
                axes[1, 1].set_title('Average Prediction Length by Batch')
                axes[1, 1].set_xlabel('Batch Index')
                axes[1, 1].set_ylabel('Average Length')
                axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if output_dir:
                plt.savefig(output_dir / 'length_distribution.png', dpi=300, bbox_inches='tight')
                print(f"âœ“ ê¸¸ì´ ë¶„í¬ ì°¨íŠ¸ ì €ì¥: {output_dir / 'length_distribution.png'}")
            else:
                plt.show()
            
            # 2. ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
            fig, axes = plt.subplots(1, 3, figsize=(18, 5))
            fig.suptitle('Error Pattern Analysis', fontsize=16)
            
            # ë¹ˆ ì˜ˆì¸¡ ë¹„ìœ¨
            empty_counts = self.df['is_empty_prediction'].value_counts()
            if len(empty_counts) == 2:
                labels = ['Valid Prediction', 'Empty Prediction']
                values = [empty_counts.get(False, 0), empty_counts.get(True, 0)]
            else:
                # ëª¨ë“  ì˜ˆì¸¡ì´ ìœ íš¨í•œ ê²½ìš°
                labels = ['Valid Prediction']
                values = [len(self.df)]
            
            axes[0].pie(values, labels=labels, autopct='%1.1f%%', startangle=90)
            axes[0].set_title('Empty Predictions Ratio')
            
            # ê°œë³„ ì²˜ë¦¬ ë¹„ìœ¨
            individual_counts = self.df['individual_processing'].value_counts()
            if len(individual_counts) == 2:
                ind_labels = ['Batch Processing', 'Individual Processing']
                ind_values = [individual_counts.get(False, 0), individual_counts.get(True, 0)]
            else:
                # ëª¨ë“  ì²˜ë¦¬ê°€ ë°°ì¹˜ ì²˜ë¦¬ì¸ ê²½ìš°
                ind_labels = ['Batch Processing']
                ind_values = [len(self.df)]
                
            axes[1].pie(ind_values, labels=ind_labels, autopct='%1.1f%%', startangle=90)
            axes[1].set_title('Individual Processing Ratio')
            
            # ë°°ì¹˜ë³„ ì˜¤ë¥˜ìœ¨
            batch_error_rates = self.df.groupby('batch_idx')['is_empty_prediction'].mean()
            if len(batch_error_rates) > 0:
                batch_indices = list(range(len(batch_error_rates)))
                axes[2].bar(batch_indices, batch_error_rates.values)
                axes[2].set_title('Error Rate by Batch')
                axes[2].set_xlabel('Batch Index')
                axes[2].set_ylabel('Empty Prediction Rate')
                
                # xì¶• í‹± ì„¤ì • (ë„ˆë¬´ ë§ìœ¼ë©´ ê°„ê²© ì¡°ì •)
                if len(batch_indices) > 20:
                    step = len(batch_indices) // 10
                    axes[2].set_xticks(batch_indices[::step])
                    axes[2].set_xticklabels([f'{i}' for i in batch_error_rates.index[::step]])
            else:
                axes[2].text(0.5, 0.5, 'No batch data', ha='center', va='center', transform=axes[2].transAxes)
                axes[2].set_title('Error Rate by Batch')
            
            plt.tight_layout()
            
            if output_dir:
                plt.savefig(output_dir / 'error_patterns.png', dpi=300, bbox_inches='tight')
                print(f"âœ“ ì˜¤ë¥˜ íŒ¨í„´ ì°¨íŠ¸ ì €ì¥: {output_dir / 'error_patterns.png'}")
            else:
                plt.show()
            
            plt.close('all')
            
        except Exception as e:
            print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ì‹œê°í™”ë¥¼ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            plt.close('all')
    
    def find_best_worst_samples(self, n_samples: int = 10) -> Dict[str, List]:
        """ìµœê³ /ìµœì•… ìƒ˜í”Œ ì°¾ê¸°"""
        print(f"\n=== ìµœê³ /ìµœì•… ìƒ˜í”Œ ë¶„ì„ ===")
        
        # ê¸¸ì´ ê¸°ë°˜ ë¶„ì„ (ì°¸ì¡°ê°€ ì—†ìœ¼ë¯€ë¡œ)
        valid_samples = self.df[~self.df['is_empty_prediction']].copy()
        
        if len(valid_samples) == 0:
            print("ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
            return {'best': [], 'worst': []}
        
        # ì˜ˆì¸¡ ê¸¸ì´ë¡œ ì •ë ¬
        best_samples = valid_samples.nlargest(n_samples, 'prediction_length')
        worst_samples = valid_samples.nsmallest(n_samples, 'prediction_length')
        
        best_list = []
        for _, row in best_samples.iterrows():
            best_list.append({
                'sample_id': row['sample_id'],
                'prediction': row['prediction'],
                'length': row['prediction_length'],
                'image_path': row['image_path']
            })
        
        worst_list = []
        for _, row in worst_samples.iterrows():
            worst_list.append({
                'sample_id': row['sample_id'],
                'prediction': row['prediction'],
                'length': row['prediction_length'],
                'image_path': row['image_path']
            })
        
        print(f"ìµœê³  ìƒ˜í”Œ (ê¸¸ì´ ê¸°ì¤€): {len(best_list)}ê°œ")
        print(f"ìµœì•… ìƒ˜í”Œ (ê¸¸ì´ ê¸°ì¤€): {len(worst_list)}ê°œ")
        
        return {'best': best_list, 'worst': worst_list}
    
    def generate_report(self, output_dir: Optional[Path] = None) -> str:
        """ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\n=== ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ===")
        
        # ëª¨ë“  ë¶„ì„ ì‹¤í–‰
        metrics = self.compute_text_metrics()
        length_analysis = self.analyze_length_distribution()
        error_analysis = self.analyze_error_patterns()
        sample_analysis = self.find_best_worst_samples()
        
        # ë¦¬í¬íŠ¸ ì‘ì„±
        report = []
        report.append("# PanoLLaVA í‰ê°€ ê²°ê³¼ ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("=" * 50)
        report.append(f"íŒŒì¼: {self.csv_file}")
        report.append(f"ë¶„ì„ ì‹œê°„: {pd.Timestamp.now()}")
        report.append("")
        
        # ê¸°ë³¸ í†µê³„
        report.append("## 1. ê¸°ë³¸ í†µê³„")
        report.append(f"- ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(self.df)}")
        report.append(f"- ìŠ¤í…Œì´ì§€: {', '.join(self.df['stage'].unique())}")
        report.append(f"- ë¹ˆ ì˜ˆì¸¡ ìˆ˜: {error_analysis['empty_predictions']['count']} ({error_analysis['empty_predictions']['percentage']:.1f}%)")
        report.append(f"- ê°œë³„ ì²˜ë¦¬ ìˆ˜: {error_analysis['individual_processing']['count']} ({error_analysis['individual_processing']['percentage']:.1f}%)")
        report.append("")
        
        # í…ìŠ¤íŠ¸ ë©”íŠ¸ë¦­
        if metrics:
            report.append("## 2. í…ìŠ¤íŠ¸ í‰ê°€ ë©”íŠ¸ë¦­")
            
            # ì°¸ì¡° ê¸°ë°˜ ë©”íŠ¸ë¦­ (BLEU, ROUGE, METEOR)
            reference_metrics = ['bleu1', 'bleu2', 'bleu3', 'bleu4', 'rouge1', 'rouge2', 'rougeL', 'meteor']
            ref_metrics_found = any(metric in metrics for metric in reference_metrics)
            
            if ref_metrics_found:
                report.append("### ì°¸ì¡° ê¸°ë°˜ ë©”íŠ¸ë¦­")
                for metric in reference_metrics:
                    if metric in metrics:
                        report.append(f"- {metric.upper()}: {metrics[metric]:.4f}")
                report.append("")
            
            # CLIP Score
            if 'clip_score' in metrics:
                report.append("### ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„")
                report.append(f"- CLIP Score: {metrics['clip_score']:.4f} Â± {metrics.get('clip_score_std', 0):.4f}")
                report.append("")
            
            # í…ìŠ¤íŠ¸ í’ˆì§ˆ ë©”íŠ¸ë¦­
            quality_metrics = ['vocabulary_diversity', 'prediction_uniqueness', 'avg_sentence_length']
            quality_metrics_found = any(metric in metrics for metric in quality_metrics)
            
            if quality_metrics_found:
                report.append("### í…ìŠ¤íŠ¸ í’ˆì§ˆ ë©”íŠ¸ë¦­")
                if 'vocabulary_diversity' in metrics:
                    report.append(f"- ì–´íœ˜ ë‹¤ì–‘ì„±: {metrics['vocabulary_diversity']:.4f}")
                if 'prediction_uniqueness' in metrics:
                    report.append(f"- ì˜ˆì¸¡ ê³ ìœ ì„±: {metrics['prediction_uniqueness']:.4f}")
                if 'avg_sentence_length' in metrics:
                    report.append(f"- í‰ê·  ë¬¸ì¥ ê¸¸ì´: {metrics['avg_sentence_length']:.1f} ë‹¨ì–´")
                if 'total_unique_words' in metrics:
                    report.append(f"- ì´ ê³ ìœ  ë‹¨ì–´ ìˆ˜: {metrics['total_unique_words']}")
                report.append("")
            
            # ê¸°íƒ€ í†µê³„
            other_metrics = [k for k in metrics.keys() if k not in reference_metrics + quality_metrics + ['clip_score', 'clip_score_std']]
            if other_metrics:
                report.append("### ê¸°íƒ€ í†µê³„")
                for metric in other_metrics:
                    if isinstance(metrics[metric], (int, float)):
                        if 'ratio' in metric or 'length' in metric:
                            report.append(f"- {metric}: {metrics[metric]:.4f}")
                        else:
                            report.append(f"- {metric}: {metrics[metric]}")
                report.append("")
        
        # ê¸¸ì´ ë¶„ì„
        report.append("## 3. í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„")
        pred_stats = length_analysis['prediction_lengths']
        report.append(f"- ì˜ˆì¸¡ í…ìŠ¤íŠ¸ ê¸¸ì´: {pred_stats['mean']:.1f} Â± {pred_stats['std']:.1f} ë‹¨ì–´")
        report.append(f"  - ìµœì†Œ/ìµœëŒ€: {pred_stats['min']}/{pred_stats['max']} ë‹¨ì–´")
        report.append(f"  - ì¤‘ê°„ê°’: {pred_stats['median']:.1f} ë‹¨ì–´")
        report.append("")
        
        # ìƒ˜í”Œ ì˜ˆì‹œ
        report.append("## 4. ìƒ˜í”Œ ì˜ˆì‹œ")
        report.append("### ê°€ì¥ ê¸´ ì˜ˆì¸¡ (ìƒìœ„ 3ê°œ)")
        for i, sample in enumerate(sample_analysis['best'][:3]):
            report.append(f"**ìƒ˜í”Œ {sample['sample_id']} ({sample['length']} ë‹¨ì–´):**")
            report.append(f"```")
            report.append(f"{sample['prediction']}")
            report.append(f"```")
            report.append("")
        
        # ê¶Œì¥ì‚¬í•­
        report.append("## 5. ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­")
        
        if error_analysis['empty_predictions']['percentage'] > 10:
            report.append("âš ï¸ **ë¹ˆ ì˜ˆì¸¡ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤** (>10%)")
            report.append("  - ëª¨ë¸ inference íŒŒì´í”„ë¼ì¸ ì ê²€ í•„ìš”")
            report.append("  - generate íŒŒë¼ë¯¸í„° ì¡°ì • ê³ ë ¤")
            report.append("")
        
        if error_analysis['individual_processing']['percentage'] > 5:
            report.append("âš ï¸ **ê°œë³„ ì²˜ë¦¬ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤** (>5%)")
            report.append("  - ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥ì„±")
            report.append("  - ë°°ì¹˜ í¬ê¸° ê°ì†Œ ê³ ë ¤")
            report.append("")
        
        if pred_stats['mean'] < 10:
            report.append("âš ï¸ **ì˜ˆì¸¡ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤** (<10 ë‹¨ì–´)")
            report.append("  - max_new_tokens ì¦ê°€ ê³ ë ¤")
            report.append("  - temperature ì¡°ì • ê³ ë ¤")
            report.append("")
        
        report_text = "\n".join(report)
        
        # íŒŒì¼ ì €ì¥
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True)
            
            report_file = output_dir / 'analysis_report.md'
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_text)
            
            # JSONìœ¼ë¡œë„ ì €ì¥
            json_data = {
                'basic_stats': {
                    'total_samples': int(len(self.df)),
                    'stages': list(self.df['stage'].unique()),
                    'empty_predictions': {
                        'count': int(error_analysis['empty_predictions']['count']),
                        'ratio': float(error_analysis['empty_predictions']['ratio']),
                        'percentage': float(error_analysis['empty_predictions']['percentage'])
                    },
                    'individual_processing': {
                        'count': int(error_analysis['individual_processing']['count']),
                        'ratio': float(error_analysis['individual_processing']['ratio']),
                        'percentage': float(error_analysis['individual_processing']['percentage'])
                    }
                },
                'metrics': {k: float(v) if isinstance(v, (np.floating, np.integer)) else v for k, v in metrics.items()},
                'length_analysis': self._convert_numpy_types(length_analysis),
                'error_analysis': self._convert_numpy_types(error_analysis),
                'sample_analysis': sample_analysis
            }
            
            json_file = output_dir / 'analysis_data.json'
            try:
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(json_data, f, indent=2, ensure_ascii=False, default=self._json_serializer)
                print(f"âœ“ ë°ì´í„° ì €ì¥: {json_file}")
            except Exception as e:
                print(f"âš ï¸ JSON ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                # ëŒ€ì•ˆìœ¼ë¡œ str() ë³€í™˜í•´ì„œ ì €ì¥
                try:
                    json_data_str = self._deep_convert_to_json_compatible(json_data)
                    with open(json_file, 'w', encoding='utf-8') as f:
                        json.dump(json_data_str, f, indent=2, ensure_ascii=False)
                    print(f"âœ“ ë°ì´í„° ì €ì¥ (íƒ€ì… ë³€í™˜ë¨): {json_file}")
                except Exception as e2:
                    print(f"âœ— JSON ì €ì¥ ì‹¤íŒ¨: {e2}")
            
            print(f"âœ“ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
        
        return report_text


def main():
    parser = argparse.ArgumentParser(description='PanoLLaVA í‰ê°€ ê²°ê³¼ ë¶„ì„ ë„êµ¬')
    
    parser.add_argument('--csv-file', required=True, help='ë¶„ì„í•  CSV íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output-dir', help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: csv íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬)')
    parser.add_argument('--save-plots', action='store_true', help='ì°¨íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥')
    parser.add_argument('--no-display', action='store_true', help='ì°¨íŠ¸ë¥¼ í™”ë©´ì— í‘œì‹œí•˜ì§€ ì•ŠìŒ')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = Path(args.csv_file).parent / 'analysis_output'
    
    if args.save_plots:
        output_dir.mkdir(exist_ok=True)
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = EvaluationAnalyzer(args.csv_file)
    
    print(f"\n{'='*60}")
    print(f"PanoLLaVA í‰ê°€ ê²°ê³¼ ë¶„ì„")
    print(f"{'='*60}")
    
    # ì‹œê°í™” ìƒì„±
    if args.save_plots or not args.no_display:
        analyzer.create_visualizations(output_dir if args.save_plots else None)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = analyzer.generate_report(output_dir if args.save_plots else None)
    
    if not args.save_plots:
        print(f"\n{report}")
    
    print(f"\n{'='*60}")
    print(f"ë¶„ì„ ì™„ë£Œ!")
    if args.save_plots:
        print(f"ê²°ê³¼ê°€ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"{'='*60}")


if __name__ == '__main__':
    main()
