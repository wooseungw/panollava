#!/usr/bin/env python3
# coding: utf-8
"""
PanoramaVLM ëª¨ë¸ ì„¤ì • ê´€ë¦¬ì
===========================

ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥/ë¡œë”©í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
í›ˆë ¨ ì‹œ ì„¤ì •ì„ ì €ì¥í•˜ê³ , ì¶”ë¡ /í‰ê°€ ì‹œ ì¼ê´€ëœ ì„¤ì •ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
"""

import json
import os
from pathlib import Path
from typing import Dict, Any, Optional, Union
from dataclasses import dataclass, asdict, field
from datetime import datetime
import warnings


@dataclass
class ModelConfig:
    """PanoramaVLM ëª¨ë¸ ì„¤ì •"""
    
    # ëª¨ë¸ ì•„í‚¤í…ì²˜
    vision_name: str = "google/siglip-base-patch16-224"
    language_model_name: str = "Qwen/Qwen2.5-0.5B-Instruct"
    resampler_type: str = "mlp"
    latent_dimension: int = 768
    image_size: Optional[tuple] = None
    
    # ë¦¬ìƒ˜í”ŒëŸ¬ ì„¸ë¶€ ì„¤ì •
    resampler_depth: int = 2
    resampler_hidden_dim: Optional[int] = None
    resampler_use_ln: bool = True
    resampler_num_latents: int = 32
    resampler_heads: int = 8
    resampler_dropout: float = 0.1
    
    # íŒŒë…¸ë¼ë§ˆ íŠ¹í™” ì„¤ì •
    resampler_enable_cross_view: bool = False
    resampler_num_views: int = 8
    
    # VICReg ê´€ë ¨ ì„¤ì •
    vicreg_loss_weight: float = 1.0
    vicreg_overlap_ratio: float = 0.5
    use_vicreg_norm: bool = True  # VICReg ê²½ë¡œì—ì„œ LayerNorm ì‚¬ìš© ì—¬ë¶€ (False = ì› ì² í•™ ì¤€ìˆ˜)
    
    # VICReg ì„¤ì • - ê°„ë‹¨í•œ x,y ì…ë ¥ ë°©ì‹
    vicreg_similarity_weight: float = 25.0
    vicreg_variance_weight: float = 25.0  
    vicreg_covariance_weight: float = 1.0
    
    # í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì„¤ì •
    max_text_length: int = 512
    
    # LoRA ì„¤ì • (ì˜µì…˜)
    use_lora: bool = False
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    lora_target_modules: Optional[list] = None
    
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì²˜ë¦¬"""
        # LoRA íƒ€ê²Ÿ ëª¨ë“ˆ ê¸°ë³¸ê°’ ì„¤ì •
        if self.use_lora and self.lora_target_modules is None:
            self.lora_target_modules = [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'ModelConfig':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±"""
        # ì•Œë ¤ì§„ í•„ë“œë§Œ ì¶”ì¶œ
        try:
            valid_fields = {f.name for f in cls.__dataclass_fields__.values()}
        except AttributeError:
            # fallback: dataclass fieldsì˜ í‚¤ë§Œ ì‚¬ìš©
            valid_fields = set(cls.__dataclass_fields__.keys())
        filtered_dict = {k: v for k, v in config_dict.items() if k in valid_fields}
        
        # tuple íƒ€ì… ë³€í™˜
        if 'image_size' in filtered_dict and isinstance(filtered_dict['image_size'], list):
            filtered_dict['image_size'] = tuple(filtered_dict['image_size'])
        
        return cls(**filtered_dict)
    
    def update(self, **kwargs) -> 'ModelConfig':
        """ì„¤ì • ì—…ë°ì´íŠ¸ (ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜)"""
        config_dict = self.to_dict()
        config_dict.update(kwargs)
        return self.from_dict(config_dict)
    
    def save(self, file_path: Union[str, Path]) -> None:
        """ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        ConfigManager.save_config(self, file_path)
    
    @classmethod
    def load(cls, file_path: Union[str, Path]) -> 'ModelConfig':
        """JSON íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë”©"""
        return ConfigManager.load_config(file_path)
    
    def get_model_kwargs(self) -> Dict[str, Any]:
        """PanoramaVLM ëª¨ë¸ ìƒì„±ì— í•„ìš”í•œ kwargs ë°˜í™˜"""
        return {
            'vision_name': self.vision_name,
            'language_model_name': self.language_model_name,
            'resampler_type': self.resampler_type,
            'latent_dimension': self.latent_dimension,
            'resampler_depth': self.resampler_depth,
            'resampler_hidden_dim': self.resampler_hidden_dim,
            'resampler_use_ln': self.resampler_use_ln,
            'resampler_num_latents': self.resampler_num_latents,
            'resampler_heads': self.resampler_heads,
            'resampler_dropout': self.resampler_dropout,
            'resampler_enable_cross_view': self.resampler_enable_cross_view,
            'resampler_num_views': self.resampler_num_views,
            'vicreg_loss_weight': self.vicreg_loss_weight,
            'vicreg_overlap_ratio': self.vicreg_overlap_ratio,
            'use_vicreg_norm': self.use_vicreg_norm,
            'max_text_length': self.max_text_length,
            # VICReg íŒŒë¼ë¯¸í„°ë“¤ - ê°„ë‹¨í•œ x,y ì…ë ¥ ë°©ì‹
            'vicreg_similarity_weight': self.vicreg_similarity_weight,
            'vicreg_variance_weight': self.vicreg_variance_weight,
            'vicreg_covariance_weight': self.vicreg_covariance_weight,
        }
    
    
    def get_lora_kwargs(self) -> Dict[str, Any]:
        """LoRA ì„¤ì •ì— í•„ìš”í•œ kwargs ë°˜í™˜"""
        return {
            'lora_r': self.lora_r,
            'lora_alpha': self.lora_alpha,
            'lora_dropout': self.lora_dropout,
            'target_modules': self.lora_target_modules,
        }
    
    def validate(self) -> bool:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            # í•„ìˆ˜ ë¬¸ìì—´ í•„ë“œ í™•ì¸
            assert self.vision_name.strip(), "vision_nameì€ ë¹„ì–´ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            assert self.language_model_name.strip(), "language_model_nameì€ ë¹„ì–´ìˆì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            assert self.resampler_type in ["mlp", "perceiver"], f"ì§€ì›í•˜ì§€ ì•ŠëŠ” resampler_type: {self.resampler_type}"
            
            # ìˆ«ì ë²”ìœ„ í™•ì¸
            assert self.latent_dimension > 0, "latent_dimensionì€ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
            assert self.vicreg_loss_weight >= 0, "vicreg_loss_weightëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤"
            assert 0 <= self.vicreg_overlap_ratio <= 1, "vicreg_overlap_ratioëŠ” 0-1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤"
            assert self.max_text_length > 0, "max_text_lengthëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
            assert self.resampler_depth > 0, "resampler_depthëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
            assert self.resampler_num_latents > 0, "resampler_num_latentsëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
            assert self.resampler_heads > 0, "resampler_headsëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
            assert 0 <= self.resampler_dropout <= 1, "resampler_dropoutì€ 0-1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤"
            
            
            # LoRA ì„¤ì • í™•ì¸
            if self.use_lora:
                assert self.lora_r > 0, "lora_rì€ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
                assert self.lora_alpha > 0, "lora_alphaëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
                assert 0 <= self.lora_dropout <= 1, "lora_dropoutì€ 0-1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤"
            
            return True
            
        except AssertionError as e:
            warnings.warn(f"ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return False
    
    def __str__(self) -> str:
        """ë¬¸ìì—´ í‘œí˜„"""
        return f"ModelConfig(vision={self.vision_name}, language={self.language_model_name}, dim={self.latent_dimension})"

class Config:
    """Training configuration management"""
    
    STAGE_DEFAULTS = {
        "vision": {
            "epochs": 1, 
            "lr": 5e-6, 
            "batch_size": 16,   # 32ì—ì„œ 8ë¡œ ê°ì†Œ
            "vicreg_loss_weight": 1.0, 
            "max_text_length": 32
        },
        "resampler": {
            "epochs": 1, 
            "lr": 2e-6, 
            "batch_size": 8,   # 16ì—ì„œ 4ë¡œ ê°ì†Œ
            "vicreg_loss_weight": 0.0, 
            "max_text_length": 64
        },
        "finetune": {
            "epochs": 1, 
            "lr": 2e-6, 
            "batch_size": 8,   # 16ì—ì„œ 4ë¡œ ê°ì†Œ
            "vicreg_loss_weight": 0.0, 
            "max_text_length": 128
        }
    }


class ConfigManager:
    """ì„¤ì • íŒŒì¼ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°"""
    
    DEFAULT_CONFIG_NAME = "model_config.json"
    
    @staticmethod
    def save_config(config: ModelConfig, file_path: Union[str, Path]) -> None:
        """ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        file_path = Path(file_path)
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬
        if not config.validate():
            warnings.warn("ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬ì— ì‹¤íŒ¨í–ˆì§€ë§Œ ì €ì¥ì„ ê³„ì†í•©ë‹ˆë‹¤")
        
        config_dict = config.to_dict()
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(config_dict, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… ì„¤ì • ì €ì¥ ì™„ë£Œ: {file_path}")
            
        except Exception as e:
            raise RuntimeError(f"ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
    
    @staticmethod
    def load_config(file_path: Union[str, Path]) -> ModelConfig:
        """JSON íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë”©"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
            
            # JSON config êµ¬ì¡°ë¥¼ ModelConfig í˜•íƒœë¡œ ë³€í™˜
            flat_config = ConfigManager._flatten_json_config(config_dict)
            config = ModelConfig.from_dict(flat_config)
            
            # ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬
            if not config.validate():
                warnings.warn("ë¡œë“œëœ ì„¤ì •ì´ ìœ íš¨ì„± ê²€ì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            print(f"âœ… ì„¤ì • ë¡œë”© ì™„ë£Œ: {file_path}")
            return config
            
        except json.JSONDecodeError as e:
            raise RuntimeError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        except Exception as e:
            raise RuntimeError(f"ì„¤ì • ë¡œë”© ì‹¤íŒ¨: {e}")
    
    @staticmethod
    def _flatten_json_config(config_dict: Dict[str, Any]) -> Dict[str, Any]:
        """JSON configì˜ nested êµ¬ì¡°ë¥¼ ModelConfigì˜ flat êµ¬ì¡°ë¡œ ë³€í™˜"""
        flat_config = {}
        
        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì • (ì‹ ê·œ í‚¤ ìš°ì„ , êµ¬í‚¤ë„ ë³‘í–‰ ì§€ì›)
        if 'models' in config_dict:
            models = config_dict['models']
            # ì‹ ê·œ í‘œì¤€í™”ëœ í‚¤
            lang_name = models.get('language_model_name', models.get('lm_model', 'Qwen/Qwen2.5-0.5B-Instruct'))
            resampler_type = models.get('resampler_type', models.get('resampler', 'mlp'))
            vision_name = models.get('vision_model_name', models.get('vision_name'))

            flat_config.update({
                'vision_name': vision_name,
                'language_model_name': lang_name,
                'resampler_type': resampler_type,
                'latent_dimension': models.get('latent_dimension', 768),
                'resampler_depth': models.get('resampler_depth', 2),
                'resampler_hidden_dim': models.get('resampler_hidden_dim', None),
                'resampler_use_ln': models.get('resampler_use_ln', True),
                'resampler_num_latents': models.get('resampler_num_latents', 32),
                'resampler_heads': models.get('resampler_heads', 8),
                'resampler_dropout': models.get('resampler_dropout', 0.1),
                'resampler_enable_cross_view': models.get('resampler_enable_cross_view', False),
                'resampler_num_views': models.get('resampler_num_views', 8)
            })
        
        # ë°ì´í„° ì„¤ì •
        if 'data' in config_dict:
            data = config_dict['data']
            # max_text_length may be "auto"; only forward numeric to ModelConfig
            mtl_val = data.get('max_text_length', 512)
            try:
                if isinstance(mtl_val, (int, float)) and mtl_val > 0:
                    flat_config['max_text_length'] = int(mtl_val)
            except Exception:
                pass
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì •ì—ì„œ vicreg_overlap_ratioì™€ image_size ì¶”ì¶œ
        if 'image_processing' in config_dict:
            img_proc = config_dict['image_processing']
            flat_config.update({
                'vicreg_overlap_ratio': img_proc.get('overlap_ratio', 0.5),
                'image_size': img_proc.get('image_size'),
                'crop_strategy': img_proc.get('crop_strategy'),
            })
        
        # í›ˆë ¨ ì„¤ì • (íŠ¹íˆ VICReg Local)
        if 'training' in config_dict:
            training = config_dict['training']
            
            # Vision stage ì„¤ì •ì—ì„œ VICReg Local íŒŒë¼ë¯¸í„°ë“¤ ì¶”ì¶œ
            if 'vision' in training:
                vision = training['vision']
                flat_config.update({
                    'vicreg_loss_weight': vision.get('vicreg_loss_weight', 1.0),
                    'vicreg_similarity_weight': vision.get('vicreg_similarity_weight', 25.0),
                    'vicreg_variance_weight': vision.get('vicreg_variance_weight', 25.0),
                    'vicreg_covariance_weight': vision.get('vicreg_covariance_weight', 1.0),
                })
        
        # LoRA ì„¤ì •
        if 'lora' in config_dict:
            lora = config_dict['lora']
            flat_config.update({
                'use_lora': lora.get('use_lora', False),
                'lora_r': lora.get('rank', 16),
                'lora_alpha': lora.get('alpha', 32),
                'lora_dropout': lora.get('dropout', 0.1),
                'lora_target_modules': lora.get('target_modules', None)
            })
        
        return flat_config
    
    @staticmethod
    def auto_detect_config(checkpoint_path: Union[str, Path]) -> Optional[ModelConfig]:
        """ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œì—ì„œ ì„¤ì • íŒŒì¼ ìë™ ê°ì§€"""
        checkpoint_path = Path(checkpoint_path)
        
        # ì²´í¬í¬ì¸íŠ¸ê°€ íŒŒì¼ì¸ ê²½ìš° ë””ë ‰í† ë¦¬ ì¶”ì¶œ
        if checkpoint_path.is_file():
            search_dir = checkpoint_path.parent
        else:
            search_dir = checkpoint_path
        
        # ì„¤ì • íŒŒì¼ í›„ë³´ë“¤
        config_candidates = [
            # 1. ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
            search_dir / ConfigManager.DEFAULT_CONFIG_NAME,
            search_dir / "config.json",
            search_dir / "model_config.json", 
            search_dir / "panovlm_config.json",
            # 2. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
            Path.cwd() / "config.json",
            Path.cwd() / ConfigManager.DEFAULT_CONFIG_NAME,
            # 3. í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •ëœ ê²½ë¡œ
        ]
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ config ê²½ë¡œ ì¶”ê°€
        env_config = os.environ.get("PANOVLM_CONFIG")
        if env_config:
            config_candidates.append(Path(env_config))
        
        for config_path in config_candidates:
            if config_path.exists():
                try:
                    print(f"ğŸ” ì„¤ì • íŒŒì¼ ë°œê²¬: {config_path}")
                    return ConfigManager.load_config(config_path)
                except Exception as e:
                    warnings.warn(f"ì„¤ì • íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ ({config_path}): {e}")
        
        # ë””ë²„ê¹…: ì°¾ì€ í›„ë³´ë“¤ê³¼ í˜„ì¬ ë””ë ‰í† ë¦¬ ì •ë³´ ì¶œë ¥
        print(f"ğŸ” ì„¤ì • íŒŒì¼ ê°ì§€ ì‹¤íŒ¨")
        print(f"   - ê²€ìƒ‰ ê²½ë¡œ: {search_dir}")
        print(f"   - í˜„ì¬ ë””ë ‰í† ë¦¬: {Path.cwd()}")
        print(f"   - ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: {checkpoint_path}")
        
        return None
    
    @staticmethod
    def create_default_config(**overrides) -> ModelConfig:
        """ê¸°ë³¸ ì„¤ì • ìƒì„± (ì˜¤ë²„ë¼ì´ë“œ ì ìš© ê°€ëŠ¥)"""
        config = ModelConfig()
        if overrides:
            config = config.update(**overrides)
        return config
    
    @staticmethod
    def migrate_old_config(old_config_dict: Dict[str, Any]) -> ModelConfig:
        """êµ¬ë²„ì „ ì„¤ì •ì„ ìƒˆ í˜•ì‹ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        # êµ¬ë²„ì „ í•„ë“œëª… ë§¤í•‘
        field_mapping = {
            'vision_model': 'vision_name',
            'language_model': 'language_model_name',
            'resampler': 'resampler_type',
            'dim': 'latent_dimension',
            'vicreg_weight': 'vicreg_loss_weight',
            'vicreg_overlap': 'vicreg_overlap_ratio',
            'max_length': 'max_text_length',
        }
        
        # í•„ë“œëª… ë³€í™˜
        migrated_dict = {}
        for old_key, value in old_config_dict.items():
            new_key = field_mapping.get(old_key, old_key)
            migrated_dict[new_key] = value
        
        return ModelConfig.from_dict(migrated_dict)
    


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_config(**kwargs) -> ModelConfig:
    """í¸ì˜ í•¨ìˆ˜: ì„¤ì • ìƒì„±"""
    return ConfigManager.create_default_config(**kwargs)

def save_config(config: ModelConfig, file_path: Union[str, Path]) -> None:
    """í¸ì˜ í•¨ìˆ˜: ì„¤ì • ì €ì¥"""
    ConfigManager.save_config(config, file_path)

def load_config(file_path: Union[str, Path]) -> ModelConfig:
    """í¸ì˜ í•¨ìˆ˜: ì„¤ì • ë¡œë”©"""
    return ConfigManager.load_config(file_path)

def auto_detect_config(checkpoint_path: Union[str, Path]) -> Optional[ModelConfig]:
    """í¸ì˜ í•¨ìˆ˜: ì„¤ì • ìë™ ê°ì§€"""
    return ConfigManager.auto_detect_config(checkpoint_path)
