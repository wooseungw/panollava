import asyncio
import aiohttp
import pandas as pd
import os
import argparse
from pathlib import Path
from tqdm.asyncio import tqdm

class RateLimiter:
    """
    ì´ˆë‹¹ ìš”ì²­ ìˆ˜(requests_per_second)ë¥¼ ì œí•œí•˜ëŠ” ê°„ë‹¨í•œ ë ˆì´íŠ¸ ë¦¬ë¯¸í„°
    """
    def __init__(self, requests_per_second: float):
        self._interval = 1.0 / requests_per_second
        self._lock = asyncio.Lock()
        self._last_call = None

    async def wait(self):
        async with self._lock:
            now = asyncio.get_event_loop().time()
            if self._last_call is not None:
                elapsed = now - self._last_call
                wait_time = self._interval - elapsed
                if wait_time > 0:
                    await asyncio.sleep(wait_time)
            self._last_call = asyncio.get_event_loop().time()

async def download_image(session, url, cache_path, semaphore, rate_limiter, max_retries=3):
    """ë¹„ë™ê¸° ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ. ì„±ê³µ ì‹œ ë¡œì»¬ ê²½ë¡œ, ì‹¤íŒ¨ ì‹œ None ë°˜í™˜."""
    async with semaphore:
        if os.path.exists(cache_path):
            return cache_path
        
        await rate_limiter.wait()
        
        for attempt in range(max_retries):
            try:
                async with session.get(url, timeout=30) as resp:
                    if resp.status == 429:
                        retry_after = int(resp.headers.get('Retry-After', 60))
                        print(f"[RATE_LIMIT] {url} - ëŒ€ê¸°: {retry_after}ì´ˆ")
                        await asyncio.sleep(retry_after)
                        continue  # ì¬ì‹œë„
                    
                    if resp.status == 200:
                        # Content-Type í™•ì¸
                        content_type = resp.headers.get('content-type', '').lower()
                        if not any(img_type in content_type for img_type in ['image/', 'application/octet-stream']):
                            print(f"[WARNING] {url} - ì´ë¯¸ì§€ê°€ ì•„ë‹Œ ì½˜í…ì¸ : {content_type}")
                            return None
                        
                        data = await resp.read()
                        if len(data) == 0:
                            print(f"[WARNING] {url} - ë¹ˆ íŒŒì¼")
                            return None
                            
                        # ë””ë ‰í† ë¦¬ ìƒì„±
                        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
                        
                        with open(cache_path, 'wb') as f:
                            f.write(data)
                        return cache_path
                    else:
                        print(f"[HTTP_ERROR] {url} - Status: {resp.status}")
                        if attempt == max_retries - 1:  # ë§ˆì§€ë§‰ ì‹œë„
                            return None
                        await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ì  ë°±ì˜¤í”„
                        
            except asyncio.TimeoutError:
                print(f"[TIMEOUT] {url} - ì‹œë„ {attempt + 1}/{max_retries}")
                if attempt == max_retries - 1:
                    return None
                await asyncio.sleep(2 ** attempt)
                
            except Exception as e:
                print(f"[ERROR] {url} - ì‹œë„ {attempt + 1}/{max_retries}: {e}")
                if attempt == max_retries - 1:
                    return None
                await asyncio.sleep(2 ** attempt)
                
        return None

async def batch_download_unique(urls, cache_dir, max_concurrent, rps):
    """
    ì¤‘ë³µ URL ì œê±° í›„ í•œ ë²ˆì”©ë§Œ ë‹¤ìš´ë¡œë“œ,
    ë°˜í™˜ê°’: { url: local_path or None, ... }
    """
    os.makedirs(cache_dir, exist_ok=True)
    sem = asyncio.Semaphore(max_concurrent)
    limiter = RateLimiter(rps)
    
    # SSL ë° íƒ€ì„ì•„ì›ƒ ì„¤ì •
    connector = aiohttp.TCPConnector(
        ssl=False,  # SSL ê²€ì¦ ë¹„í™œì„±í™” (í•„ìš”ì‹œ)
        limit=100,
        limit_per_host=max_concurrent
    )
    
    timeout = aiohttp.ClientTimeout(total=60, connect=30)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    async with aiohttp.ClientSession(
        connector=connector, 
        timeout=timeout,
        headers=headers
    ) as session:
        tasks = []
        valid_urls = []
        
        for url in urls:
            if not url or pd.isna(url) or not isinstance(url, str):  # URL ìœ íš¨ì„± ê²€ì‚¬ ê°•í™”
                continue
                
            url = url.strip()  # ê³µë°± ì œê±°
            if not url.startswith(('http://', 'https://')):  # í”„ë¡œí† ì½œ í™•ì¸
                continue
                
            fname = os.path.basename(url.split('?')[0])  # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
            if not fname or '.' not in fname:
                # URLì—ì„œ í•´ì‹œê°’ì„ ì´ìš©í•œ ê³ ìœ  íŒŒì¼ëª… ìƒì„±
                fname = f"image_{abs(hash(url)) % 1000000}.jpg"  # ê¸°ë³¸ íŒŒì¼ëª…
                
            path = os.path.join(cache_dir, fname)
            tasks.append(download_image(session, url, path, sem, limiter))
            valid_urls.append(url)
            
        if not tasks:
            print("[WARNING] ë‹¤ìš´ë¡œë“œí•  ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return {url: None for url in urls}  # ëª¨ë“  URLì— ëŒ€í•´ None ë°˜í™˜
            
        try:
            results = await tqdm.gather(*tasks, desc="Downloading unique images")
        except Exception as e:
            print(f"[ERROR] ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            results = [None] * len(tasks)  # ëª¨ë“  ê²°ê³¼ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
    
    # ì›ë³¸ URLsì™€ ê²°ê³¼ ë§¤í•‘ (ìœ íš¨í•˜ì§€ ì•Šì€ URLë“¤ë„ í¬í•¨)
    url_to_result = {}
    valid_idx = 0
    
    for url in urls:
        if (url and not pd.isna(url) and isinstance(url, str) and 
            url.strip() and url.strip().startswith(('http://', 'https://'))):
            if valid_idx < len(results):
                url_to_result[url] = results[valid_idx]
                valid_idx += 1
            else:
                url_to_result[url] = None
        else:
            url_to_result[url] = None
    
    return url_to_result

def process_split(
    input_csv: str,
    output_csv: str,
    image_dir: str,
    url_col: str,
    max_concurrent: int,
    rps: float,
    out_dir: str,
    split: str
):
    # 1) CSV ë¡œë“œ
    print(f"ğŸ“– CSV íŒŒì¼ ë¡œë“œ ì¤‘: {input_csv}")
    try:
        df = pd.read_csv(input_csv)
        print(f"   ì´ {len(df)} í–‰ ë°œê²¬")
    except Exception as e:
        print(f"âŒ ERROR: CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    if len(df) == 0:
        print("âš ï¸ WARNING: ë¹ˆ CSV íŒŒì¼ì…ë‹ˆë‹¤.")
        # ë¹ˆ êµ¬ì¡° ìœ ì§€í•˜ë©° ì¶œë ¥ íŒŒì¼ ìƒì„±
        try:
            os.makedirs(os.path.dirname(output_csv), exist_ok=True)
            df.to_csv(output_csv, index=False)
            print(f"âœ… ë¹ˆ CSV íŒŒì¼ ë³µì‚¬ë¨: {output_csv}")
        except Exception as e:
            print(f"âŒ ERROR: ë¹ˆ CSV íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # URL ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
    if url_col not in df.columns:
        print(f"âŒ ERROR: '{url_col}' ì»¬ëŸ¼ì´ CSVì— ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
        return
    
    # ìœ íš¨í•œ URL í•„í„°ë§
    valid_urls = df[url_col].dropna()
    print(f"   ìœ íš¨í•œ URL: {len(valid_urls)} ê°œ")

    # 2) ìœ ë‹ˆí¬ URL ë‹¤ìš´ë¡œë“œ
    unique_urls = valid_urls.unique().tolist()
    # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ì˜ëª»ëœ URL ì œê±°
    unique_urls = [url for url in unique_urls if url and isinstance(url, str) and url.strip()]
    print(f"   ì¤‘ë³µ ì œê±° í›„: {len(unique_urls)} ê°œ ë‹¤ìš´ë¡œë“œ ì˜ˆì •")
    
    if not unique_urls:
        print("âš ï¸ WARNING: ë‹¤ìš´ë¡œë“œí•  ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤.")
        # ë¹ˆ CSVë¼ë„ ìƒì„±í•˜ì—¬ êµ¬ì¡° ìœ ì§€
        print("ğŸ“ ë¹ˆ CSV íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
        try:
            os.makedirs(os.path.dirname(output_csv), exist_ok=True)
            df_empty = df.iloc[:0].copy()  # êµ¬ì¡°ë§Œ ìœ ì§€í•˜ê³  ë¹ˆ ë°ì´í„°í”„ë ˆì„
            df_empty.to_csv(output_csv, index=False)
            print(f"âœ… ë¹ˆ CSV íŒŒì¼ ìƒì„±ë¨: {output_csv}")
        except Exception as e:
            print(f"âŒ ERROR: ë¹ˆ CSV íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    url2path = asyncio.run(batch_download_unique(
        unique_urls,
        cache_dir=image_dir,
        max_concurrent=max_concurrent,
        rps=rps
    ))

    # 3) ë§¤í•‘ & í•„í„°ë§
    df['local_path'] = df[url_col].map(url2path)
    success_count = df['local_path'].notna().sum()
    print(f"âœ… ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ: {success_count}/{len(df)} ê°œ")
    
    # ë‹¤ìš´ë¡œë“œ ì„±ê³µë¥ ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê²½ê³ 
    success_rate = success_count / len(df) if len(df) > 0 else 0
    if success_rate < 0.1:  # 10% ë¯¸ë§Œ
        print(f"âš ï¸ WARNING: ë‹¤ìš´ë¡œë“œ ì„±ê³µë¥ ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ ({success_rate:.1%})")
    
    df_filtered = df[df['local_path'].notna()].copy()
    
    if len(df_filtered) == 0:
        print("âŒ ERROR: ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨í•´ë„ ì›ë³¸ êµ¬ì¡°ë¥¼ ìœ ì§€í•œ ë¹ˆ CSV ìƒì„±
        print("ğŸ“ ì›ë³¸ êµ¬ì¡°ë¥¼ ìœ ì§€í•œ ë¹ˆ CSV íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
        try:
            os.makedirs(os.path.dirname(output_csv), exist_ok=True)
            df_empty = df.iloc[:0].copy()  # êµ¬ì¡°ë§Œ ìœ ì§€
            df_empty.to_csv(output_csv, index=False)
            print(f"âœ… ë¹ˆ CSV íŒŒì¼ ìƒì„±ë¨: {output_csv}")
        except Exception as e:
            print(f"âŒ ERROR: ë¹ˆ CSV íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        return

    # 4) URL ì»¬ëŸ¼ì„ ìƒëŒ€ê²½ë¡œë¡œ í¬ë§·
    def format_path(p: str):
        fname = os.path.basename(p)
        # data/quic360/train/images/img.jpg ì™€ ê°™ì´ ê³ ì •ëœ ìƒëŒ€ê²½ë¡œ ë°˜í™˜
        return os.path.join(out_dir, split, "images", fname)

    df_filtered[url_col] = df_filtered['local_path'].apply(format_path)
    df_filtered.drop(columns=['local_path'], inplace=True)

    # 5) ê²°ê³¼ ì €ì¥
    try:
        os.makedirs(os.path.dirname(output_csv), exist_ok=True)
        df_filtered.to_csv(output_csv, index=False)
        print(f"âœ… [{Path(input_csv).name}] â†’ {output_csv} ({len(df_filtered)} rows)")
    except Exception as e:
        print(f"âŒ ERROR: CSV íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        print(f"   ì¶œë ¥ ê²½ë¡œ: {output_csv}")
        print(f"   ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(os.path.dirname(output_csv))}")
        
        # ëŒ€ì•ˆìœ¼ë¡œ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥ ì‹œë„
        fallback_path = f"{split}_output.csv"
        try:
            df_filtered.to_csv(fallback_path, index=False)
            print(f"âœ… ëŒ€ì•ˆ ê²½ë¡œì— ì €ì¥ë¨: {fallback_path}")
        except Exception as e2:
            print(f"âŒ ERROR: ëŒ€ì•ˆ ì €ì¥ë„ ì‹¤íŒ¨: {e2}")
            raise

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="data/raw/QuIC360/*.csv â†’ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ + CSV ê°±ì‹ "
    )
    parser.add_argument("--splits", nargs="*",
                        help="ì²˜ë¦¬í•  split ì´ë¦„(ì˜ˆ: train test). ìƒëµ ì‹œ raw_dir/*.csv ì „ë¶€")
    parser.add_argument("--raw_dir", default="data/raw/QuIC360",
                        help="ì›ë³¸ CSV ë””ë ‰í† ë¦¬ (default: data/raw/QuIC360)")
    parser.add_argument("--out_dir", default="data/quic360",
                        help="ì¶œë ¥ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ (default: data/quic360)")
    parser.add_argument("--url_col", default="url", help="URL ì»¬ëŸ¼ëª…")
    parser.add_argument("--max_concurrent", type=int, default=8, help="ë™ì‹œ ë‹¤ìš´ë¡œë“œ ìˆ˜")
    parser.add_argument("--requests_per_second", type=float, default=0.7, help="ì´ˆë‹¹ ìš”ì²­ ìˆ˜ ì œí•œ")
    args = parser.parse_args()

    raw_dir = Path(args.raw_dir)
    out_dir = args.out_dir  # e.g. "data/quic360"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ë¯¸ë¦¬ ìƒì„±
    try:
        Path(out_dir).mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„: {out_dir}")
    except Exception as e:
        print(f"âŒ ERROR: ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        exit(1)

    if args.splits:
        splits = args.splits
    else:
        if not raw_dir.exists():
            print(f"âŒ ERROR: Raw ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {raw_dir}")
            exit(1)
        splits = [p.stem for p in raw_dir.glob("*.csv")]
        if not splits:
            print(f"âŒ ERROR: {raw_dir}ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            exit(1)

    print(f"ğŸ” ì²˜ë¦¬í•  splits: {splits}")

    for split in splits:
        print(f"\nğŸ”„ [{split}] ì²˜ë¦¬ ì¤‘â€¦")
        in_csv  = raw_dir / f"{split}.csv"
        out_csv = Path(out_dir) / f"{split}.csv"
        img_dir = Path(out_dir) / split / "images"

        if not in_csv.exists():
            print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {in_csv}, ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        try:
            process_split(
                input_csv=str(in_csv),
                output_csv=str(out_csv),
                image_dir=str(img_dir),
                url_col=args.url_col,
                max_concurrent=args.max_concurrent,
                rps=args.requests_per_second,
                out_dir=out_dir,
                split=split
            )
        except Exception as e:
            print(f"âŒ ERROR: [{split}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ë‹¤ìŒ split ê³„ì† ì²˜ë¦¬
            continue
    
    print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")