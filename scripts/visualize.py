#!/usr/bin/env python3
"""
í•™ìŠµëœ PanoLLaVA ëª¨ë¸ì˜ Vision Encoder ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸

í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ì—ì„œ vision encoderë¥¼ ì¶”ì¶œí•˜ê³ ,
DINOv2 ìŠ¤íƒ€ì¼ì˜ PCA ê¸°ë°˜ ì‹œê°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ê¸°ë³¸ ì‚¬ìš© (ì²´í¬í¬ì¸íŠ¸ì—ì„œ vision encoder ì¶”ì¶œ)
    python scripts/visualize_trained_model.py \
        --checkpoint runs/SQ3_1_latent768_PE_e2p_vision_mlp/last.ckpt \
        --image data/quic360/test/pano_image.jpg \
        --output_dir results/trained_vision_viz

    # ì„¤ì • íŒŒì¼ê³¼ í•¨ê»˜ ì‚¬ìš©
    python script    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir is None:
        args.output_dir = f"visualizations/{checkpoint_name}"

    print("="*70)ualize_trained_model.py \
        --checkpoint runs/SQ3_1_latent768_PE_e2p_vision_mlp/last.ckpt \
        --config configs/default.yaml \
        --image data/quic360/test/pano_image.jpg \
        --crop_strategy anyres_e2p
"""

import argparse
import sys
from pathlib import Path
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import yaml

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# Import legacy config directly from the module file
import importlib.util
spec = importlib.util.spec_from_file_location(
    "panovlm_config_legacy",
    str(Path(__file__).parent.parent / "src" / "panovlm" / "config.py")
)
config_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(config_module)
PanoVLMConfig = config_module.PanoVLMConfig

from panovlm.models.model import PanoramaVLM
from panovlm.evaluation.dino import DinoVisualizer
from panovlm.processors.image import PanoramaImageProcessor


def load_trained_model(
    checkpoint_path: str,
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
):
    """
    í•™ìŠµëœ PanoLLaVA ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    PanoramaVLM.from_checkpoint ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

    Args:
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤

    Returns:
        model: ë¡œë“œëœ PanoramaVLM ëª¨ë¸
        model_config: ëª¨ë¸ ì„¤ì •
    """
    print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")

    # PanoramaVLM.from_checkpoint ì‚¬ìš© (LoRA ìë™ ê°ì§€ í¬í•¨)
    model = PanoramaVLM.from_checkpoint(
        checkpoint_path=checkpoint_path,
        device=device,
        auto_detect_lora=True,
        strict_loading=False
    )

    # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    model_config = model.config

    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    print(f"   Vision: {model_config.vision_name}")
    print(f"   Language: {model_config.language_model_name}")
    print(f"   Resampler: {model_config.resampler_type}")

    model.eval()

    return model, model_config


def extract_vision_features_from_model(
    model: PanoramaVLM,
    image_path: str,
    crop_strategy: str = "e2p",
    image_size: int = 224,
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
):
    """
    í•™ìŠµëœ ëª¨ë¸ì˜ vision encoderì—ì„œ hidden statesë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        model: PanoramaVLM ëª¨ë¸
        image_path: ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ
        crop_strategy: ì´ë¯¸ì§€ crop ì „ëµ
        image_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤

    Returns:
        vision_features: Vision encoderì˜ features (numpy array)
        resampled_features: Resamplerë¥¼ ê±°ì¹œ features (numpy array)
        pixel_values: ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í…ì„œ
        view_metadata: View ë©”íƒ€ë°ì´í„°
    """
    print(f"\nğŸ“¸ ì´ë¯¸ì§€ ë¡œë”©: {image_path}")
    image = Image.open(image_path).convert("RGB")

    print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (strategy: {crop_strategy})")
    
    # Panorama image processor ì„¤ì •
    pano_processor = PanoramaImageProcessor(
        crop_strategy=crop_strategy,
        image_size=(image_size, image_size),
        fov_deg=90,
        overlap_ratio=0.5,
        normalize=True,
        use_vision_processor=True,
        vision_model_name=model.config.vision_name if hasattr(model, 'config') and hasattr(model.config, 'vision_name') else None
    )
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    pixel_values = pano_processor(image)
    
    if pixel_values.dim() == 3:
        pixel_values = pixel_values.unsqueeze(0)  # [V, C, H, W]
    
    num_views = pixel_values.shape[0]
    print(f"âœ… {num_views}ê°œ view ìƒì„±ë¨")
    
    # View metadata ì¶”ì¶œ
    view_metadata = pano_processor.view_metadata if hasattr(pano_processor, 'view_metadata') else None
    
    # anyres_e2pì˜ ê²½ìš° 0ë²ˆì§¸ view(global view) ì œê±°
    skip_first_view = (crop_strategy == "anyres_e2p")
    if skip_first_view and num_views > 1:
        print(f"âš ï¸  anyres_e2p ì „ëµ: 0ë²ˆì§¸ global view ì œì™¸ ({num_views} â†’ {num_views-1} views)")
        pixel_values = pixel_values[1:]  # 0ë²ˆì§¸ ì œê±°
        num_views = pixel_values.shape[0]
    
    # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
    img_batch = pixel_values.unsqueeze(0).to(device)  # [1, V, C, H, W]
    
    # Vision encoderì™€ Resamplerë¥¼ í†µí•œ features ì¶”ì¶œ
    print("ğŸ§  Vision encoderì™€ Resamplerì—ì„œ features ì¶”ì¶œ ì¤‘...")
    model.eval()
    
    with torch.no_grad():
        # 1. Vision encoder ì²˜ë¦¬
        vision_result = model._process_vision_encoder(img_batch)
        vision_features = vision_result["vision_features"]  # [B*V, S, D_vision]
        batch_size = vision_result["batch_size"]
        num_views_from_model = vision_result["num_views"]
        
        print(f"  Vision features shape: {vision_features.shape}")
        
        # 2. Resampler ì²˜ë¦¬
        resampled_features = None
        if hasattr(model, '_process_resampler') and model.resampler is not None:
            resampled_features = model._process_resampler(
                vision_features, batch_size, num_views_from_model
            )  # [B*V, S, D_latent]
            print(f"  Resampled features shape: {resampled_features.shape}")
        
        # Numpyë¡œ ë³€í™˜
        vision_features_np = vision_features.cpu().detach().numpy()
        resampled_features_np = resampled_features.cpu().detach().numpy() if resampled_features is not None else None
    
    return vision_features_np, resampled_features_np, pixel_values, view_metadata


def visualize_features(
    vision_features,
    resampled_features,
    pixel_values,
    output_dir: str,
    n_components: int = 3,
    remove_cls_token: bool = False,
    bg_removal_method: str = "threshold",
    analyze_similarity: bool = True,
    view_metadata = None,
    checkpoint_name: str = ""
):
    """
    ì¶”ì¶œëœ vision featuresë¥¼ ì‹œê°í™”í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        vision_features: Vision encoderì˜ features (numpy array, shape: [V, S, D])
        resampled_features: Resamplerë¥¼ ê±°ì¹œ features (numpy array, shape: [V, S, D])
        pixel_values: ì›ë³¸ ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        n_components: PCA ì£¼ì„±ë¶„ ê°œìˆ˜
        remove_cls_token: CLS í† í° ì œê±° ì—¬ë¶€
        bg_removal_method: ë°°ê²½ ì œê±° ë°©ë²•
        analyze_similarity: ìœ ì‚¬ë„ ë¶„ì„ ìˆ˜í–‰ ì—¬ë¶€
        view_metadata: View ë©”íƒ€ë°ì´í„°
        checkpoint_name: ì²´í¬í¬ì¸íŠ¸ ì´ë¦„ (ì‹œê°í™” ì œëª©ìš©)
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    num_views = vision_features.shape[0] if len(vision_features.shape) == 3 else len(vision_features)
    
    print("\n" + "="*70)
    print("ğŸ¨ Vision Encoder Features ì‹œê°í™”")
    print("="*70)
    
    # 1. ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
    print(f"\nğŸ“· [1/4] ì›ë³¸ View ì´ë¯¸ì§€ ì €ì¥")
    fig, axes = plt.subplots(1, num_views, figsize=(4 * num_views, 4))
    if num_views == 1:
        axes = [axes]
    
    for i in range(num_views):
        img = pixel_values[i].permute(1, 2, 0).cpu().numpy()
        # Denormalize
        # mean = np.array([0.485, 0.456, 0.406])
        # std = np.array([0.229, 0.224, 0.225])
        mean = np.array([0.5, 0.5, 0.5])
        std = np.array([0.5, 0.5, 0.5])
        img = std * img + mean
        img = np.clip(img, 0, 1)
        
        axes[i].imshow(img)
        axes[i].set_title(f'View {i+1}', fontsize=12, fontweight='bold')
        axes[i].axis('off')
    
    plt.suptitle(f'Input Views - {checkpoint_name}', fontsize=14, fontweight='bold')
    plt.tight_layout()
    orig_save_path = output_path / f"original_views{checkpoint_name}.png"
    plt.savefig(orig_save_path, dpi=300, bbox_inches='tight')
    print(f"   âœ… ì €ì¥: {orig_save_path}")
    plt.close()
    
    # 2. Vision Encoder Features ì‹œê°í™”
    print(f"\nğŸ“Š [2/4] Vision Encoder Features PCA ì‹œê°í™”")

    # SigLIPì€ CLS í† í°ì´ ì—†ìœ¼ë¯€ë¡œ ì œê±°í•˜ì§€ ì•ŠìŒ
    # (196 patches = 14x14ë¡œ ì •ì‚¬ê°í˜•ì„ ìœ ì§€í•´ì•¼ í•¨)
    vision_visualizer = DinoVisualizer(
        hidden_states_list=vision_features,
        remove_cls_token=False  # SigLIP: CLS í† í° ì—†ìŒ
    )
    
    print(f"   PCA í•™ìŠµ (n_components={n_components}, bg_removal={bg_removal_method})")
    vision_visualizer.fit_pca(
        n_components=n_components,
        use_background_removal=True if bg_removal_method != "none" else False,
        bg_removal_method=bg_removal_method if bg_removal_method != "none" else None,
        use_global_scaling=True
    )
    
    titles = [f'View {i+1}' for i in range(num_views)]
    save_path = output_path / f"vision_encoder_pca{checkpoint_name}.png"
    vision_visualizer.plot_pca_results(
        titles=titles,
        save_path=str(save_path),
        figsize=(4 * num_views, 5)
    )
    print(f"   âœ… ì €ì¥: {save_path}")
    
    # 3. Resampled Features ì‹œê°í™” (ìˆëŠ” ê²½ìš°)
    resampler_visualizer = None
    if resampled_features is not None:
        print(f"\nğŸ“Š [3/4] Resampled Features PCA ì‹œê°í™”")
        resampler_visualizer = DinoVisualizer(
            hidden_states_list=resampled_features,
            remove_cls_token=False  # Resampler outputì€ ë³´í†µ CLS tokenì´ ì—†ìŒ
        )
        
        resampler_visualizer.fit_pca(
            n_components=n_components,
            use_background_removal=True if bg_removal_method != "none" else False,
            bg_removal_method=bg_removal_method if bg_removal_method != "none" else None,
            use_global_scaling=True
        )
        
        save_path_resampled = output_path / f"resampled_features_pca{checkpoint_name}.png"
        resampler_visualizer.plot_pca_results(
            titles=titles,
            save_path=str(save_path_resampled),
            figsize=(4 * num_views, 5)
        )
        print(f"   âœ… ì €ì¥: {save_path_resampled}")
        
        # Resampler Dashboard ìƒì„±
        if hasattr(resampler_visualizer, 'create_comprehensive_dashboard'):
            resampler_dashboard_path = output_path / f"resampler_dashboard{checkpoint_name}.png"
            try:
                resampler_visualizer.create_comprehensive_dashboard(
                    titles=f"Resampler - {checkpoint_name}",
                    view_metadata=view_metadata,
                    save_path=str(resampler_dashboard_path)
                )
                print(f"   âœ… Dashboard ì €ì¥: {resampler_dashboard_path}")
            except Exception as e:
                print(f"   âš ï¸  Resampler Dashboard ìƒì„± ì‹¤íŒ¨: {e}")
    else:
        print(f"\nâ­ï¸  [3/4] Resampled Features ì—†ìŒ (ê±´ë„ˆë›°ê¸°)")
    
    # 4. Comprehensive Dashboard ìƒì„± (Vision Encoder)
    print(f"\nğŸ“Š [4/4] Vision Encoder Dashboard ìƒì„±")
    if hasattr(vision_visualizer, 'create_comprehensive_dashboard'):
        dashboard_path = output_path / f"vision_encoder_dashboard{checkpoint_name}.png"
        try:
            vision_visualizer.create_comprehensive_dashboard(
                titles=f"Vision Encoder - {checkpoint_name}",
                view_metadata=view_metadata,
                save_path=str(dashboard_path)
            )
            print(f"   âœ… ì €ì¥: {dashboard_path}")
        except Exception as e:
            print(f"   âš ï¸  Dashboard ìƒì„± ì‹¤íŒ¨: {e}")
    
    # 5. ìœ ì‚¬ë„ ë¶„ì„
    if analyze_similarity and num_views > 1:
        print("\n" + "="*70)
        print("ğŸ“ˆ ìœ ì‚¬ë„ ë¶„ì„")
        print("="*70)
        
        # ì¸ì ‘ view ê°„ ìœ ì‚¬ë„ ë¶„ì„
        pairs = [(i, (i + 1) % num_views) for i in range(num_views)]
        
        # Vision Encoder ìœ ì‚¬ë„
        print("\nğŸ” Vision Encoder ìœ ì‚¬ë„:")
        try:
            vision_sim = vision_visualizer.get_hidden_similarity(
                pairs=pairs,
                view_metadata=view_metadata
            )
        except Exception as e:
            print(f"   âš ï¸  ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            vision_sim = {}
        
        # Resampler ìœ ì‚¬ë„
        resampler_sim = {}
        if resampler_visualizer is not None:
            print("\nğŸ” Resampled Features ìœ ì‚¬ë„:")
            try:
                resampler_sim = resampler_visualizer.get_hidden_similarity(
                    pairs=pairs,
                    view_metadata=view_metadata
                )
            except Exception as e:
                print(f"   âš ï¸  ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        # ìœ ì‚¬ë„ ê²°ê³¼ ì €ì¥
        similarity_file = output_path / f"similarity_analysis{checkpoint_name}.txt"
        with open(similarity_file, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write(f"í•™ìŠµëœ ëª¨ë¸ Vision Encoder ìœ ì‚¬ë„ ë¶„ì„\n")
            f.write(f"Checkpoint: {checkpoint_name}\n")
            f.write("=" * 70 + "\n\n")
            
            if vision_sim:
                f.write("=" * 70 + "\n")
                f.write("Vision Encoder Features ìœ ì‚¬ë„\n")
                f.write("=" * 70 + "\n")
                for metric, values in vision_sim.items():
                    if values is None or (isinstance(values, list) and all(v is None for v in values)):
                        continue
                    f.write(f"\n{metric}:\n")
                    if isinstance(values, list):
                        for pair, val in zip(pairs, values):
                            if val is not None:
                                f.write(f"  View {pair[0]+1} â†” View {pair[1]+1}: {val:.4f}\n")
                        valid_values = [v for v in values if v is not None]
                        if valid_values:
                            f.write(f"  í‰ê· : {np.mean(valid_values):.4f} Â± {np.std(valid_values):.4f}\n")
                    else:
                        f.write(f"  ê°’: {values:.4f}\n")
            
            if resampler_sim:
                f.write("\n" + "=" * 70 + "\n")
                f.write("Resampled Features ìœ ì‚¬ë„\n")
                f.write("=" * 70 + "\n")
                for metric, values in resampler_sim.items():
                    if values is None or (isinstance(values, list) and all(v is None for v in values)):
                        continue
                    f.write(f"\n{metric}:\n")
                    if isinstance(values, list):
                        for pair, val in zip(pairs, values):
                            if val is not None:
                                f.write(f"  View {pair[0]+1} â†” View {pair[1]+1}: {val:.4f}\n")
                        valid_values = [v for v in values if v is not None]
                        if valid_values:
                            f.write(f"  í‰ê· : {np.mean(valid_values):.4f} Â± {np.std(valid_values):.4f}\n")
                    else:
                        f.write(f"  ê°’: {values:.4f}\n")
        
        print(f"âœ… ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼ ì €ì¥: {similarity_file}")
    
    print("\n" + "="*70)
    print(f"âœ… ëª¨ë“  ì‹œê°í™” ì™„ë£Œ! ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_path}")
    print("="*70)


def main():
    parser = argparse.ArgumentParser(
        description="í•™ìŠµëœ PanoLLaVA ëª¨ë¸ì˜ Vision Encoder ì‹œê°í™” ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‚¬ìš©
  python scripts/visualize_trained_model.py \\
      --checkpoint runs/SQ3_1_latent768_PE_e2p_vision_mlp/last.ckpt \\
      --image data/quic360/test/image.jpg \\
      --crop_strategy anyres_e2p
        """
    )

    parser.add_argument("--checkpoint", type=str, required=True,
                       help="í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (.ckpt)")
    parser.add_argument("--image", type=str, required=True,
                       help="ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ")
    parser.add_argument("--crop_strategy", type=str, default="e2p",
                       help="ì´ë¯¸ì§€ crop ì „ëµ (e2p, anyres_e2p, cubemap, etc.)")
    parser.add_argument("--image_size", type=int, default=224,
                       help="ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°")
    parser.add_argument("--output_dir", type=str, default=None,
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: visualizations/{checkpoint_name})")
    parser.add_argument("--n_components", type=int, default=3,
                       help="PCA ì£¼ì„±ë¶„ ê°œìˆ˜ (RGB ì‹œê°í™”ìš©)")
    parser.add_argument("--no_cls_token", action="store_true",
                       help="CLS í† í°ì„ ì œê±°í•˜ì§€ ì•ŠìŒ")
    parser.add_argument("--bg_removal", type=str, default="threshold",
                       choices=["threshold", "remove_first_pc", "outlier_removal", "none"],
                       help="ë°°ê²½ ì œê±° ë°©ë²•")
    parser.add_argument("--no_similarity", action="store_true",
                       help="ìœ ì‚¬ë„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ")
    parser.add_argument("--device", type=str, default="auto",
                       help="ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (auto, cuda, cpu)")

    args = parser.parse_args()

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device

    # ì²´í¬í¬ì¸íŠ¸ ì´ë¦„ ì¶”ì¶œ
    checkpoint_name = Path(args.checkpoint).parent.name

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir is None:
        args.output_dir = f"results/visualizations/{checkpoint_name}"

    print("="*70)
    print("ğŸš€ í•™ìŠµëœ ëª¨ë¸ Vision Encoder ì‹œê°í™”")
    print("="*70)
    print(f"Checkpoint: {args.checkpoint}")
    print(f"Image: {args.image}")
    print(f"Crop Strategy: {args.crop_strategy}")
    print(f"Device: {device}")
    print(f"Output: {args.output_dir}")
    print("="*70)

    # ëª¨ë¸ ë¡œë“œ
    model, model_config = load_trained_model(
        checkpoint_path=args.checkpoint,
        device=device
    )

    # Feature ì¶”ì¶œ
    vision_features, resampled_features, pixel_values, view_metadata = extract_vision_features_from_model(
        model=model,
        image_path=args.image,
        crop_strategy=args.crop_strategy,
        image_size=args.image_size,
        device=device
    )
    
    # ì‹œê°í™”
    bg_removal = None if args.bg_removal == "none" else args.bg_removal
    visualize_features(
        vision_features=vision_features,
        resampled_features=resampled_features,
        pixel_values=pixel_values,
        output_dir=args.output_dir,
        n_components=args.n_components,
        remove_cls_token=not args.no_cls_token,
        bg_removal_method=bg_removal or "threshold",
        analyze_similarity=not args.no_similarity,
        view_metadata=view_metadata,
        checkpoint_name=f"_{checkpoint_name}"
    )


if __name__ == "__main__":
    main()
