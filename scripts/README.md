# Panorama VLM Training Scripts

YAML κΈ°λ° μ„¤μ • μ‹μ¤ν…μ„ μ‚¬μ©ν• 3λ‹¨κ³„ ν•™μµ νμ΄ν”„λΌμΈ

## π“ κµ¬μ΅°

```
scripts/
β”β”€β”€ train_vision.sh       # Stage 1: Vision ν•™μµ (Linux/macOS)
β”β”€β”€ train_resampler.sh    # Stage 2: Resampler ν•™μµ (Linux/macOS)  
β”β”€β”€ train_finetune.sh     # Stage 3: LoRA νμΈνλ‹ (Linux/macOS)
β”β”€β”€ train_all.sh          # μ „μ²΄ νμ΄ν”„λΌμΈ μ‹¤ν–‰ (Linux/macOS)
β”β”€β”€ train_vision.bat      # Stage 1: Vision ν•™μµ (Windows)
β”β”€β”€ train_resampler.bat   # Stage 2: Resampler ν•™μµ (Windows)
β”β”€β”€ train_finetune.bat    # Stage 3: LoRA νμΈνλ‹ (Windows)
β”β”€β”€ train_all.bat         # μ „μ²΄ νμ΄ν”„λΌμΈ μ‹¤ν–‰ (Windows)
β””β”€β”€ old_scripts/          # κΈ°μ΅΄ μ¤ν¬λ¦½νΈ λ°±μ—…
```

## π€ μ‚¬μ©λ²•

### ν™κ²½ μ„¤μ •

**ν•„μ ν™κ²½ λ³€μ:**
```bash
export CSV_TRAIN="path/to/train.csv"
export CSV_VAL="path/to/val.csv"
```

**μ„ νƒμ  ν™κ²½ λ³€μ:**
```bash
export CUDA_VISIBLE_DEVICES=0
export WANDB_PROJECT="my-panorama-vlm"
```

### Linux/macOS

**κ°λ³„ Stage μ‹¤ν–‰:**
```bash
# Stage 1: Vision Encoder ν•™μµ
./scripts/train_vision.sh

# Stage 2: Resampler ν•™μµ  
./scripts/train_resampler.sh

# Stage 3: LoRA νμΈνλ‹
./scripts/train_finetune.sh
```

**μ „μ²΄ νμ΄ν”„λΌμΈ μ‹¤ν–‰:**
```bash
./scripts/train_all.sh
```

### Windows

**κ°λ³„ Stage μ‹¤ν–‰:**
```cmd
REM Stage 1: Vision Encoder ν•™μµ
scripts\train_vision.bat

REM Stage 2: Resampler ν•™μµ
scripts\train_resampler.bat

REM Stage 3: LoRA νμΈνλ‹
scripts\train_finetune.bat
```

**μ „μ²΄ νμ΄ν”„λΌμΈ μ‹¤ν–‰:**
```cmd
scripts\train_all.bat
```

## β™οΈ μ„¤μ • μ»¤μ¤ν„°λ§μ΄μ§•

### 1. YAML μ„¤μ • νμΌ μμ •

κ° stageλ³„ μ„¤μ •μ€ `configs/stages/` λ””λ ‰ν† λ¦¬μ—μ„ μμ •:

```
configs/
β”β”€β”€ base.yaml           # κΈ°λ³Έ μ„¤μ •
β””β”€β”€ stages/
    β”β”€β”€ vision.yaml     # Stage 1 μ„¤μ •
    β”β”€β”€ resampler.yaml  # Stage 2 μ„¤μ •
    β””β”€β”€ finetune.yaml   # Stage 3 μ„¤μ •
```

### 2. ν™κ²½ λ³€μλ΅ μ¤λ²„λΌμ΄λ“

```bash
# ν•™μµλ¥  λ³€κ²½
export PANO_VLM_TRAINING_LEARNING_RATE=1e-4

# λ°°μΉ ν¬κΈ° λ³€κ²½
export PANO_VLM_DATA_BATCH_SIZE=8

# LoRA μ„¤μ • λ³€κ²½
export PANO_VLM_MODEL_LORA_R=16
export PANO_VLM_MODEL_LORA_ALPHA=32
```

### 3. λ…λ Ήν–‰ μΈμλ΅ μ¤λ²„λΌμ΄λ“

```bash
./scripts/train_finetune.sh --lr 2e-4 --batch-size 4
```

## π“ LoRA μ„¤μ •

Stage 3 νμΈνλ‹μ—μ„ μ‚¬μ©λλ” LoRA νλΌλ―Έν„°:

| νλΌλ―Έν„° | κΈ°λ³Έκ°’ | μ„¤λ… |
|----------|--------|------|
| `r` | 16 | LoRA rank (λ‚®μ„μλ΅ ν¨μ¨μ ) |
| `alpha` | 32 | ν•™μµ κ°€μ¤‘μΉ (λ³΄ν†µ rμ 2λ°°) |
| `dropout` | 0.1 | μ •κ·ν™” λ“λ΅­μ•„μ›ƒ |

## π”§ κ³ κΈ‰ μ‚¬μ©λ²•

### μ»¤μ¤ν…€ μ„¤μ • νμΌ μ‚¬μ©

```bash
python train.py \
    --config-stage finetune \
    --config-override my_custom_config.yaml
```

### μ²΄ν¬ν¬μΈνΈμ—μ„ μ¬μ‹μ‘

```bash
./scripts/train_resampler.sh --resume-from ./runs/e2p_vision_mlp/best.ckpt
```

### λ‹¤μ¤‘ GPU μ‚¬μ©

```bash
export CUDA_VISIBLE_DEVICES=0,1,2,3
./scripts/train_all.sh
```

## π“ κ²°κ³Ό ν™•μΈ

ν•™μµ μ™„λ£ ν›„ κ²°κ³Όλ” λ‹¤μ μ„μΉμ— μ €μ¥λ©λ‹λ‹¤:

```
runs/
β”β”€β”€ e2p_vision_mlp/     # Stage 1 κ²°κ³Ό
β”β”€β”€ e2p_resampler_mlp/  # Stage 2 κ²°κ³Ό
β””β”€β”€ e2p_finetune_mlp/   # Stage 3 κ²°κ³Ό (LoRA μ–΄λ‘ν„°)
```

## π› λ¬Έμ  ν•΄κ²°

### μΌλ°μ μΈ λ¬Έμ λ“¤

1. **CUDA OOM μ—λ¬**
   ```bash
   export PANO_VLM_DATA_BATCH_SIZE=1
   ```

2. **μ²΄ν¬ν¬μΈνΈ μ—†μ μ—λ¬**
   - μ΄μ „ stageκ°€ μ™„λ£λμ—λ”μ§€ ν™•μΈ
   - `--resume-from` μΈμλ΅ μλ™ μ§€μ •

3. **YAML μ„¤μ • μ—λ¬**
   - `configs/` λ””λ ‰ν† λ¦¬ μ΅΄μ¬ ν™•μΈ
   - YAML λ¬Έλ²• κ²€μ¦

### λ΅κ·Έ ν™•μΈ

```bash
tail -f training.log
```

## π“ λ§μ΄κ·Έλ μ΄μ… λ…ΈνΈ

κΈ°μ΅΄ μ¤ν¬λ¦½νΈμ—μ„ μƒλ΅μ΄ YAML κΈ°λ° μ‹μ¤ν…μΌλ΅ λ§μ΄κ·Έλ μ΄μ…:

- β… κΈ°μ΅΄ μ¤ν¬λ¦½νΈλ” `old_scripts/`λ΅ λ°±μ—…λ¨
- β… λ¨λ“  κΈ°λ¥μ΄ μƒλ΅μ΄ μ‹μ¤ν…μ—μ„ μ§€μ›λ¨
- β… ν™κ²½ λ³€μ μ΄λ¦„μ΄ `PANO_VLM_*` ν•μ‹μΌλ΅ λ³€κ²½λ¨
- β… λ” λ‚μ€ μ„¤μ • κ΄€λ¦¬μ™€ μ¤λ²„λΌμ΄λ“ κΈ°λ¥ μ κ³µ

## π†• μƒλ΅μ΄ κΈ°λ¥

### YAML κΈ°λ° μ„¤μ • μ‹μ¤ν…
- κ³„μΈµμ  μ„¤μ • κ΄€λ¦¬ (base + stage override)
- ν™κ²½ λ³€μλ¥Ό ν†µν• λ°νƒ€μ„ μ¤λ²„λΌμ΄λ“
- νƒ€μ… μ•μ „μ„±κ³Ό μ„¤μ • κ²€μ¦

### LoRA μ§€μ›
- Stage 3μ—μ„ ν¨μ¨μ μΈ νμΈνλ‹
- λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ€ν­ κ°μ†
- λΉ λ¥Έ μλ ΄κ³Ό μΆ‹μ€ μ„±λ¥

### μλ™ μ²΄ν¬ν¬μΈνΈ νƒμ§€
- μ΄μ „ stage κ²°κ³Όλ¥Ό μλ™μΌλ΅ μ°Ύμ•„μ„ μ—°κ²°
- μλ™ μ§€μ • μ—†μ΄λ„ νμ΄ν”„λΌμΈ μ‹¤ν–‰ κ°€λ¥

### ν–¥μƒλ λ΅κΉ…
- WandB ν†µν•© λ΅κΉ…
- μ„¤μ • μ •λ³΄ μλ™ κΈ°λ΅
- λ””λ²„κΉ… μ •λ³΄ μ¶λ ¥

## π”„ μ‚¬μ© μμ‹

### λΉ λ¥Έ μ‹μ‘
```bash
# λ°μ΄ν„° μ¤€λΉ„
export CSV_TRAIN="data/quic360/train.csv"
export CSV_VAL="data/quic360/valid.csv"

# μ „μ²΄ νμ΄ν”„λΌμΈ μ‹¤ν–‰
./scripts/train_all.sh
```

### μ»¤μ¤ν…€ LoRA μ„¤μ •μΌλ΅ νμΈνλ‹
```bash
# LoRA νλΌλ―Έν„° μ„¤μ •
export PANO_VLM_MODEL_LORA_R=32
export PANO_VLM_MODEL_LORA_ALPHA=64
export PANO_VLM_MODEL_LORA_DROPOUT=0.05

# Stage 3λ§ μ‹¤ν–‰
./scripts/train_finetune.sh
```

### κ°λ° λ¨λ“ (μ‘μ€ λ°°μΉ ν¬κΈ°)
```bash
export PANO_VLM_DATA_BATCH_SIZE=1
export PANO_VLM_TRAINING_EPOCHS=1
./scripts/train_vision.sh
```