#!/bin/bash

# =============================================================================
# LoRAë¥¼ ì‚¬ìš©í•œ Finetune Stage í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
# =============================================================================

# ì„¤ì • íŒŒì¼ ë¡œë“œ
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/config.sh"

# LoRA ì„¤ì • í™œì„±í™”
override_config "use_lora" true
override_config "lora_rank" 16
override_config "lora_alpha" 32
override_config "lora_dropout" 0.1

echo "ğŸš€ Starting LoRA Finetune Stage Training..."
echo "âš™ï¸  Configuration:"
echo "   - Vision Model: $VISION_MODEL"
echo "   - Language Model: $LM_MODEL"
echo "   - Resampler: $RESAMPLER"
echo "   - Crop Strategy: $CROP_STRATEGY"
echo "   - Batch Size: $FINETUNE_BATCH_SIZE"
echo "   - Epochs: $FINETUNE_EPOCHS"
echo "   - Max Text Length: $MAX_TEXT_LENGTH"
echo "   - LoRA Rank: $LORA_RANK"
echo "   - LoRA Alpha: $LORA_ALPHA"
echo "   - LoRA Dropout: $LORA_DROPOUT"
echo "   - Save LoRA Only: $SAVE_LORA_ONLY"
echo ""
echo "ğŸ’¡ Note: LoRA weights will be automatically saved to: ./runs/${CROP_STRATEGY}_finetune_${RESAMPLER}/lora_weights"
echo ""

# í›ˆë ¨ ëª…ë ¹ì–´ êµ¬ì„±
TRAIN_CMD="python train.py \
    --csv-train \"$CSV_TRAIN\" \
    --csv-val \"$CSV_VAL\" \
    --vision-name \"$VISION_MODEL\" \
    --lm-name \"$LM_MODEL\" \
    --resampler \"$RESAMPLER\" \
    --stage finetune \
    --crop-strategy \"$CROP_STRATEGY\" \
    --image-size $IMAGE_SIZE \
    --epochs $FINETUNE_EPOCHS \
    --batch-size $FINETUNE_BATCH_SIZE \
    --lr 5e-5 \
    --num-workers $NUM_WORKERS \
    --max-text-length $MAX_TEXT_LENGTH \
    --system-msg \"$FINETUNE_SYSTEM_MSG\" \
    --wandb-project \"$WANDB_PROJECT\" \
    --wandb-name \"${CROP_STRATEGY}_finetune_${RESAMPLER}_lora\" \
    --use-lora \
    --lora-rank $LORA_RANK \
    --lora-alpha $LORA_ALPHA \
    --lora-dropout $LORA_DROPOUT"

# LoRA ê°€ì¤‘ì¹˜ë§Œ ì €ì¥í• ì§€ ê²°ì •
if [ "$SAVE_LORA_ONLY" = "true" ]; then
    TRAIN_CMD="$TRAIN_CMD --save-lora-only"
fi

# ì´ì „ ë‹¨ê³„ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
RESAMPLER_CKPT="./runs/${CROP_STRATEGY}_resampler_${RESAMPLER}/best.ckpt"
if [ -f "$RESAMPLER_CKPT" ]; then
    echo "ğŸ“‚ Found resampler checkpoint: $RESAMPLER_CKPT"
    TRAIN_CMD="$TRAIN_CMD --resume-from \"$RESAMPLER_CKPT\""
else
    echo "âš ï¸  No resampler checkpoint found at $RESAMPLER_CKPT"
    echo "   Starting finetune stage from scratch"
fi

echo ""
echo "ğŸ”§ Executing command:"
echo "$TRAIN_CMD"
echo ""

# ëª…ë ¹ì–´ ì‹¤í–‰
eval $TRAIN_CMD

# ì‹¤í–‰ ê²°ê³¼ í™•ì¸
if [ $? -eq 0 ]; then
    echo ""
    echo "âœ… LoRA Finetune training completed successfully!"
    
    # ê²°ê³¼ íŒŒì¼ í™•ì¸
    RESULT_DIR="./runs/${CROP_STRATEGY}_finetune_${RESAMPLER}"
    if [ -d "$RESULT_DIR" ]; then
        echo "ğŸ“ Results saved in: $RESULT_DIR"
        echo "ğŸ“‹ Available files:"
        ls -la "$RESULT_DIR"/ | head -10
        
        # LoRA ê°€ì¤‘ì¹˜ í™•ì¸ (í•­ìƒ ì €ì¥ë¨)
        if [ -d "$RESULT_DIR/lora_weights" ]; then
            echo ""
            echo "âœ… LoRA weights saved for evaluation:"
            echo "ğŸ“‚ $RESULT_DIR/lora_weights/"
            ls -la "$RESULT_DIR/lora_weights"/ 2>/dev/null || echo "   (Contents may be loading...)"
        else
            echo "âš ï¸  LoRA weights directory not found"
        fi
        
        # ì „ì²´ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        if [ -f "$RESULT_DIR/best.ckpt" ]; then
            echo "âœ… PyTorch Lightning checkpoint: $RESULT_DIR/best.ckpt"
        fi
        
        if [ -f "$RESULT_DIR/model_final.safetensors" ]; then
            echo "âœ… Final model weights: $RESULT_DIR/model_final.safetensors"
        elif [ "$SAVE_LORA_ONLY" = "true" ]; then
            echo "ğŸ’¾ Full model weights skipped (save-lora-only enabled)"
        fi
    fi
    
    echo ""
    echo "ğŸ¯ To evaluate the LoRA model:"
    echo "   bash scripts/eval_lora_finetune.sh"
    echo ""
    echo "ğŸ”„ To merge LoRA weights back to base model:"
    echo "   python -c \"from panovlm.model import PanoramaVLM; model = PanoramaVLM(...); model.load_lora_weights('$RESULT_DIR/lora_weights'); model.merge_lora_weights()\""
else
    echo ""
    echo "âŒ Training failed with exit code $?"
    echo "ğŸ“‹ Check the logs for more details"
    exit 1
fi
