# coding: utf-8
"""
PanoLLaVA Comprehensive Model Evaluation System
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ë‹¨ê³„ë³„ í‰ê°€ ì‹œìŠ¤í…œ:
1. ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ (ChatPanoTestDataset, VLMDataModule)
3. ë°°ì¹˜ë³„ í…ìŠ¤íŠ¸ ìƒì„± (generate)
4. ì˜ˆì¸¡/ì •ë‹µ í…ìŠ¤íŠ¸ ì €ì¥ ë° ë¡œê¹…
5. í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (BLEU, ROUGE, METEOR, SPICE, CIDEr, CLIP-S, RefCLIP-S)

ì‚¬ìš©ë²•:
    # ë°©ë²• 1: Config ê¸°ë°˜ í‰ê°€ (ìë™ ì²´í¬í¬ì¸íŠ¸ íƒìƒ‰)
    python eval.py --config config.yaml --csv-input data/quic360/test.csv
    
    # ë°©ë²• 2: ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì§ì ‘ ì§€ì • (ê¶Œì¥) âœ¨
    python eval.py --checkpoint-dir runs/ADDDATA_SQ3_1/finetune/anyres-e2p_mlp/ \\
                   --csv-input data/quic360/test.csv
    
    # ë°©ë²• 3: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ëª…ì‹œì  ì§€ì • (ê°€ì¥ ì§ì ‘ì ) âœ¨âœ¨
    python eval.py --checkpoint runs/ADDDATA_SQ3_1/finetune/anyres-e2p_mlp/best.ckpt \\
                   --csv-input data/quic360/test.csv
    
    # ë°©ë²• 4: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ìë™ ì„¤ì • (config ë¶ˆí•„ìš”)
    python eval.py --checkpoint-dir runs/ADDDATA_SQ3_1/finetune/anyres-e2p_mlp/
    # â†’ checkpoint_metadata.jsonì—ì„œ ëª¨ë“  ì„¤ì • ìë™ ë¡œë“œ
    # â†’ best.ckpt ë˜ëŠ” last.ckpt ìë™ ì„ íƒ
    
    # ë°©ë²• 5: CSV ë©”íŠ¸ë¦­ ì „ìš© ëª¨ë“œ (ëª¨ë¸ ë¡œë”© ìƒëµ) ğŸš€
    python eval.py --csv-input results/model_predictions_20251113.csv
    # â†’ CSVì— 'prediction'/'reference' ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ë©”íŠ¸ë¦­ë§Œ ê³„ì‚°
    # â†’ ëª¨ë¸ ë¡œë”©ê³¼ ìƒì„± ê³¼ì •ì„ ì™„ì „íˆ ê±´ë„ˆëœ€ (ë¹ ë¥¸ ë©”íŠ¸ë¦­ ì¬ê³„ì‚°)
    
ì£¼ìš” ê¸°ëŠ¥:
    - checkpoint_metadata.json ìë™ ë¡œë“œ (ëª¨ë¸ ì„¤ì •, í•˜ì´í¼íŒŒë¼ë¯¸í„°)
    - best.ckpt/last.ckpt ì‹¬ë³¼ë¦­ ë§í¬ ìš°ì„  ì‚¬ìš©
    - LoRA ê°€ì¤‘ì¹˜ ìë™ íƒìƒ‰ ë° ë¡œë“œ
"""

import argparse
import torch
import json
import logging
import time
import traceback
import os
import sys
import re
from pathlib import Path
from tqdm import tqdm
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple

# Add src to Python path
script_dir = Path(__file__).parent
project_root = script_dir.parent
src_path = project_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from panovlm.config.loader import load_config_dict as _load_train_config_dict
from panovlm.runtime.model_factory import ModelFactory

# ë‚´ë¶€ ëª¨ë“ˆ
# Silence HF tokenizers fork/parallelism warnings and avoid deadlocks
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

from panovlm.dataset import VLMDataModule
from panovlm.processors.universal_text_formatter import UniversalTextFormatter

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


_STAGE_CANONICAL_MAP = {
    "vision": ("vision", "vision_pretraining", "vision_pretrain"),
    "resampler": ("resampler", "resampler_training"),
    "finetune": ("finetune", "instruction_tuning", "instruction_tune"),
    "generate": ("generate", "inference"),
}

_STAGE_VARIANT_LOOKUP = {
    variant: canonical
    for canonical, variants in _STAGE_CANONICAL_MAP.items()
    for variant in variants
}


def _infer_prefix_from_runs(runs_root: Path, crop_strategy: str, stage_names: List[str], resampler: str) -> Optional[str]:
    if not runs_root.exists() or not runs_root.is_dir():
        return None

    search_stage_names = stage_names or []
    patterns: List[Tuple[str, Optional[str]]] = []

    for stage_name in search_stage_names:
        patterns.append((f"*_{crop_strategy}_{stage_name}_{resampler}", stage_name))

    if not patterns:
        patterns.append((f"*_{crop_strategy}_*_{resampler}", None))

    for pattern, stage_hint in patterns:
        try:
            matches = sorted(
                runs_root.glob(pattern),
                key=lambda p: p.stat().st_mtime,
                reverse=True
            )
        except Exception:
            matches = sorted(runs_root.glob(pattern))
        if not matches:
            continue
        for match in matches:
            name = match.name
            if stage_hint:
                suffix = f"_{crop_strategy}_{stage_hint}_{resampler}"
                if not name.endswith(suffix):
                    continue
                prefix_candidate = name[:-len(suffix)]
                if prefix_candidate.endswith('_'):
                    prefix_candidate = prefix_candidate[:-1]
            else:
                if not name.endswith(f"_{resampler}"):
                    continue
                base = name[: -len(f"_{resampler}")]
                if base.endswith('_'):
                    base = base[:-1]
                token = f"_{crop_strategy}_"
                idx = base.rfind(token)
                if idx == -1:
                    continue
                prefix_candidate = base[:idx]
            if prefix_candidate:
                logger.info(f"ğŸ” Inferred prefix '{prefix_candidate}' from runs/{name}")
                return prefix_candidate

    return None


def _stage_variants(stage: Optional[str]) -> List[str]:
    """Return ordered unique stage variants covering historical aliases."""
    if stage is None:
        return []

    stage_key = str(stage).strip()
    canonical = _STAGE_VARIANT_LOOKUP.get(stage_key)
    if canonical:
        variants = _STAGE_CANONICAL_MAP[canonical]
        # Preserve order while removing duplicates
        seen = set()
        return [s for s in variants if not (s in seen or seen.add(s))]

    return [stage_key] if stage_key else []


def load_checkpoint_metadata(ckpt_path: Path) -> Optional[Dict[str, Any]]:
    """
    ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬(ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì˜ ë¶€ëª¨)ì—ì„œ checkpoint_metadata.jsonì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        ckpt_path: ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ë˜ëŠ” ê°œë³„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None (íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°)
    """
    ckpt_dir = ckpt_path if ckpt_path.is_dir() else ckpt_path.parent
    if not ckpt_dir.exists():
        logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ckpt_dir}")
        return None
    
    metadata_path = ckpt_dir / "checkpoint_metadata.json"
    if not metadata_path.exists():
        logger.warning(f"âš ï¸ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_path}")
        return None
    
    try:
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ: {metadata_path}")
        return metadata
    except Exception as e:
        logger.warning(f"âš ï¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def find_checkpoint_in_dir(ckpt_path: Path) -> Optional[Path]:
    """
    ë””ë ‰í† ë¦¬ì—ì„œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ê±°ë‚˜, ì…ë ¥ì´ ì´ë¯¸ .ckpt íŒŒì¼ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìš°ì„ ìˆœìœ„: best.ckpt > last.ckpt > *.ckpt (ìµœì‹ )
    
    Args:
        ckpt_path: ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ë˜ëŠ” ê°œë³„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
    """
    # íŒŒì¼ì´ ì§ì ‘ ì£¼ì–´ì¡Œë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (.ckpt í™•ì¥ìë§Œ í—ˆìš©)
    if ckpt_path.is_file():
        if ckpt_path.suffix == ".ckpt":
            logger.info(f"âœ… Using explicit checkpoint: {ckpt_path}")
            return ckpt_path
        logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í˜•ì‹: {ckpt_path}")
        return None
    
    ckpt_dir = ckpt_path
    # 1. ì‹¬ë³¼ë¦­ ë§í¬ ìš°ì„  (best.ckpt)
    best_ckpt = ckpt_dir / "best.ckpt"
    if best_ckpt.exists():
        # ì‹¬ë³¼ë¦­ ë§í¬ì¸ ê²½ìš° ì‹¤ì œ ê²½ë¡œë¡œ í•´ì„
        resolved = best_ckpt.resolve() if best_ckpt.is_symlink() else best_ckpt
        logger.info(f"âœ… Using best checkpoint: {resolved}")
        return resolved
    
    # 2. last.ckpt
    last_ckpt = ckpt_dir / "last.ckpt"
    if last_ckpt.exists():
        resolved = last_ckpt.resolve() if last_ckpt.is_symlink() else last_ckpt
        logger.info(f"âœ… Using last checkpoint: {resolved}")
        return resolved
    
    # 3. ê°€ì¥ ìµœê·¼ .ckpt íŒŒì¼ (ìˆ˜ì • ì‹œê°„ ê¸°ì¤€)
    try:
        ckpt_files = list(ckpt_dir.glob("*.ckpt"))
        if ckpt_files:
            latest_ckpt = max(ckpt_files, key=lambda p: p.stat().st_mtime)
            logger.info(f"âœ… Using latest checkpoint: {latest_ckpt}")
            return latest_ckpt
    except Exception as e:
        logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    return None


def resolve_model_dir(config_or_path, stage: str = None, crop_strategy: str = None) -> str:
    """
    HF-style ëª¨ë¸ ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰ (PyTorch bin ê¸°ë°˜)
    - config_or_path: dict ë˜ëŠ” JSON íŒŒì¼ ê²½ë¡œ(str)
    - stage/crop_strategy: runs/<prefix>_<crop>_<stage>_<resampler>/hf_model íŒíŠ¸ êµ¬ì„±ì— ì‚¬ìš©
    """
    try:
        # config ë¡œë”© (dict ë˜ëŠ” íŒŒì¼ ê²½ë¡œ)
        if isinstance(config_or_path, (str, Path)):
            config = _load_train_config_dict(str(config_or_path))
        elif isinstance(config_or_path, dict):
            config = config_or_path
        else:
            raise TypeError(f"Unsupported config type: {type(config_or_path)}")

        resampler = config.get('models', {}).get('resampler_type') or config.get('models', {}).get('resampler', 'mlp')
        if stage is None:
            stage = config.get('training', {}).get('default_stage', 'finetune')

        if crop_strategy is None:
            crop_strategy = config.get('image_processing', {}).get('crop_strategy', 'e2p')

        stage_variants = _stage_variants(stage)

        prefix = config.get('training', {}).get('prefix')
        if not prefix:
            prefix = config.get('experiment', {}).get('name') if isinstance(config.get('experiment'), dict) else None
        if not prefix:
            prefix = config.get('experiment', {}).get('id') if isinstance(config.get('experiment'), dict) else None
        if not prefix:
            prefix = _infer_prefix_from_runs(
                Path(config.get('paths', {}).get('runs_dir', 'runs')),
                crop_strategy,
                stage_variants,
                resampler
            )
        if not prefix:
            raise KeyError("training.prefix is required in config.json")

        # ì¶”ê°€: pretrained_dir ì§€ì› ë° HF ë””ë ‰í† ë¦¬/ì²´í¬í¬ì¸íŠ¸ ìë™ íƒìƒ‰
        paths_cfg = config.get('paths', {}) if isinstance(config, dict) else {}
        pretrained_dir = paths_cfg.get('pretrained_dir')
        if pretrained_dir and Path(pretrained_dir).exists():
            p = Path(pretrained_dir)
            if p.is_file() and p.suffix == '.ckpt':
                logger.info(f"âœ… Using checkpoint from config: {pretrained_dir}")
            else:
                logger.info(f"âœ… Using pretrained_dir from config: {pretrained_dir}")
            return str(p)

        # runs ë””ë ‰í† ë¦¬ ë‚´ hf_model í´ë” ìë™ íƒìƒ‰
        runs_root = Path(paths_cfg.get('runs_dir', 'runs'))

        def try_from_run_dir(run_dir: Path, stage_hint: str) -> Optional[str]:
            if not run_dir.exists() or not run_dir.is_dir():
                return None

            hf_dir = run_dir / 'hf_model'
            if hf_dir.exists() and hf_dir.is_dir():
                logger.info(f"âœ… Using HF model dir (stage='{stage_hint}'): {str(hf_dir)}")
                return str(hf_dir)

            pano_dir = run_dir / 'panorama_model'
            if pano_dir.exists() and pano_dir.is_dir():
                logger.info(f"âœ… Using panorama_model dir (stage='{stage_hint}'): {str(pano_dir)}")
                return str(pano_dir)

            best_ckpt = run_dir / 'best.ckpt'
            if best_ckpt.exists():
                logger.info(f"âœ… Using best checkpoint (stage='{stage_hint}'): {str(best_ckpt)}")
                return str(best_ckpt)

            last_ckpt = run_dir / 'last.ckpt'
            if last_ckpt.exists():
                logger.info(f"âœ… Using last checkpoint (stage='{stage_hint}'): {str(last_ckpt)}")
                return str(last_ckpt)

            try:
                any_ckpts = sorted(run_dir.glob('*.ckpt'))
                if any_ckpts:
                    logger.info(f"âœ… Using checkpoint (stage='{stage_hint}'): {str(any_ckpts[0])}")
                    return str(any_ckpts[0])
            except Exception:
                pass

            return None

        checked_stages = set()
        for stage_name in stage_variants:
            checked_stages.add(stage_name)
            candidate_run_dir = runs_root / f"{prefix}_{crop_strategy}_{stage_name}_{resampler}"
            resolved = try_from_run_dir(candidate_run_dir, stage_name)
            if resolved:
                return resolved

        # wildcard fallback: scan any stage between prefix/crop/resampler
        pattern_prefix = f"{prefix}_{crop_strategy}_"
        pattern_suffix = f"_{resampler}"
        wildcard_pattern = f"{pattern_prefix}*{pattern_suffix}"
        for run_dir in sorted(runs_root.glob(wildcard_pattern)):
            stage_hint = run_dir.name[len(pattern_prefix):-len(pattern_suffix)] if len(pattern_suffix) > 0 else run_dir.name[len(pattern_prefix):]
            if stage_hint in checked_stages:
                continue
            resolved = try_from_run_dir(run_dir, stage_hint or "unknown")
            if resolved:
                return resolved

        raise FileNotFoundError("No pretrained model dir found. Set paths.pretrained_dir or pass --model-dir")

    except Exception as e:
        logger.error(f"Failed to resolve model dir: {e}")
        raise



def load_model_and_lora(
    model_dir: str,
    lora_weights_path: Optional[str],
    device: torch.device,
    config_path: Optional[str] = None,
    config_data: Optional[Dict[str, Any]] = None,
    **model_kwargs
):
    """
    1ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ì™€ LoRA ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ì—¬ ìƒì„±ìš© ëª¨ë¸ ì¤€ë¹„ (ì„¤ì • ì‹œìŠ¤í…œ í†µí•©)
    - ìƒˆë¡œìš´ PanoramaVLM ì¸í„°í˜ì´ìŠ¤ ìš°ì„  ì‹œë„
    - ì‹¤íŒ¨ ì‹œ VLMModule í´ë°± (ì´ë•Œ model_configë¥¼ ë°˜ë“œì‹œ ì „ë‹¬)
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ 1ë‹¨ê³„: ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ (ì„¤ì • ì‹œìŠ¤í…œ í†µí•©)")
    logger.info("=" * 60)

    # ë””ë°”ì´ìŠ¤ ë¬¸ìì—´
    device_str = str(device) if device != "auto" else "auto"

    # config ê°ì²´ ì¤€ë¹„ (ModelConfig ë˜ëŠ” dict)
    config_obj = None
    if config_data is not None:
        if isinstance(config_data, dict):
            config_obj = config_data
        else:
            logger.warning("config_data is not a dict; ignoring runtime config override")
    elif config_path:
        try:
            from panovlm.config import ModelConfig
            try:
                config_obj = ModelConfig.load(config_path)
                logger.info(f"ğŸ“‹ ModelConfig ë¡œë“œ ì™„ë£Œ(from {config_path})")
            except Exception as e:
                logger.warning(f"ModelConfig.load ì‹¤íŒ¨, config dictë¡œ ëŒ€ì²´: {e}")
                config_obj = _load_train_config_dict(str(config_path))
        except Exception as e:
            logger.warning(f"panovlm.config.ModelConfig ì‚¬ìš© ë¶ˆê°€, config dictë¡œ ëŒ€ì²´: {e}")
            config_obj = _load_train_config_dict(str(config_path))

    # â”€â”€ PanoramaVLM (HF ë””ë ‰í† ë¦¬ ë˜ëŠ” .ckpt) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        from panovlm.models.model import PanoramaVLM

        model_factory = None
        if config_obj is not None:
            from panovlm.config import ModelConfig as _ModelConfig

            if isinstance(config_obj, _ModelConfig):
                model_factory = ModelFactory(config_obj)
            elif isinstance(config_obj, dict):
                try:
                    model_factory = ModelFactory(_ModelConfig.from_dict(config_obj))
                except Exception:
                    model_factory = None

        extra_cfg = {}
        if config_obj is not None:
            extra_cfg["config"] = config_obj
            extra_cfg["model_config"] = config_obj

        mpath = Path(model_dir)
        if model_factory is not None:
            if mpath.is_file() and mpath.suffix == ".ckpt":
                logger.info(f"ğŸ“¦ Loading from checkpoint: {str(mpath)} (factory)")
                model = model_factory.load_checkpoint(
                    str(mpath),
                    device=device_str,
                    **{k: v for k, v in model_kwargs.items() if v is not None},
                )
            else:
                model = model_factory.load_pretrained_dir(
                    str(mpath),
                    device=device_str,
                    **{k: v for k, v in model_kwargs.items() if v is not None},
                )
        else:
            if mpath.is_file() and mpath.suffix == ".ckpt":
                logger.info(f"ğŸ“¦ Loading from checkpoint: {str(mpath)}")
                model = PanoramaVLM.from_checkpoint(
                    str(mpath),
                    device=device_str,
                    **extra_cfg,
                    **{k: v for k, v in model_kwargs.items() if v is not None}
                )
            else:
                model = PanoramaVLM.from_pretrained_dir(
                    str(mpath),
                    device=device_str,
                    **extra_cfg,
                    **{k: v for k, v in model_kwargs.items() if v is not None}
                )

        # ì„¤ì • ì •ë³´ ë¡œê·¸
        if hasattr(model, "config") and model.config:
            logger.info("ğŸ“‹ Model Configuration ìš”ì•½:")
            for k in [
                "vision_name", "language_model_name", "latent_dimension",
                "image_size", "crop_strategy", "use_lora", "lora_r", "lora_alpha"
            ]:
                try:
                    val = getattr(model.config, k, None)
                except Exception:
                    val = None
                if val is not None:
                    logger.info(f"   - {k}: {val}")

        # ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„ ìœ„í•œ ë˜í¼
        class ModelWrapper:
            def __init__(self, panorama_model):
                self.model = panorama_model
                self._stage_key = "finetune"
            def eval(self):
                self.model.eval(); return self
            def to(self, dev):
                self.model = self.model.to(dev); return self

        wrapped_model = ModelWrapper(model).eval()
        logger.info(f"âœ“ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ - Device: {device}")
        return wrapped_model

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise

def prepare_test_dataset(
    csv_input: str,
    batch_size: int,
    max_text_length: str | int,
    crop_strategy: str,
    lm_name: str,
    num_workers: int = 0,
    overlap_ratio: float = 0.5,
    *,
    image_size: Tuple[int, int] | List[int] | None = None,
    fov_deg: float = 90.0,
    image_mean: Optional[List[float]] = None,
    image_std: Optional[List[float]] = None,
    anyres_patch_size: Optional[int] = None,  # Noneì´ë©´ image_sizeì—ì„œ ìë™ ì¶”ë¡ 
    anyres_max_patches: int = 12,
    normalize: bool = True,
    vision_name: Optional[str] = None,
    system_msg: Optional[str] = None,
    use_vision_processor: bool = True,
    auto_max_text_length_cap: Optional[int] = None,
    auto_max_text_length_floor: Optional[int] = None,
    auto_max_text_length_scan_limit: Optional[int] = None
) -> Tuple[VLMDataModule, Any]:
    """
    2ë‹¨ê³„: ChatPanoTestDatasetê³¼ VLMDataModuleì„ í™œìš©í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    - config.jsonì˜ image_processing/ training ë‚´ìš©ì„ ì¸ìí™”í•˜ì—¬ ë°˜ì˜
    """
    logger.info("=" * 60)
    logger.info("ğŸ“Š 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„")
    logger.info("=" * 60)

    logger.info(f"ğŸ“‚ CSV ì…ë ¥: {csv_input}")
    system_msg = system_msg or "You are an expert assistant specialized in analyzing panoramic images. Please provide detailed, accurate, and helpful responses about what you observe in the panoramic view shortly."

    # Normalize image_size
    _img_size = None
    if image_size is not None:
        try:
            if isinstance(image_size, (list, tuple)) and len(image_size) == 2:
                _img_size = (int(image_size[0]), int(image_size[1]))
        except Exception:
            _img_size = None

    datamodule = VLMDataModule(
        csv_train=csv_input,
        csv_val=csv_input,  # í‰ê°€ìš©ìœ¼ë¡œ ë™ì¼í•œ íŒŒì¼ ì‚¬ìš©
        batch_size=batch_size,
        num_workers=num_workers,
        tokenizer_name=lm_name,
        max_text_length=max_text_length,
        image_size=_img_size or (224, 224),
        crop_strategy=crop_strategy,
        eval_mode=True,
        system_msg=system_msg,
        overlap_ratio=overlap_ratio,
        fov_deg=fov_deg,
        image_mean=image_mean,
        image_std=image_std,
        anyres_patch_size=anyres_patch_size,
        anyres_max_patches=anyres_max_patches,
        normalize=normalize,
        vision_model_name=vision_name,
        use_vision_processor=use_vision_processor,
        auto_max_text_length_cap=int(auto_max_text_length_cap) if auto_max_text_length_cap is not None else 8192,
        auto_max_text_length_floor=int(auto_max_text_length_floor) if auto_max_text_length_floor is not None else None,
        auto_max_text_length_scan_limit=int(auto_max_text_length_scan_limit) if auto_max_text_length_scan_limit is not None else None
    )

    datamodule.setup()
    test_dataloader = datamodule.val_dataloader()

    logger.info(f"âœ“ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
    logger.info(f"   - ì´ ë°°ì¹˜ ìˆ˜: {len(test_dataloader)}")
    logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    logger.info(f"   - í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´ (requested): {max_text_length}")
    logger.info(f"   - í¬ë¡­ ì „ëµ: {crop_strategy}")
    logger.info(f"   - ê²¹ì¹¨ ë¹„ìœ¨: {overlap_ratio}")
    logger.info(f"   - ì›Œì»¤ ìˆ˜: {num_workers}")
    logger.info(f"   - Vision ëª¨ë¸: {vision_name}")
    logger.info(f"   - ì´ë¯¸ì§€ í¬ê¸°: {(_img_size or (224, 224))}")
    logger.info(f"   - fov_deg: {fov_deg}")
    logger.info(f"   - normalize: {normalize} | use_vision_processor: {use_vision_processor}")
    if image_mean is not None and image_std is not None:
        logger.info(f"   - image_mean/std: {image_mean} / {image_std}")
    logger.info(f"   - use_vision_processor: {use_vision_processor}")

    return datamodule, test_dataloader

def generate_predictions(
    model: Any,
    test_dataloader,
    datamodule: VLMDataModule,
    device: torch.device,
    *,
    max_new_tokens: int = 32,
    temperature: float = 0.6,
    top_p: float = 0.95,
    top_k: int = 20,
    min_p: float = 0.0,
    repetition_penalty: float = 1.1,
    length_penalty: float = 1.0,
    min_new_tokens: int = 5,
    system_msg: Optional[str] = None,
    max_samples: Optional[int] = None,
    log_samples: bool = True,
    log_interval: int = 25,
    log_max_samples: int = 50,
) -> Tuple[List[str], List[str], List[str], List[str]]:
    """
    3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë°°ì¹˜ë³„ í…ìŠ¤íŠ¸ ìƒì„±
    - config.training.system_msg(ë˜ëŠ” system_messages.default) ë¥¼ UniversalTextFormatterì— ë°˜ì˜
    """
    logger.info("=" * 60)
    logger.info("ğŸ¤– 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ìƒì„± (UniversalTextFormatter í™œìš©)")
    logger.info("=" * 60)

    predictions, references, image_paths, input_texts = [], [], [], []

    tokenizer = datamodule.tokenizer
    sys_msg = system_msg or "You are an expert assistant specialized in analyzing panoramic images. Please provide detailed, accurate, and helpful responses about what you observe in the panoramic view shortly."
    text_formatter = UniversalTextFormatter(
        tokenizer,
        system_msg=sys_msg
    )

    logger.info(f"ğŸ¯ ìƒì„± íŒŒë¼ë¯¸í„° - Max tokens: {max_new_tokens}, Min tokens: {min_new_tokens}, Temperature: {temperature}, Top P: {top_p}, Top K: {top_k}")
    logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ í¬ë§·í„° - ëª¨ë¸: {text_formatter.model_family} ({'Instruct' if text_formatter.is_instruct else 'Base'})")
    if max_samples is not None:
        logger.info(f"ğŸ”¢ ìµœëŒ€ í‰ê°€ ìƒ˜í”Œ ìˆ˜ ì œí•œ: {max_samples}")
    if not log_samples:
        logger.info("ğŸ›‘ ìƒì„¸ ìƒ˜í”Œ ë¡œê·¸ ë¹„í™œì„±í™” ( --log-samples ë¡œ í™œì„±í™” ê°€ëŠ¥ )")

    with torch.no_grad():
        total_logged_samples = 0
        for batch_idx, batch in enumerate(tqdm(test_dataloader, desc="ìƒì„± ì¤‘")):
            try:
                pixel_values = batch["pixel_values"].to(device, non_blocking=True)
                input_ids = batch.get("input_ids")
                if input_ids is not None:
                    input_ids = input_ids.to(device, non_blocking=True)
                attention_mask = batch.get("attention_mask")
                if attention_mask is not None:
                    attention_mask = attention_mask.to(device, non_blocking=True)

                batch_size = pixel_values.shape[0]

                # ê°„ì†Œí™”ëœ ì •ë‹µÂ·ë©”íƒ€ ì¶”ì¶œ
                batch_references = []
                if "reference" in batch:
                    refs = batch["reference"]
                    batch_references = [str(r).strip() for r in (refs if isinstance(refs, list) else [refs]*batch_size)]
                else:
                    batch_references = [f"no_reference_{i}" for i in range(batch_size)]

                batch_image_paths = batch.get("image_path", [f"batch_{batch_idx}_sample_{i}" for i in range(batch_size)])
                batch_input_texts = batch.get("original_query", batch.get("input_text", [f"no_query_{i}" for i in range(batch_size)]))
                if not isinstance(batch_input_texts, list):
                    batch_input_texts = [batch_input_texts] * batch_size

                generation_config = text_formatter.get_generation_config()
                gen_kwargs = {
                    "pixel_values": pixel_values,
                    "input_ids": input_ids,
                    "attention_mask": attention_mask,
                    "max_new_tokens": max_new_tokens,
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": top_k,
                    "min_p": min_p,
                    "repetition_penalty": repetition_penalty,
                    "length_penalty": length_penalty,
                    "min_new_tokens": min_new_tokens,
                    "do_sample": True,
                    "pad_token_id": tokenizer.pad_token_id,
                    "eos_token_id": tokenizer.eos_token_id,
                }
                if hasattr(model, 'model') and hasattr(model.model, 'generation_config'):
                    if hasattr(model.model.generation_config, 'stop_strings'):
                        gen_kwargs["stop_strings"] = generation_config["stop_strings"][:3]

                if hasattr(model, 'model') and hasattr(model.model, 'generate'):
                    output = model.model.generate(**gen_kwargs)
                elif hasattr(model, 'generate'):
                    output = model.generate(**gen_kwargs)
                else:
                    raise AttributeError("ëª¨ë¸ì— generate ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")

                batch_predictions = []
                if isinstance(output, torch.Tensor):
                    for i in range(batch_size):
                        input_length = input_ids[i].shape[0] if input_ids is not None else 0
                        generated_tokens = output[i][input_length:]
                        raw_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                        clean_prediction = text_formatter.extract_assistant_response(raw_text)
                        batch_predictions.append(clean_prediction)
                elif isinstance(output, dict) and "text" in output:
                    for raw_text in output["text"]:
                        clean_prediction = text_formatter.extract_assistant_response(raw_text)
                        batch_predictions.append(clean_prediction)
                else:
                    logger.warning(f"Unexpected output format: {type(output)}")
                    batch_predictions = ["[ìƒì„± ì‹¤íŒ¨]"] * batch_size

                # í¬ê¸° ì •í•©
                if len(batch_predictions) != batch_size:
                    if len(batch_predictions) < batch_size:
                        batch_predictions.extend(["[í¬ê¸° ë¶€ì¡±]"] * (batch_size - len(batch_predictions)))
                    else:
                        batch_predictions = batch_predictions[:batch_size]

                # ì •ë¦¬
                cleaned_predictions = []
                for pred in batch_predictions:
                    cleaned_predictions.append(pred.strip().replace('\n\n', '\n') if pred and pred.strip() else "[ë¹ˆ ì‘ë‹µ]")

                # ë¡œê·¸ & ì¶•ì 
                should_log_batch = log_samples and (
                    batch_idx == 0
                    or (log_interval > 0 and (batch_idx + 1) % log_interval == 0)
                )
                if should_log_batch and total_logged_samples < log_max_samples:
                    logger.info(f"=== ë°°ì¹˜ {batch_idx} ê²°ê³¼ ë¡œê·¸ ===")
                    for i, (pred, ref) in enumerate(zip(cleaned_predictions, batch_references)):
                        if total_logged_samples >= log_max_samples:
                            break
                        logger.info(f"  ìƒ˜í”Œ {len(predictions) + i}")
                        logger.info(f"    ì˜ˆì¸¡: '{pred}'")
                        logger.info(f"    ì •ë‹µ: '{ref}'")
                        total_logged_samples += 1
                    logger.info(f"==========================")

                predictions.extend(cleaned_predictions)
                references.extend(batch_references)
                image_paths.extend(batch_image_paths)
                input_texts.extend(batch_input_texts)

                if max_samples is not None and len(predictions) >= max_samples:
                    overflow = len(predictions) - max_samples
                    if overflow > 0:
                        del predictions[-overflow:]
                        del references[-overflow:]
                        del image_paths[-overflow:]
                        del input_texts[-overflow:]
                    logger.info(f"ğŸ“‰ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ {max_samples}ì— ë„ë‹¬í•˜ì—¬ ì¡°ê¸° ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break

                if batch_idx % 10 == 0:
                    logger.info(f"ì§„í–‰: {batch_idx + 1}/{len(test_dataloader)} ë°°ì¹˜ ì™„ë£Œ ({len(predictions)} ìƒ˜í”Œ)")

            except Exception as e:
                logger.error(f"ë°°ì¹˜ {batch_idx} ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                bs = pixel_values.shape[0] if 'pixel_values' in locals() else 1
                predictions.extend([f"[ë°°ì¹˜ ì˜¤ë¥˜_{i}]" for i in range(bs)])
                references.extend(batch_references if 'batch_references' in locals() else [f"[ì •ë‹µ ì—†ìŒ_{i}]" for i in range(bs)])
                image_paths.extend(batch_image_paths if 'batch_image_paths' in locals() else [f"error_batch_{batch_idx}_sample_{i}" for i in range(bs)])
                input_texts.extend(batch_input_texts if 'batch_input_texts' in locals() else [f"error_input_{i}" for i in range(bs)])
                continue

        if max_samples is not None and len(predictions) > max_samples:
            overflow = len(predictions) - max_samples
            del predictions[-overflow:]
            del references[-overflow:]
            del image_paths[-overflow:]
            del input_texts[-overflow:]

    logger.info(f"âœ“ í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ! ì´ ìƒ˜í”Œ ìˆ˜: {len(predictions)}")
    return predictions, references, image_paths, input_texts



def save_and_log_results(
    predictions: List[str],
    references: List[str],
    image_paths: List[str],
    input_texts: List[str],
    output_dir: Path,
    timestamp: str,
    prefix: str,
) -> pd.DataFrame:
    """
    4ë‹¨ê³„: ìƒì„±ëœ ë‹µë³€ê³¼ ì •ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ê³  ë¡œê¹… (ê°œì„ ëœ ë¶„ì„ í¬í•¨)
    """
    logger.info("=" * 60)
    logger.info("ğŸ’¾ 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ë¶„ì„")
    logger.info("=" * 60)
    
    # ê°œì„ ëœ CSV ë°ì´í„° ì¤€ë¹„
    results_data = []
    for i, (pred, ref, img_path) in enumerate(zip(predictions, references, image_paths)):
        # ë¹ˆ ê°’ ì²˜ë¦¬ ë° ê¸°ë³¸ ì •ë¦¬
        pred_str = str(pred).strip() if pred is not None else ""
        ref_str = str(ref).strip() if ref is not None else ""
        img_path_str = str(img_path) if img_path is not None else ""
        
        # ì˜ˆì¸¡ê°’ í’ˆì§ˆ ë¶„ì„
        is_error = pred_str.startswith('[') and pred_str.endswith(']')
        is_empty = not pred_str or pred_str in ["", "[ë¹ˆ ì‘ë‹µ]"]
        
        # input_text ì¶”ì¶œ (ì¸ë±ìŠ¤ í™•ì¸ í›„ ì•ˆì „í•˜ê²Œ)
        input_text_str = ""
        if i < len(input_texts):
            input_text_str = str(input_texts[i]).strip() if input_texts[i] is not None else ""
        
        results_data.append({
            'sample_id': i,
            'image_path': img_path_str,
            'original_query': input_text_str,
            'prediction': pred_str,
            'reference': ref_str,
            'pred_length': len(pred_str.split()),
            'ref_length': len(ref_str.split()),
            'is_error': is_error,
            'is_empty': is_empty
        })
    
    # DataFrame ìƒì„± ë° ì €ì¥
    df = pd.DataFrame(results_data)
    safe_prefix = prefix if prefix else "model"
    csv_path = output_dir / f"{safe_prefix}_predictions_{timestamp}.csv"
    df.to_csv(csv_path, index=False, encoding='utf-8')
    
    # ê°œì„ ëœ ê²°ê³¼ í†µê³„ ë¶„ì„
    total_samples = len(df)
    error_count = df['is_error'].sum()
    empty_count = df['is_empty'].sum()
    valid_count = total_samples - error_count - empty_count
    
    # ê¸¸ì´ í†µê³„ (ìœ íš¨í•œ ì˜ˆì¸¡ê°’ë§Œ)
    valid_df = df[~df['is_error'] & ~df['is_empty']]
    if len(valid_df) > 0:
        avg_pred_length = valid_df['pred_length'].mean()
        avg_ref_length = valid_df['ref_length'].mean()
        pred_length_std = valid_df['pred_length'].std()
    else:
        avg_pred_length = avg_ref_length = pred_length_std = 0.0
    
    logger.info(f"ğŸ“Š ìƒì„± í’ˆì§ˆ ë¶„ì„:")
    logger.info(f"   - ì´ ìƒ˜í”Œ: {total_samples}")
    logger.info(f"   - ì„±ê³µì  ìƒì„±: {valid_count}ê°œ ({valid_count/total_samples*100:.1f}%)")
    logger.info(f"   - ìƒì„± ì˜¤ë¥˜: {error_count}ê°œ ({error_count/total_samples*100:.1f}%)")
    logger.info(f"   - ë¹ˆ ì‘ë‹µ: {empty_count}ê°œ ({empty_count/total_samples*100:.1f}%)")
    
    if valid_count > 0:
        logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„:")
        logger.info(f"   - í‰ê·  ì˜ˆì¸¡ ê¸¸ì´: {avg_pred_length:.1f} Â± {pred_length_std:.1f} ë‹¨ì–´")
        logger.info(f"   - í‰ê·  ì •ë‹µ ê¸¸ì´: {avg_ref_length:.1f} ë‹¨ì–´")
        logger.info(f"   - ê¸¸ì´ ë¹„ìœ¨ (ì˜ˆì¸¡/ì •ë‹µ): {avg_pred_length/avg_ref_length:.2f}")
    
    logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
    return df
    


def basic_cleanup(text: str) -> str:
    """
    Level 1: ê¸°ë³¸ ì •ë¦¬ - ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë§Œ ì œê±° (ì˜ë¯¸ ë³´ì¡´)

    - íŠ¹ìˆ˜ í† í° ì œê±° (<image>, <|im_start|> ë“±)
    - ì—­í•  íƒœê·¸ ì œê±° (ASSISTANT:, USER: ë“±)
    - <think> íƒœê·¸ ë° ë‚´ìš© ì™„ì „ ì œê±°
    - ë©”íƒ€ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±° ("Okay, let's...", "First, I need to..." ë“±)
    - í”„ë¡¬í”„íŠ¸ ëˆ„ìˆ˜ ì œê±°
    - ê³¼ë„í•œ ê³µë°± ì •ë¦¬

    ëŒ€ì†Œë¬¸ì, êµ¬ë‘ì ì€ ë³´ì¡´í•˜ì—¬ ì‹¤ì œ í’ˆì§ˆì„ ë°˜ì˜í•©ë‹ˆë‹¤.
    """
    if not text or pd.isna(text):
        return ""

    text = str(text)

    # 1. <think>...</think> íƒœê·¸ì™€ ë‚´ìš© ì™„ì „ ì œê±° (ì¤„ë°”ê¿ˆ í¬í•¨)
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
    # ë‚¨ì€ </think> íƒœê·¸ë„ ì œê±°
    text = re.sub(r'</think>', '', text, flags=re.IGNORECASE)

    # 2. ë©”íƒ€ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±° (ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •)
    # "Okay, let's..." í˜•íƒœì˜ ë¬¸ì¥ ì œê±°
    text = re.sub(r'^(Okay|Alright|Well|So),?\s+(let\'?s?|I\'?ll?|we\'?ll?)\s+.*?\.\s*', '', text, flags=re.IGNORECASE)
    # "First, I need to..." í˜•íƒœì˜ ë¬¸ì¥ ì œê±°  
    text = re.sub(r'^(First|Then|Next|Now),?\s+(I|we)\s+(need to|should|will|can)\s+.*?\.\s*', '', text, flags=re.IGNORECASE)
    # "The user mentioned..." í˜•íƒœì˜ ë¬¸ì¥ ì œê±°
    text = re.sub(r'^The (user|question|query|prompt)\s+(mentioned|asked|provided|wants).*?\.\s*', '', text, flags=re.IGNORECASE)
    # "Looking at..." í˜•íƒœì˜ ë¬¸ì¥ ì œê±°
    text = re.sub(r'^(Looking at|Analyzing|Examining|Considering)\s+.*?\.\s*', '', text, flags=re.IGNORECASE)

    # 3. íŠ¹ìˆ˜ í† í° ì œê±°
    text = re.sub(r"<\|.*?\|>|<image>|</image>|<img>|</img>", " ", text, flags=re.I)
    text = re.sub(r"<vision_start>|<vision_end>|<image_pad>", " ", text, flags=re.I)

    # 4. ì—­í•  íƒœê·¸ ì œê±° (ë¬¸ì¥ ì‹œì‘ ë¶€ë¶„ì—ì„œ)
    text = re.sub(r"^(USER:|ASSISTANT:|Question:|Answer:)\s*", "", text, flags=re.I)

    # 5. ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+", " ", text).strip()

    return text


def calculate_evaluation_metrics(data_input, output_dir: Path, timestamp: str, prefix: str) -> Dict[str, float]:
    """
    5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (BLEU-4, METEOR, ROUGE-L, SPICE, CIDEr)

    Args:
        data_input: pandas DataFrame ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œ (str/Path)
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´
        prefix: ê²°ê³¼ íŒŒì¼ ì ‘ë‘ì–´

    Changes:
        - sacrebleu ì‚¬ìš© (í‘œì¤€ í† í°í™”, ì¬í˜„ ê°€ëŠ¥í•œ BLEU)
        - basic_cleanupìœ¼ë¡œ íŠ¹ìˆ˜ í† í°/ì—­í•  íƒœê·¸ ì œê±°
        - ëŒ€ì†Œë¬¸ì/êµ¬ë‘ì  ë³´ì¡´ (ì‹¤ì œ í’ˆì§ˆ ë°˜ì˜)
    """
    logger.info("=" * 60)
    logger.info("ğŸ“ˆ 5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°")
    logger.info("=" * 60)
    
    # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬: CSV íŒŒì¼ì´ë©´ DataFrameìœ¼ë¡œ ë³€í™˜
    if isinstance(data_input, (str, Path)):
        csv_path = Path(data_input)
        if not csv_path.exists():
            raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        
        logger.info(f"ğŸ“‚ CSV íŒŒì¼ ë¡œë“œ: {csv_path}")
        df = pd.read_csv(csv_path, encoding='utf-8')
        logger.info(f"âœ“ DataFrame ë³€í™˜ ì™„ë£Œ - ì´ {len(df)}ê°œ ìƒ˜í”Œ")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['prediction', 'reference']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"CSV íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}. í•„ìš”í•œ ì»¬ëŸ¼: {required_columns}")
        
        # ì˜µì…˜ ì»¬ëŸ¼ í™•ì¸ ë° ë¡œê·¸
        optional_columns = ['image_path']
        available_optional = [col for col in optional_columns if col in df.columns]
        logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: í•„ìˆ˜ {required_columns} + ì„ íƒ {available_optional}")
    
    elif isinstance(data_input, pd.DataFrame):
        df = data_input
        logger.info(f"âœ“ DataFrame ì…ë ¥ - ì´ {len(df)}ê°œ ìƒ˜í”Œ")
    else:
        raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {type(data_input)}. pandas DataFrame ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    # ìœ íš¨í•œ ìƒ˜í”Œë§Œ ì„ íƒ (ì˜ˆì¸¡ê³¼ ì •ë‹µê°€ ëª¨ë‘ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°)
    valid_df = df[(df['prediction'].str.strip() != '') & (df['reference'].str.strip() != '')]
    
    if len(valid_df) == 0:
        logger.error("âŒ ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    logger.info(f"ğŸ“Š í‰ê°€ ëŒ€ìƒ: {len(valid_df)}/{len(df)} ìƒ˜í”Œ")
    
    # ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (NaN ê°’ ì²˜ë¦¬)
    raw_predictions = [str(pred) if pred is not None and not pd.isna(pred) else "" for pred in valid_df['prediction'].tolist()]
    raw_references = [str(ref) if ref is not None and not pd.isna(ref) else "" for ref in valid_df['reference'].tolist()]

    # Level 1 ì •ë¦¬: íŠ¹ìˆ˜ í† í°, ì—­í•  íƒœê·¸ ì œê±°
    logger.info("ğŸ§¹ í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ (íŠ¹ìˆ˜ í† í°/ì—­í•  íƒœê·¸ ì œê±°)...")
    predictions = [basic_cleanup(pred) for pred in raw_predictions]
    references = [basic_cleanup(ref) for ref in raw_references]

    # "Assistant:" ë¶€ë¶„ ì²˜ë¦¬ (ì´ë¯¸ basic_cleanupì—ì„œ ì œê±°ë˜ì§€ë§Œ ì¶”ê°€ ì²´í¬)
    ref_texts_cleaned = []
    for ref in references:
        if "Assistant:" in ref:
            assistant_part = ref.split("Assistant:")[-1].strip()
            ref_texts_cleaned.append(assistant_part)
        else:
            ref_texts_cleaned.append(ref)
    references = ref_texts_cleaned

    # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
    valid_pairs = [(pred, ref) for pred, ref in zip(predictions, references) if pred.strip() and ref.strip()]

    if not valid_pairs:
        logger.error("âŒ ìœ íš¨í•œ ì˜ˆì¸¡-ì •ë‹µ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
        return {}

    predictions, references = zip(*valid_pairs)
    predictions = list(predictions)
    references = list(references)

    logger.info(f"ğŸ“Š ìµœì¢… í‰ê°€ ëŒ€ìƒ: {len(valid_pairs)} ìƒ˜í”Œ")
    logger.info(f"ğŸ“ ì˜ˆì‹œ - ì˜ˆì¸¡: '{predictions[0][:100]}...'")
    logger.info(f"ğŸ“ ì˜ˆì‹œ - ì •ë‹µ: '{references[0][:100]}...'")

    metrics = {}
    
    # 1. BLEU-4 ê³„ì‚° (sacrebleu ê³µì‹ ë ˆí¬ì§€í† ë¦¬ ì‚¬ìš©)
    try:
        import sacrebleu

        # sacrebleuëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
        if len(predictions) == 0 or len(references) == 0:
            logger.warning("âš ï¸ BLEU-4: ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['bleu4'] = 0.0
        else:
            logger.info("ğŸ“Š BLEU-4 ê³„ì‚° ì¤‘...")
            try:
                # sacrebleu ê³„ì‚° (í‘œì¤€ ì„¤ì •)
                # ê³µì‹ ë ˆí¬ì§€í† ë¦¬: https://github.com/mjpost/sacrebleu
                bleu = sacrebleu.corpus_bleu(
                    predictions,
                    [references],           # ì°¸ì¡°ëŠ” ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸
                    smooth_method="exp",    # í‘œì¤€ ìŠ¤ë¬´ë”©
                    lowercase=False,        # ëŒ€ì†Œë¬¸ì ë³´ì¡´ (ì‹¤ì œ í’ˆì§ˆ ë°˜ì˜)
                    tokenize="13a",         # Moses í† í¬ë‚˜ì´ì € (í•™ìˆ  í‘œì¤€)
                    use_effective_order=True  # ì§§ì€ ë¬¸ì¥ ì•ˆì •í™”
                )
                metrics['bleu4'] = bleu.score / 100.0  # 0~1 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
                logger.info(f"âœ“ BLEU-4 (ê³µì‹ sacrebleu): {metrics['bleu4']:.4f}")
                logger.info(f"  â†’ í† í°í™”: 13a (Moses), ìŠ¤ë¬´ë”©: exp, ëŒ€ì†Œë¬¸ì: ë³´ì¡´")
            except Exception as bleu_e:
                logger.warning(f"âš ï¸ sacrebleu ê³„ì‚° ì˜¤ë¥˜: {bleu_e}")
                # BLEU í´ë°±: NLTK ì‚¬ìš©
                logger.info("BLEU-4 í´ë°±: NLTK ì‚¬ìš©...")
                try:
                    from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

                    ref_tokens = [[ref.split()] for ref in references if ref.strip()]
                    pred_tokens = [pred.split() for pred in predictions if pred.strip()]

                    if len(ref_tokens) == 0 or len(pred_tokens) == 0:
                        logger.warning("âš ï¸ BLEU-4: ìœ íš¨í•œ í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
                        metrics['bleu4'] = 0.0
                    else:
                        smoothing = SmoothingFunction().method1
                        metrics['bleu4'] = corpus_bleu(ref_tokens, pred_tokens, 
                                                       weights=(0.25, 0.25, 0.25, 0.25), 
                                                       smoothing_function=smoothing)
                        logger.info(f"âœ“ BLEU-4 (NLTK í´ë°±): {metrics['bleu4']:.4f}")
                except Exception as nltk_bleu_e:
                    logger.error(f"âŒ NLTK BLEU-4ë„ ì‹¤íŒ¨: {nltk_bleu_e}")
                    metrics['bleu4'] = 0.0
                    
    except ImportError:
        logger.error("âŒ sacrebleuë¥¼ ì„¤ì¹˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.error("   ê¶Œì¥ ì„¤ì¹˜: pip install sacrebleu")
        logger.error("   github: https://github.com/mjpost/sacrebleu")
        logger.info("BLEU-4 í´ë°±: NLTK ì‚¬ìš©...")
        try:
            from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

            ref_tokens = [[ref.split()] for ref in references if ref.strip()]
            pred_tokens = [pred.split() for pred in predictions if pred.strip()]

            if len(ref_tokens) == 0 or len(pred_tokens) == 0:
                logger.warning("âš ï¸ BLEU-4: ìœ íš¨í•œ í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
                metrics['bleu4'] = 0.0
            else:
                smoothing = SmoothingFunction().method1
                metrics['bleu4'] = corpus_bleu(ref_tokens, pred_tokens, 
                                               weights=(0.25, 0.25, 0.25, 0.25), 
                                               smoothing_function=smoothing)
                logger.info(f"âœ“ BLEU-4 (NLTK í´ë°±): {metrics['bleu4']:.4f}")
        except Exception as e:
            logger.error(f"âŒ NLTK BLEU-4ë„ ì‹¤íŒ¨: {e}")
            metrics['bleu4'] = 0.0
    except Exception as e:
        logger.error(f"âŒ BLEU-4 ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['bleu4'] = 0.0
    
    # 2. METEOR ê³„ì‚° (ê³µì‹ NLTK ë ˆí¬ì§€í† ë¦¬ ì‚¬ìš©)
    try:
        import nltk
        try:
            nltk.data.find('corpora/wordnet')
        except LookupError:
            logger.info("NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘ (wordnet, punkt)...")
            nltk.download('wordnet', quiet=True)
            nltk.download('punkt', quiet=True)

        from nltk.translate.meteor_score import meteor_score

        logger.info("ğŸ“Š METEOR ê³„ì‚° ì¤‘...")
        meteor_scores = []
        batch_size = 500
        
        # ë°°ì¹˜ë³„ ì²˜ë¦¬ (ì§„í–‰ìƒí™© í‘œì‹œ)
        for idx, (ref, pred) in enumerate(zip(references, predictions)):
            if (idx + 1) % batch_size == 0:
                logger.info(f"  ì²˜ë¦¬ ì¤‘: {idx + 1}/{len(references)}")
            
            if ref.strip() and pred.strip():  # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                ref_tokens = ref.split()
                pred_tokens = pred.split()
                if len(ref_tokens) > 0 and len(pred_tokens) > 0:
                    try:
                        score = meteor_score([ref_tokens], pred_tokens)
                        meteor_scores.append(score)
                    except Exception as item_e:
                        logger.debug(f"  ìƒ˜í”Œ {idx} METEOR ê³„ì‚° ì˜¤ë¥˜: {item_e}")
                        meteor_scores.append(0.0)

        if meteor_scores:
            metrics['meteor'] = float(np.mean(meteor_scores))
            logger.info(f"âœ“ METEOR (ê³µì‹ NLTK): {metrics['meteor']:.4f}")
        else:
            logger.warning("âš ï¸ METEOR: ìœ íš¨í•œ ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['meteor'] = 0.0
            
    except ImportError:
        logger.error("âŒ NLTKë¥¼ ì„¤ì¹˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.error("   ì„¤ì¹˜: pip install nltk")
        metrics['meteor'] = 0.0
    except Exception as e:
        logger.error(f"âŒ METEOR ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['meteor'] = 0.0

    # 3. ROUGE-L ê³„ì‚° (ê³µì‹ rouge-score ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
    try:
        from rouge_score import rouge_scorer
        
        logger.info("ğŸ“Š ROUGE-L ê³„ì‚° ì¤‘ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬)...")
        
        # ROUGEëŠ” ë§¤ìš° í° ë°ì´í„°ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°°ì¹˜ ì²˜ë¦¬
        rouge_scores = []
        batch_size = 100
        
        # ë°°ì¹˜ë³„ ì²˜ë¦¬
        for batch_idx in range(0, len(predictions), batch_size):
            batch_end = min(batch_idx + batch_size, len(predictions))
            batch_preds = predictions[batch_idx:batch_end]
            batch_refs = references[batch_idx:batch_end]
            
            if (batch_idx + batch_size) % 500 == 0:
                logger.info(f"  ì²˜ë¦¬ ì¤‘: {batch_end}/{len(predictions)}")
            
            # ê° ë°°ì¹˜ë§ˆë‹¤ ìƒˆë¡œìš´ scorer ìƒì„± (ë©”ëª¨ë¦¬ ê´€ë¦¬)
            scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
            
            for ref, pred in zip(batch_refs, batch_preds):
                if ref.strip() and pred.strip():  # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                    try:
                        scores = scorer.score(ref, pred)
                        rouge_scores.append(scores['rougeL'].fmeasure)
                    except Exception as item_e:
                        # ê°œë³„ ìƒ˜í”Œ ì˜¤ë¥˜ëŠ” ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰
                        logger.debug(f"  ìƒ˜í”Œ ROUGE-L ê³„ì‚° ì˜¤ë¥˜: {item_e}")
                        rouge_scores.append(0.0)

        if rouge_scores:
            metrics['rougeL'] = float(np.mean(rouge_scores))
            logger.info(f"âœ“ ROUGE-L (ê³µì‹ rouge-score): {metrics['rougeL']:.4f}")
        else:
            logger.warning("âš ï¸ ROUGE-L: ìœ íš¨í•œ ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['rougeL'] = 0.0
            
    except ImportError:
        logger.error("âŒ rouge-scoreë¥¼ ì„¤ì¹˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.error("   ì„¤ì¹˜: pip install rouge-score")
        metrics['rougeL'] = 0.0
    except Exception as e:
        logger.error(f"âŒ ROUGE-L ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['rougeL'] = 0.0
    
    # 4. SPICE ê³„ì‚° (pycocoevalcap ê³µì‹ ë ˆí¬ì§€í† ë¦¬ ì‚¬ìš© - Java 11+ í˜¸í™˜ì„±)
    try:
        import os
        import subprocess
        from pycocoevalcap.spice.spice import Spice
        
        logger.info("ğŸ“Š SPICE ê³„ì‚° ì‹œì‘...")
        logger.info(f"   ì´ ìƒ˜í”Œ ìˆ˜: {len(predictions)}")
        
        # Java ë²„ì „ í™•ì¸ ë° ì ì ˆí•œ ì˜µì…˜ ì„¤ì •
        java_version = 8  # ê¸°ë³¸ê°’
        try:
            java_version_output = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT, text=True)
            logger.info(f"   Java ë²„ì „: {java_version_output.split('\\n')[0].strip()}")
            
            # ë²„ì „ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: "1.8.0" -> 8, "11.0.1" -> 11, "21.0.8" -> 21)
            import re
            version_match = re.search(r'version "(\d+)\.?(\d*)', java_version_output)
            if version_match:
                major_version = version_match.group(1)
                if major_version == "1":  # Java 8 í˜•ì‹: "1.8.0"
                    java_version = int(version_match.group(2))
                else:  # Java 9+ í˜•ì‹: "11.0.1", "21.0.8"
                    java_version = int(major_version)
                logger.info(f"   ê°ì§€ëœ Java ë©”ì´ì € ë²„ì „: {java_version}")
        except Exception as jv_e:
            logger.warning(f"   Java ë²„ì „ í™•ì¸ ì‹¤íŒ¨: {jv_e}, Java 8ë¡œ ê°€ì •í•©ë‹ˆë‹¤")
        
        # Java ë²„ì „ë³„ ì˜µì…˜ ì„¤ì •
        if java_version >= 9:
            # Java 9+ Module System í˜¸í™˜ì„± ì„¤ì •
            # SPICEì˜ FST ì§ë ¬í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë¦¬í”Œë ‰ì…˜ìœ¼ë¡œ java.base íŒ¨í‚¤ì§€ ì ‘ê·¼ ì‹œ ì œí•œë¨
            # _JAVA_OPTIONS ì‚¬ìš© (java -jar ëª…ë ¹ì— ì „ë‹¬ë¨, JAVA_TOOL_OPTIONSëŠ” ë¬´ì‹œë¨)
            logger.info(f"   Java {java_version} ê°ì§€ - Module System í˜¸í™˜ì„± ì˜µì…˜ ì ìš©")
            java_opts = (
                '-Xmx8G '
                '--add-opens=java.base/java.lang=ALL-UNNAMED '
                '--add-opens=java.base/java.util=ALL-UNNAMED '
                '--add-opens=java.base/java.io=ALL-UNNAMED '
                '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED '
                '--add-opens=java.base/java.text=ALL-UNNAMED '
                '--add-opens=java.base/java.math=ALL-UNNAMED '
                '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED '
                '--add-opens=java.base/java.net=ALL-UNNAMED '
                '--add-opens=java.desktop/java.awt.font=ALL-UNNAMED'
            ).strip()
            os.environ['_JAVA_OPTIONS'] = java_opts
        else:
            # Java 8 - --add-opens ì˜µì…˜ ë¶ˆí•„ìš” (ì˜¤íˆë ¤ ì—ëŸ¬ ë°œìƒ)
            logger.info(f"   Java {java_version} ê°ì§€ - ê¸°ë³¸ ë©”ëª¨ë¦¬ ì„¤ì •ë§Œ ì ìš©")
            os.environ['_JAVA_OPTIONS'] = '-Xmx8G'
        
        # JAVA_TOOL_OPTIONS ì œê±° (java -jarì—ì„œ ì¶©ëŒ ë°©ì§€)
        if 'JAVA_TOOL_OPTIONS' in os.environ:
            del os.environ['JAVA_TOOL_OPTIONS']
        
        logger.info(f"   _JAVA_OPTIONS ì„¤ì • ì™„ë£Œ (Java 21 í˜¸í™˜ì„±)")
        
        spice_scorer = Spice()
        logger.info("   âœ“ Spice scorer ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§ (ìŒì„ ìœ ì§€í•˜ë©´ì„œ í•„í„°ë§)
        valid_pairs_for_spice = [(pred, ref) for pred, ref in zip(predictions, references) 
                                  if pred.strip() and ref.strip()]
        
        if len(valid_pairs_for_spice) == 0:
            logger.warning("âš ï¸ SPICE: ìœ íš¨í•œ í…ìŠ¤íŠ¸ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
            metrics['spice'] = 0.0
        else:
            valid_preds_for_spice, valid_refs_for_spice = zip(*valid_pairs_for_spice)
            valid_preds_for_spice = list(valid_preds_for_spice)
            valid_refs_for_spice = list(valid_refs_for_spice)
            
            logger.info(f"   SPICE ê³„ì‚°: {len(valid_pairs_for_spice)}ê°œ ìœ íš¨ ìƒ˜í”Œ")
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: í† í°í™” ì˜¤ë¥˜ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” ë¬¸ì ì •ë¦¬
            def clean_for_spice(text, max_length=250):
                """SPICE í† í°í™” ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì¶”ê°€ í…ìŠ¤íŠ¸ ì •ë¦¬
                
                Args:
                    text: ì…ë ¥ í…ìŠ¤íŠ¸ (ì´ë¯¸ basic_cleanup ì ìš©ë¨)
                    max_length: ìµœëŒ€ ë¬¸ì ê¸¸ì´ (SPICE ìºì‹œ ì œí•œ)
                
                Note:
                    <think> íƒœê·¸ëŠ” ì´ë¯¸ basic_cleanupì—ì„œ ì œê±°ë˜ì—ˆìŒ
                """
                if not text or not isinstance(text, str):
                    return ""
                
                # ì œì–´ ë¬¸ì ì œê±° (SPICE í† í°í™” ì˜¤ë¥˜ ë°©ì§€)
                text = ''.join(char for char in text if char.isprintable() or char.isspace())
                
                # ì—°ì†ëœ ê³µë°± ì •ë¦¬
                text = ' '.join(text.split())
                
                # ê¸¸ì´ ì œí•œ (ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìë¥´ê¸°)
                if len(text) > max_length:
                    words = text.split()
                    truncated = []
                    current_length = 0
                    for word in words:
                        if current_length + len(word) + 1 > max_length:
                            break
                        truncated.append(word)
                        current_length += len(word) + 1
                    text = ' '.join(truncated)
                    if text:  # ë§ˆì¹¨í‘œ ì¶”ê°€
                        text = text.rstrip('.,!?;:') + '.'
                
                # íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ ë¬¸ìë¥¼ ASCIIë¡œ ê·¼ì‚¬ (SPICEëŠ” ASCIIë§Œ ì²˜ë¦¬)
                text = text.encode('ascii', 'ignore').decode('ascii')
                
                return text.strip()
            
            # ì „ì²˜ë¦¬ ì ìš©
            cleaned_preds = [clean_for_spice(pred) for pred in valid_preds_for_spice]
            cleaned_refs = [clean_for_spice (ref) for ref in valid_refs_for_spice]
            
            # ê¸¸ì´ í†µê³„
            truncated_preds = sum(1 for orig, clean in zip(valid_preds_for_spice, cleaned_preds) if len(orig) > len(clean))
            truncated_refs = sum(1 for orig, clean in zip(valid_refs_for_spice, cleaned_refs) if len(orig) > len(clean))
            if truncated_preds > 0 or truncated_refs > 0:
                logger.info(f"   ğŸ“ ê¸¸ì´ ì œí•œìœ¼ë¡œ ì˜ë¦° í…ìŠ¤íŠ¸: Predictions={truncated_preds}, References={truncated_refs}")
            
            # ì „ì²˜ë¦¬ í›„ ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
            final_pairs = [(pred, ref, i) for i, (pred, ref) in enumerate(zip(cleaned_preds, cleaned_refs))
                          if pred and ref and len(pred.split()) > 0 and len(ref.split()) > 0]
            
            if len(final_pairs) == 0:
                logger.warning("âš ï¸ SPICE: ì „ì²˜ë¦¬ í›„ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
                metrics['spice'] = 0.0
            else:
                filtered_preds, filtered_refs, indices = zip(*final_pairs)
                filtered_preds = list(filtered_preds)
                filtered_refs = list(filtered_refs)
                
                skipped_count = len(valid_pairs_for_spice) - len(final_pairs)
                if skipped_count > 0:
                    logger.warning(f"   âš ï¸ í† í°í™” ë¶ˆê°€ëŠ¥í•œ {skipped_count}ê°œ ìƒ˜í”Œ ì œì™¸ë¨")
                
                logger.info(f"   ìµœì¢… SPICE ê³„ì‚°: {len(final_pairs)}ê°œ ìƒ˜í”Œ")
                
                gts = {str(i): [ref] for i, ref in enumerate(filtered_refs)}
                res = {str(i): [pred] for i, pred in enumerate(filtered_preds)}
            
            logger.info("   compute_score í˜¸ì¶œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            
            try:
                # ì§ì ‘ compute_score í˜¸ì¶œ (pycocoevalcap ê³µì‹ ì¸í„°í˜ì´ìŠ¤)
                spice_score, spice_scores = spice_scorer.compute_score(gts, res)
                metrics['spice'] = float(spice_score)
                logger.info(f"âœ“ SPICE (ê³µì‹ pycocoevalcap): {metrics['spice']:.4f}")
                if skipped_count > 0:
                    logger.info(f"  (ì°¸ê³ : {skipped_count}ê°œ ìƒ˜í”Œ ì œì™¸í•˜ê³  ê³„ì‚°ë¨)")
            except subprocess.CalledProcessError as spice_e:
                # Java í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì‹¤íŒ¨
                logger.error(f"âŒ SPICE Java í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨ (exit code: {spice_e.returncode})")
                logger.error(f"   ëª…ë ¹ì–´: {' '.join(spice_e.cmd[:5])}...")
                
                # SPICE ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œê·¸ íŒŒì¼ í™•ì¸ ì‹œë„
                import glob
                spice_pkg_path = os.path.dirname(os.path.abspath(Spice.__init__.__globals__['__file__']))
                tmp_dir = os.path.join(spice_pkg_path, 'tmp')
                
                if os.path.exists(tmp_dir):
                    # ìµœê·¼ ìƒì„±ëœ íŒŒì¼ë“¤ í™•ì¸
                    recent_files = sorted(glob.glob(os.path.join(tmp_dir, '*')), 
                                        key=os.path.getmtime, reverse=True)[:5]
                    if recent_files:
                        logger.info(f"   SPICE ì„ì‹œ íŒŒì¼ ë””ë ‰í† ë¦¬: {tmp_dir}")
                        logger.info(f"   ìµœê·¼ íŒŒì¼: {[os.path.basename(f) for f in recent_files]}")
                        
                        # JSON ì…ë ¥ íŒŒì¼ ë‚´ìš© í™•ì¸ (ì²« ëª‡ ì¤„)
                        for f in recent_files:
                            if os.path.isfile(f) and os.path.getsize(f) < 10000:  # 10KB ì´í•˜ë§Œ
                                try:
                                    with open(f, 'r', encoding='utf-8', errors='ignore') as tmp_f:
                                        content = tmp_f.read(500)
                                        if content:
                                            logger.debug(f"   íŒŒì¼ {os.path.basename(f)} ë‚´ìš© (ì¼ë¶€):")
                                            logger.debug(f"   {content[:200]}...")
                                except:
                                    pass
                
                # í† í°í™” ì˜¤ë¥˜ë¡œ ì¶”ì •ë˜ëŠ” ê²½ìš° ê°œë³„ ì¬ì‹œë„
                error_msg = str(spice_e)
                logger.warning("âš ï¸ Java ì‹¤í–‰ ì˜¤ë¥˜ ë°œìƒ - ê°œë³„ ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤")
                logger.info("   ëŒ€ì•ˆ: ê°œë³„ ìƒ˜í”Œ SPICE ê³„ì‚° (ëŠë¦¬ì§€ë§Œ ì•ˆì •ì )")
                
                # ê°œë³„ ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ì¬ì‹œë„
                individual_scores = []
                failed_samples = []
                
                # ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì‹œë„ (10ê°œì”©)
                batch_size = 10
                for batch_start in range(0, len(filtered_preds), batch_size):
                    batch_end = min(batch_start + batch_size, len(filtered_preds))
                    batch_preds = filtered_preds[batch_start:batch_end]
                    batch_refs = filtered_refs[batch_start:batch_end]
                    
                    try:
                        # ì‘ì€ ë°°ì¹˜ë¡œ ì‹œë„
                        batch_gts = {str(i): [ref] for i, ref in enumerate(batch_refs)}
                        batch_res = {str(i): [pred] for i, pred in enumerate(batch_preds)}
                        batch_score, batch_scores = spice_scorer.compute_score(batch_gts, batch_res)
                        individual_scores.extend(batch_scores)
                        logger.debug(f"   ë°°ì¹˜ {batch_start}-{batch_end} ì„±ê³µ: í‰ê·  {batch_score:.4f}")
                    except Exception as batch_e:
                        # ë°°ì¹˜ë„ ì‹¤íŒ¨í•˜ë©´ ê°œë³„ë¡œ
                        logger.debug(f"   ë°°ì¹˜ {batch_start}-{batch_end} ì‹¤íŒ¨, ê°œë³„ ì‹œë„...")
                        for i, (pred, ref) in enumerate(zip(batch_preds, batch_refs)):
                            abs_idx = batch_start + i
                            try:
                                mini_gts = {'0': [ref]}
                                mini_res = {'0': [pred]}
                                mini_score, _ = spice_scorer.compute_score(mini_gts, mini_res)
                                individual_scores.append(mini_score)
                            except Exception as sample_e:
                                failed_samples.append((abs_idx, pred[:50], ref[:50], str(sample_e)[:100]))
                
                if individual_scores:
                    metrics['spice'] = float(np.mean(individual_scores))
                    logger.info(f"âœ“ SPICE (ê°œë³„/ë°°ì¹˜ ê³„ì‚°): {metrics['spice']:.4f}")
                    logger.info(f"  ì„±ê³µ: {len(individual_scores)}/{len(filtered_preds)} ìƒ˜í”Œ")
                    if failed_samples:
                        logger.warning(f"  ì‹¤íŒ¨í•œ ìƒ˜í”Œ {len(failed_samples)}ê°œ:")
                        for idx, pred_preview, ref_preview, error in failed_samples[:3]:
                            logger.warning(f"    [{idx}] Pred: {pred_preview}...")
                            logger.warning(f"         Ref: {ref_preview}...")
                            logger.warning(f"         Error: {error}")
                else:
                    logger.error("âŒ ëª¨ë“  ìƒ˜í”Œì—ì„œ SPICE ê³„ì‚° ì‹¤íŒ¨")
                    metrics['spice'] = 0.0
                    
            except Exception as spice_e:
                # ê¸°íƒ€ SPICE ê³„ì‚° ì‹¤íŒ¨ ì‹œ
                logger.error(f"âŒ SPICE ê³„ì‚° ì‹¤íŒ¨: {spice_e}")
                logger.error(f"   ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                
                # ì—ëŸ¬ ë¡œê·¸ì—ì„œ ë¬¸ì œê°€ ë˜ëŠ” í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ ì‹œë„
                error_msg = str(spice_e)
                if "tokenize" in error_msg.lower() or "parse" in error_msg.lower():
                    logger.warning("âš ï¸ í† í°í™” ì˜¤ë¥˜ ë°œìƒ - ì¼ë¶€ ìƒ˜í”Œì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                    logger.info("   ëŒ€ì•ˆ: ê°œë³„ ìƒ˜í”Œ ë‹¨ìœ„ë¡œ SPICE ê³„ì‚° ì‹œë„ ì¤‘...")
                    
                    # ê°œë³„ ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ì¬ì‹œë„
                    individual_scores = []
                    failed_samples = []
                    
                    for i, (pred, ref) in enumerate(zip(filtered_preds, filtered_refs)):
                        try:
                            mini_gts = {'0': [ref]}
                            mini_res = {'0': [pred]}
                            mini_score, _ = spice_scorer.compute_score(mini_gts, mini_res)
                            individual_scores.append(mini_score)
                        except Exception as sample_e:
                            failed_samples.append((i, pred[:50], ref[:50]))
                            logger.debug(f"   ìƒ˜í”Œ {i} ì‹¤íŒ¨: {str(sample_e)[:100]}")
                    
                    if individual_scores:
                        metrics['spice'] = float(np.mean(individual_scores))
                        logger.info(f"âœ“ SPICE (ê°œë³„ ê³„ì‚°): {metrics['spice']:.4f}")
                        logger.info(f"  ì„±ê³µ: {len(individual_scores)}/{len(filtered_preds)} ìƒ˜í”Œ")
                        if failed_samples:
                            logger.warning(f"  ì‹¤íŒ¨í•œ ìƒ˜í”Œ {len(failed_samples)}ê°œ:")
                            for idx, pred_preview, ref_preview in failed_samples[:5]:
                                logger.warning(f"    [{idx}] Pred: {pred_preview}... / Ref: {ref_preview}...")
                    else:
                        logger.error("âŒ ëª¨ë“  ìƒ˜í”Œì—ì„œ SPICE ê³„ì‚° ì‹¤íŒ¨")
                        metrics['spice'] = 0.0
                else:
                    logger.warning("âš ï¸ Java Module System í˜¸í™˜ì„± ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                    logger.info("í•´ê²° ë°©ë²•:")
                    logger.info("  Option 1: Java 8 ì„¤ì¹˜ (ê¶Œì¥)")
                    logger.info("    sudo apt-get install openjdk-8-jre")
                    logger.info("    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64")
                    metrics['spice'] = 0.0
            
    except ImportError as ie:
        logger.error(f"âŒ pycocoevalcap ì„í¬íŠ¸ ì‹¤íŒ¨: {ie}")
        logger.error("   ì„¤ì¹˜: pip install git+https://github.com/salaniz/pycocoevalcap.git")
        metrics['spice'] = 0.0
    except Exception as e:
        logger.error(f"âŒ SPICE ê³„ì‚° ì˜¤ë¥˜: {e}")
        logger.error(f"   ìƒì„¸: {traceback.format_exc()}")
        metrics['spice'] = 0.0
    
    # 5. CIDEr ê³„ì‚° (pycocoevalcap ê³µì‹ ë ˆí¬ì§€í† ë¦¬ ì‚¬ìš©)
    try:
        from pycocoevalcap.cider.cider import Cider
        
        logger.info("ğŸ“Š CIDEr ê³„ì‚° ì¤‘...")
        cider_scorer = Cider()
        
        # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§ (ìŒì„ ìœ ì§€í•˜ë©´ì„œ í•„í„°ë§)
        valid_pairs_for_cider = [(pred, ref) for pred, ref in zip(predictions, references) 
                                  if pred.strip() and ref.strip()]
        
        if len(valid_pairs_for_cider) == 0:
            logger.warning("âš ï¸ CIDEr: ìœ íš¨í•œ í…ìŠ¤íŠ¸ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
            metrics['cider'] = 0.0
        else:
            valid_preds_for_cider, valid_refs_for_cider = zip(*valid_pairs_for_cider)
            valid_preds_for_cider = list(valid_preds_for_cider)
            valid_refs_for_cider = list(valid_refs_for_cider)
            
            logger.info("  CIDEr ê³„ì‚°: {}ê°œ ìœ íš¨ ìƒ˜í”Œ".format(len(valid_pairs_for_cider)))
            
            gts = {str(i): [ref] for i, ref in enumerate(valid_refs_for_cider)}
            res = {str(i): [pred] for i, pred in enumerate(valid_preds_for_cider)}
            
            try:
                # ì§ì ‘ compute_score í˜¸ì¶œ (pycocoevalcap ê³µì‹ ì¸í„°í˜ì´ìŠ¤)
                cider_score, cider_scores = cider_scorer.compute_score(gts, res)
                metrics['cider'] = float(cider_score)
                logger.info(f"âœ“ CIDEr (ê³µì‹ pycocoevalcap): {metrics['cider']:.4f}")
            except Exception as cider_e:
                logger.error(f"âŒ CIDEr compute_score ì˜¤ë¥˜: {cider_e}")
                metrics['cider'] = 0.0
                
    except ImportError:
        logger.error("âŒ pycocoevalcapì„ ì„¤ì¹˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.error("   ì„¤ì¹˜: pip install git+https://github.com/salaniz/pycocoevalcap.git")
        metrics['cider'] = 0.0
    except Exception as e:
        logger.error(f"âŒ CIDEr ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['cider'] = 0.0
    
    # ë©”íŠ¸ë¦­ ì €ì¥
    safe_prefix = prefix if prefix else "model"
    metrics_path = output_dir / f"{safe_prefix}_metrics_{timestamp}.json"
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ“ ë©”íŠ¸ë¦­ ì €ì¥: {metrics_path}")
    return metrics


def print_final_results(metrics: Dict[str, float]):
    """
    ìµœì¢… ê²°ê³¼ ì¶œë ¥ (ëª¨ë“  ë©”íŠ¸ë¦­ í¬í•¨)
    """
    print("\n" + "=" * 90)
    print("ğŸ‰ PanoLLaVA ëª¨ë¸ í‰ê°€ ì™„ë£Œ - ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚° ì„±ê³µ")
    print("=" * 90)
    
    print("\nğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­ ê²°ê³¼ (ê³µì‹ ë ˆí¬ì§€í† ë¦¬ ê¸°ë°˜):")
    print("-" * 90)
    
    metric_info = {
        'bleu4': ('BLEU-4', 'sacrebleu (https://github.com/mjpost/sacrebleu)'),
        'meteor': ('METEOR', 'NLTK (https://www.nltk.org/)'),
        'rougeL': ('ROUGE-L', 'rouge-score (https://github.com/google-research/rouge)'),
        'spice': ('SPICE', 'pycocoevalcap (https://github.com/salaniz/pycocoevalcap)'),
        'cider': ('CIDEr', 'pycocoevalcap (https://github.com/salaniz/pycocoevalcap)'),
    }
    
    for key, (display_name, source) in metric_info.items():
        if key in metrics:
            value = metrics[key]
            status = "âœ“" if value > 0 else "âœ—"
            print(f"{status} {display_name:12s} (â†‘): {value:8.4f}  | ì¶œì²˜: {source}")
    
    print("-" * 90)
    print("ğŸ’¡ (â†‘) í‘œì‹œëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤.")
    print("\nğŸ“Œ ë©”íŠ¸ë¦­ ì„¤ëª…:")
    print("  â€¢ BLEU-4   : ê¸°ê³„ ë²ˆì—­ í’ˆì§ˆ í‰ê°€ (n-gram ì •í™•ë„)")
    print("  â€¢ METEOR   : ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ ê³ ë ¤ (ë™ì˜ì–´, ì–´ê·¼ ì¼ì¹˜)")
    print("  â€¢ ROUGE-L  : ì¬í˜„ìœ¨ ì¤‘ì‹¬ í‰ê°€ (ìµœëŒ€ ê³µí†µ ë¶€ë¶„ìˆ˜ì—´)")
    print("  â€¢ SPICE    : ì˜ë¯¸ì  ëª…ì œ ê¸°ë°˜ í‰ê°€ (ê·¸ë˜í”„ êµ¬ì¡°)")
    print("  â€¢ CIDEr    : ì´ë¯¸ì§€ ìº¡ì…˜ í‰ê°€ (ìš©ì–´ ì‹ ë¢°ë„ ê¸°ë°˜)")
    print("=" * 90)




def load_global_config(config_path: Optional[str] = None) -> Dict[str, Any]:
    """Load evaluation configuration using the shared training loader."""

    try:
        return _load_train_config_dict(config_path)
    except Exception as exc:
        logger.error(f"Failed to load configuration: {exc}")
        raise


def main():
    parser = argparse.ArgumentParser(description="PanoLLaVA ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ")
    # ì…ë ¥ ì¸ì: --config, --csv-input, --checkpoint-dir, --checkpoint
    parser.add_argument('--config', help='Global config YAML ê²½ë¡œ (ë¯¸ì§€ì • ì‹œ PANOVLM_CONFIG or ./config.yaml ì‚¬ìš©)')
    parser.add_argument('--checkpoint-dir', dest='checkpoint_dir', default=None,
                        help='ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì˜ˆ: runs/ADDDATA_SQ3_1/finetune/anyres-e2p_mlp/). '
                             'checkpoint_metadata.jsonì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤. '
                             'best.ckpt ë˜ëŠ” last.ckpt ì‹¬ë³¼ë¦­ ë§í¬ë¥¼ ìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤.')
    parser.add_argument('--checkpoint', '--ckpt', dest='checkpoint_file', default=None,
                        help='ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì • (ì˜ˆ: runs/.../best.ckpt). '
                             '--checkpoint-dirë³´ë‹¤ ìš°ì„  ì ìš©ë©ë‹ˆë‹¤.')
    parser.add_argument('--csv-input', dest='csv_input', default=None,
                        help='í‰ê°€ì— ì‚¬ìš©í•  CSV ê²½ë¡œ (ì˜ˆ: data/quic360/test.csv). '
                             'predictionê³¼ reference ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ë°”ë¡œ ë©”íŠ¸ë¦­ ê³„ì‚°, ì—†ìœ¼ë©´ ëª¨ë¸ë¡œ ìƒì„±í•©ë‹ˆë‹¤.')
    parser.add_argument('--metrics-only', action='store_true',
                        help='CSVì— prediction/referenceê°€ ìˆì„ ë•Œ ë©”íŠ¸ë¦­ë§Œ ê³„ì‚° (ëª¨ë¸ ë¡œë”© ìƒëµ)')
    parser.add_argument('--max-samples', type=int, default=None,
                        help='í‰ê°€ì— ì‚¬ìš©í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©)')
    parser.add_argument('--log-samples', action='store_true',
                        help='ë°°ì¹˜ë³„ ìƒì„¸ ì˜ˆì¸¡/ì •ë‹µ ë¡œê·¸ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.')
    parser.add_argument('--log-interval', type=int, default=25,
                        help='--log-samples ì‚¬ìš© ì‹œ ë°°ì¹˜ ë¡œê·¸ ê°„ê²© (ê¸°ë³¸ 25)')
    parser.add_argument('--log-max-samples', type=int, default=50,
                        help='--log-samples ì‚¬ìš© ì‹œ ìµœëŒ€ ë¡œê·¸ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ 50)')

    args = parser.parse_args()

    # ========== CSV íŒŒì¼ ì‚¬ì „ ê²€ì‚¬: prediction/reference ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ==========
    # ë©”íŠ¸ë¦­ ì „ìš© ëª¨ë“œ íŒë³„ì„ ìœ„í•´ ê°€ì¥ ë¨¼ì € ì‹¤í–‰
    preliminary_csv_input = args.csv_input
    metrics_only_mode = False
    
    if preliminary_csv_input:
        csv_path = Path(preliminary_csv_input)
        if csv_path.exists() and csv_path.suffix.lower() == '.csv':
            try:
                # CSV ì»¬ëŸ¼ í™•ì¸
                df_check = pd.read_csv(csv_path, nrows=5)  # ìƒìœ„ 5ê°œ í–‰ë§Œ ì½ì–´ì„œ í™•ì¸
                has_prediction = 'prediction' in df_check.columns
                has_reference = 'reference' in df_check.columns
                
                if has_prediction and has_reference:
                    metrics_only_mode = True
                    logger.info("=" * 60)
                    logger.info("ğŸ” CSV íŒŒì¼ì— prediction/reference ì»¬ëŸ¼ ë°œê²¬!")
                    logger.info("ğŸ“Š ë©”íŠ¸ë¦­ ì „ìš© ëª¨ë“œ í™œì„±í™” (ëª¨ë¸ ë¡œë”© ë° Config ë¡œë”© ìƒëµ)")
                    logger.info("=" * 60)
                elif args.metrics_only:
                    logger.warning("âš ï¸ --metrics-only ì˜µì…˜ì´ ì§€ì •ë˜ì—ˆìœ¼ë‚˜ CSVì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    logger.warning(f"   í˜„ì¬ ì»¬ëŸ¼: {df_check.columns.tolist()}")
                    logger.warning("   í•„ìˆ˜ ì»¬ëŸ¼: ['prediction', 'reference']")
                    raise ValueError("ë©”íŠ¸ë¦­ ì „ìš© ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ CSVì— predictionê³¼ reference ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            except pd.errors.EmptyDataError:
                logger.warning(f"âš ï¸ CSV íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {csv_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ CSV ì‚¬ì „ ê²€ì‚¬ ì‹¤íŒ¨ (ì¼ë°˜ ëª¨ë“œë¡œ ì§„í–‰): {e}")
    
    if args.metrics_only and not metrics_only_mode:
        raise ValueError("--metrics-only ì˜µì…˜ì€ CSVì— predictionê³¼ reference ì»¬ëŸ¼ì´ ìˆì„ ë•Œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    # ========== ë©”íŠ¸ë¦­ ì „ìš© ëª¨ë“œ: ë°”ë¡œ ë©”íŠ¸ë¦­ ê³„ì‚°ìœ¼ë¡œ ì´ë™ ==========
    if metrics_only_mode:
        logger.info("ğŸ“Š ë©”íŠ¸ë¦­ ì „ìš© ëª¨ë“œ - Config ë° ëª¨ë¸ ë¡œë”© ì™„ì „ ìƒëµ")
        
        # í•„ìš”í•œ ìµœì†Œ ë³€ìˆ˜ë§Œ ì„¤ì •
        max_samples_cli = args.max_samples if args.max_samples and args.max_samples > 0 else None
        safe_prefix = csv_path.stem  # CSV íŒŒì¼ëª…ì„ prefixë¡œ ì‚¬ìš©
        output_dir = Path("results/eval_results") / safe_prefix
        output_dir.mkdir(parents=True, exist_ok=True)
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“Š ë©”íŠ¸ë¦­ ê³„ì‚° ëª¨ë“œ")
            logger.info("=" * 60)
            logger.info(f"ğŸ“‚ CSV ì…ë ¥: {csv_path}")
            
            # CSV ì „ì²´ ë¡œë“œ
            df = pd.read_csv(csv_path, encoding='utf-8')
            logger.info(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")
            
            # max_samples ì ìš©
            if max_samples_cli is not None and len(df) > max_samples_cli:
                logger.info(f"ğŸ“‰ ìƒ˜í”Œ ìˆ˜ ì œí•œ: {len(df)} â†’ {max_samples_cli}")
                df = df.head(max_samples_cli)
            
            # 5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (CSV DataFrame ì§ì ‘ ì „ë‹¬)
            metrics = calculate_evaluation_metrics(df, output_dir, timestamp, safe_prefix)
            
            # ìµœì¢… ê²°ê³¼ ì¶œë ¥
            print_final_results(metrics)
            
            return  # ë©”íŠ¸ë¦­ ê³„ì‚° í›„ ì¢…ë£Œ
            
        except Exception as e:
            logger.error(f"âŒ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            raise

    # ========== ì¼ë°˜ ëª¨ë“œ: Config ë¡œë”© í•„ìš” ==========
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼/ë””ë ‰í† ë¦¬ ìš°ì„  ì²˜ë¦¬
    checkpoint_metadata = None
    explicit_checkpoint_path = None
    
    # --checkpoint ì˜µì…˜ì´ ì£¼ì–´ì§„ ê²½ìš° ìš°ì„  ì‚¬ìš©
    if args.checkpoint_file:
        ckpt_file = Path(args.checkpoint_file)
        
        if not ckpt_file.exists():
            raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ckpt_file}")
        
        if not ckpt_file.is_file():
            raise ValueError(f"ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œê°€ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {ckpt_file}")
        
        logger.info("=" * 60)
        logger.info(f"ğŸ“„ ëª…ì‹œì  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼: {ckpt_file}")
        logger.info("=" * 60)
        
        explicit_checkpoint_path = ckpt_file
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ì—ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹œë„
        ckpt_dir = ckpt_file.parent
        checkpoint_metadata = load_checkpoint_metadata(ckpt_dir)
        
        if checkpoint_metadata:
            logger.info("=" * 60)
            logger.info("ğŸ“‹ ë©”íƒ€ë°ì´í„°ì—ì„œ ë¡œë“œëœ ì •ë³´:")
            logger.info(f"  - Experiment: {checkpoint_metadata.get('experiment_name', 'N/A')}")
            logger.info(f"  - Stage: {checkpoint_metadata.get('stage', 'N/A')}")
            logger.info(f"  - Vision: {checkpoint_metadata.get('model_config', {}).get('vision_name', 'N/A')}")
            logger.info(f"  - Language: {checkpoint_metadata.get('model_config', {}).get('language_model_name', 'N/A')}")
            logger.info(f"  - Resampler: {checkpoint_metadata.get('model_config', {}).get('resampler_type', 'N/A')}")
            logger.info(f"  - Crop Strategy: {checkpoint_metadata.get('training_config', {}).get('crop_strategy', 'N/A')}")
            logger.info("=" * 60)
    
    elif args.checkpoint_dir:
        ckpt_dir = Path(args.checkpoint_dir)
        
        if not ckpt_dir.exists():
            raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ckpt_dir}")
        
        logger.info("=" * 60)
        node_desc = "ë””ë ‰í† ë¦¬" if ckpt_dir.is_dir() else "íŒŒì¼"
        logger.info(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ {node_desc}: {ckpt_dir}")
        logger.info("=" * 60)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹œë„
        checkpoint_metadata = load_checkpoint_metadata(ckpt_dir)
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì°¾ê¸°
        explicit_checkpoint_path = find_checkpoint_in_dir(ckpt_dir)
        
        if not explicit_checkpoint_path:
            raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ckpt_dir}")
        
        logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ íŒŒì¼: {explicit_checkpoint_path}")
        
        if checkpoint_metadata:
            logger.info("=" * 60)
            logger.info("ğŸ“‹ ë©”íƒ€ë°ì´í„°ì—ì„œ ë¡œë“œëœ ì •ë³´:")
            logger.info(f"  - Experiment: {checkpoint_metadata.get('experiment_name', 'N/A')}")
            logger.info(f"  - Stage: {checkpoint_metadata.get('stage', 'N/A')}")
            logger.info(f"  - Vision: {checkpoint_metadata.get('model_config', {}).get('vision_name', 'N/A')}")
            logger.info(f"  - Language: {checkpoint_metadata.get('model_config', {}).get('language_model_name', 'N/A')}")
            logger.info(f"  - Resampler: {checkpoint_metadata.get('model_config', {}).get('resampler_type', 'N/A')}")
            logger.info(f"  - Crop Strategy: {checkpoint_metadata.get('training_config', {}).get('crop_strategy', 'N/A')}")
            logger.info("=" * 60)

    # ========== CSV íŒŒì¼ ì‚¬ì „ ê²€ì‚¬: prediction/reference ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ==========
    # CLIì—ì„œ ì§€ì •í•œ CSV ë˜ëŠ” ê¸°ë³¸ê°’
    preliminary_csv_input = args.csv_input or "data/quic360/test.csv"
    csv_path = Path(preliminary_csv_input)
    metrics_only_mode = False
    
    if csv_path.exists() and csv_path.suffix.lower() == '.csv':
        try:
            # CSV ì»¬ëŸ¼ í™•ì¸
            df_check = pd.read_csv(csv_path, nrows=5)  # ìƒìœ„ 5ê°œ í–‰ë§Œ ì½ì–´ì„œ í™•ì¸
            has_prediction = 'prediction' in df_check.columns
            has_reference = 'reference' in df_check.columns
            
            if has_prediction and has_reference:
                metrics_only_mode = True
                logger.info("=" * 60)
                logger.info("ğŸ” CSV íŒŒì¼ì— prediction/reference ì»¬ëŸ¼ ë°œê²¬!")
                logger.info("ğŸ“Š ë©”íŠ¸ë¦­ ì „ìš© ëª¨ë“œ í™œì„±í™” (ëª¨ë¸ ë¡œë”© ìƒëµ)")
                logger.info("=" * 60)
            elif args.metrics_only:
                logger.warning("âš ï¸ --metrics-only ì˜µì…˜ì´ ì§€ì •ë˜ì—ˆìœ¼ë‚˜ CSVì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                logger.warning(f"   í˜„ì¬ ì»¬ëŸ¼: {df_check.columns.tolist()}")
                logger.warning("   í•„ìˆ˜ ì»¬ëŸ¼: ['prediction', 'reference']")
                raise ValueError("ë©”íŠ¸ë¦­ ì „ìš© ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ CSVì— predictionê³¼ reference ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        except pd.errors.EmptyDataError:
            logger.warning(f"âš ï¸ CSV íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {csv_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ CSV ì‚¬ì „ ê²€ì‚¬ ì‹¤íŒ¨ (ì¼ë°˜ ëª¨ë“œë¡œ ì§„í–‰): {e}")
    
    if args.metrics_only and not metrics_only_mode:
        raise ValueError("--metrics-only ì˜µì…˜ì€ CSVì— predictionê³¼ reference ì»¬ëŸ¼ì´ ìˆì„ ë•Œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    # ========== ë©”íŠ¸ë¦­ ì „ìš© ëª¨ë“œ: Config ë¡œë”© ìƒëµ ==========
    if metrics_only_mode:
        logger.info("ğŸ“Š ë©”íŠ¸ë¦­ ì „ìš© ëª¨ë“œ - Config ë¡œë”© ìƒëµ")
        # í•„ìš”í•œ ìµœì†Œ ë³€ìˆ˜ë§Œ ì„¤ì •
        max_samples_cli = args.max_samples if args.max_samples and args.max_samples > 0 else None
        safe_prefix = csv_path.stem  # CSV íŒŒì¼ëª…ì„ prefixë¡œ ì‚¬ìš©
        output_dir = Path("results/eval_results") / safe_prefix
        output_dir.mkdir(parents=True, exist_ok=True)
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        
        try:
            # CSV ì „ì²´ ë¡œë“œ
            logger.info("=" * 60)
            logger.info("ğŸ“Š ë©”íŠ¸ë¦­ ê³„ì‚° ëª¨ë“œ")
            logger.info("=" * 60)
            logger.info(f"ğŸ“‚ CSV ì…ë ¥: {csv_path}")
            
            df = pd.read_csv(csv_path, encoding='utf-8')
            logger.info(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")
            
            # max_samples ì ìš©
            if max_samples_cli is not None and len(df) > max_samples_cli:
                logger.info(f"ğŸ“‰ ìƒ˜í”Œ ìˆ˜ ì œí•œ: {len(df)} â†’ {max_samples_cli}")
                df = df.head(max_samples_cli)
            
            # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (CSV DataFrame ì§ì ‘ ì „ë‹¬)
            metrics = calculate_evaluation_metrics(df, output_dir, timestamp, safe_prefix)
            
            # ìµœì¢… ê²°ê³¼ ì¶œë ¥
            print_final_results(metrics)
            return  # ì—¬ê¸°ì„œ ì¢…ë£Œ
            
        except Exception as e:
            logger.error(f"âŒ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            raise

    # ========== ì¼ë°˜ ëª¨ë“œ: Config ë¡œë”© í•„ìš” ==========
    global_config = load_global_config(args.config)
    max_samples_cli = args.max_samples if args.max_samples and args.max_samples > 0 else None
    log_samples_flag = bool(args.log_samples)
    log_interval_cli = args.log_interval if args.log_interval and args.log_interval > 0 else 0
    log_max_samples_cli = args.log_max_samples if args.log_max_samples and args.log_max_samples > 0 else 50

    # ========== ë©”íƒ€ë°ì´í„° ìš°ì„  ì„¤ì • ë³‘í•© ==========
    # checkpoint_metadataê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ config ì‚¬ìš©
    if checkpoint_metadata:
        model_config_meta = checkpoint_metadata.get('model_config', {})
        training_config_meta = checkpoint_metadata.get('training_config', {})
        dataset_meta = checkpoint_metadata.get('dataset', {})
        
        # ëª¨ë¸ ì„¤ì • ë³‘í•© (ë©”íƒ€ë°ì´í„° ìš°ì„ )
        global_config.setdefault('models', {})
        global_config['models']['vision_name'] = model_config_meta.get('vision_name', global_config.get('models', {}).get('vision_name'))
        global_config['models']['language_model_name'] = model_config_meta.get('language_model_name', global_config.get('models', {}).get('language_model_name'))
        global_config['models']['resampler_type'] = model_config_meta.get('resampler_type', global_config.get('models', {}).get('resampler_type'))
        global_config['models']['latent_dimension'] = model_config_meta.get('latent_dimension', global_config.get('models', {}).get('latent_dimension'))
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì • ë³‘í•©
        global_config.setdefault('image_processing', {})
        global_config['image_processing']['crop_strategy'] = training_config_meta.get('crop_strategy', global_config.get('image_processing', {}).get('crop_strategy'))
        global_config['image_processing']['image_size'] = model_config_meta.get('image_size', global_config.get('image_processing', {}).get('image_size'))
        global_config['image_processing']['fov_deg'] = training_config_meta.get('fov_deg', global_config.get('image_processing', {}).get('fov_deg'))
        global_config['image_processing']['overlap_ratio'] = training_config_meta.get('overlap_ratio', global_config.get('image_processing', {}).get('overlap_ratio'))
        global_config['image_processing']['use_vision_processor'] = training_config_meta.get('use_vision_processor', global_config.get('image_processing', {}).get('use_vision_processor'))
        global_config['image_processing']['normalize'] = training_config_meta.get('normalize', global_config.get('image_processing', {}).get('normalize'))
        
        # í›ˆë ¨ ì„¤ì • ë³‘í•©
        global_config.setdefault('training', {})
        global_config['training']['max_text_length'] = model_config_meta.get('max_text_length', global_config.get('training', {}).get('max_text_length'))
        
        logger.info("âœ… ë©”íƒ€ë°ì´í„°ë¥¼ configì— ë³‘í•© ì™„ë£Œ")

    env_config = global_config.get("environment", {})
    model_config = global_config.get("models", {})
    data_config = global_config.get("data", {})
    training_config = global_config.get("training", {})
    image_cfg = global_config.get("image_processing", {})
    system_msgs = global_config.get("system_messages", {})

    # ë””ë°”ì´ìŠ¤ ì„¤ì •: í™˜ê²½ë³€ìˆ˜ ëŒ€ì‹  config ê¸°ë°˜ìœ¼ë¡œ GPU indexë¥¼ ì„ íƒ
    cuda_vis = env_config.get("cuda_visible_devices")
    if torch.cuda.is_available():
        try:
            first_idx = int(str(cuda_vis).split(",")[0].strip())
            torch.cuda.set_device(first_idx)
            logger.info(f"Device: using GPU index {first_idx} (from config)")
        except Exception as e:
            logger.warning(f"Invalid cuda_visible_devices in config: {cuda_vis} ({e})")

    # CSV ì…ë ¥ ê²½ë¡œ: CLI ìš°ì„  -> config ìš°ì„ ìˆœìœ„ -> ê¸°ë³¸ê°’
    eff_csv_input = (
        args.csv_input
        or data_config.get("csv_test")
        or data_config.get("csv_val")
        or global_config.get("paths", {}).get("csv_val")
        or "data/quic360/test.csv"
    )


    # Model core
    eff_vision_name = model_config.get("vision_name")
    eff_lm_name = model_config.get("language_model_name") or model_config.get("lm_model")
    eff_resampler = model_config.get("resampler_type") or model_config.get("resampler")
    # Image processing
    eff_crop_strategy = image_cfg.get("crop_strategy", "e2p")
    eff_overlap_ratio = image_cfg.get("overlap_ratio", 0.5)
    eff_use_vp = image_cfg.get("use_vision_processor", True)
    eff_image_size = image_cfg.get("image_size", [224, 224])
    eff_fov_deg = image_cfg.get("fov_deg", 90.0)
    eff_image_mean = image_cfg.get("image_mean")
    eff_image_std = image_cfg.get("image_std")
    eff_anyres_patch_size = image_cfg.get("anyres_patch_size")  # Noneì´ë©´ image_sizeì—ì„œ ìë™ ì¶”ë¡ 
    eff_anyres_max_patches = image_cfg.get("anyres_max_patches", 12)
    eff_normalize = image_cfg.get("normalize", True)
    # Tokenization
    eff_max_text_length = str(training_config.get("max_text_length", data_config.get("max_text_length", "auto")))
    eff_num_workers = training_config.get("num_workers", 16)
    eff_batch_size = (
        training_config.get("eval_batch_size")
        or training_config.get("batch_size")
        or training_config.get("finetune", {}).get("batch_size")
        or 16
    )
    eff_system_msg = training_config.get("system_msg", system_msgs.get("default", "You are a helpful assistant."))
    eff_output_dir = global_config.get("paths", {}).get("eval_dir", "results/eval_results")
    eff_prefix = training_config.get("prefix") or "model"
    safe_prefix = str(eff_prefix).strip() or "model"
    for ch in ["/", "\\", " "]:
        safe_prefix = safe_prefix.replace(ch, "_")
    # Generation
    gen_cfg = global_config.get("generation", {}) if isinstance(global_config, dict) else {}
    def _g(key, default):
        return gen_cfg.get(key, default) if isinstance(gen_cfg, dict) else default
    eff_gen_max_new_tokens = _g('max_new_tokens', 128)
    eff_gen_temperature = _g('temperature', 0.6)
    eff_gen_min_new_tokens = _g('min_new_tokens', 5)
    eff_gen_top_p = _g('top_p', 0.95)
    eff_gen_top_k = _g('top_k', 20)
    eff_gen_repetition_penalty = _g('repetition_penalty', 1.1)
    eff_gen_length_penalty = _g('length_penalty', 1.0)

    # ========== ëª¨ë¸ ë””ë ‰í† ë¦¬/ì²´í¬í¬ì¸íŠ¸ í•´ê²° ==========
    # --checkpoint-dirì´ ì§€ì •ë˜ë©´ ìš°ì„  ì‚¬ìš©, ì•„ë‹ˆë©´ ìë™ íƒìƒ‰
    if explicit_checkpoint_path:
        model_dir = str(explicit_checkpoint_path)
        logger.info(f"âœ… ëª…ì‹œì  ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©: {model_dir}")
    else:
        # stage strictly from config
        stage_from_cfg = training_config.get('default_stage', 'finetune')
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìë™ í•´ê²°
        cfg_source = args.config if args.config else global_config
        model_dir = resolve_model_dir(cfg_source, stage_from_cfg, crop_strategy=eff_crop_strategy)

    # LoRA ê°€ì¤‘ì¹˜ ìë™ ì„¤ì • (config-only; no CLI override)
    lora_weights_path = None
    if model_dir:
        checkpoint_dir = Path(model_dir)
        checkpoint_dir = checkpoint_dir if checkpoint_dir.is_dir() else checkpoint_dir.parent
        potential_lora_path = checkpoint_dir / "lora_weights"
        if potential_lora_path.exists():
            lora_weights_path = str(potential_lora_path)
            logger.info(f"âœ… Auto-found LoRA weights: {lora_weights_path}")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path(eff_output_dir) / safe_prefix
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = time.strftime('%Y%m%d_%H%M%S')

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    try:
        # ========== ì¼ë°˜ ëª¨ë“œ: ëª¨ë¸ ë¡œë”© + ìƒì„± + ë©”íŠ¸ë¦­ ê³„ì‚° ==========
        # 1ë‹¨ê³„: ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
        # Convert max_text_length for model only if numeric; otherwise omit (DataModule handles "auto")
        _mtl_val = None
        try:
            _mtl_val = int(eff_max_text_length)
        except Exception:
            _mtl_val = None
        # Use canonical config keys to avoid mismatches; include only when provided
        model_kwargs = {}
        if eff_vision_name:
            model_kwargs["vision_name"] = eff_vision_name
        if eff_lm_name:
            model_kwargs["language_model_name"] = eff_lm_name
        if eff_resampler:
            model_kwargs["resampler_type"] = eff_resampler
        if _mtl_val is not None:
            model_kwargs["max_text_length"] = _mtl_val
        model = load_model_and_lora(
            model_dir,
            lora_weights_path,
            device,
            config_path=args.config,  # ModelConfigë¥¼ ë³„ë„ë¡œ ì“°ëŠ” ê²½ìš°
            config_data=global_config if isinstance(global_config, dict) else None,
            **model_kwargs
        )

        # 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ (config ë°˜ì˜ ì¸ì ì¶”ê°€)
        datamodule, test_dataloader = prepare_test_dataset(
            csv_input=eff_csv_input,
            batch_size=eff_batch_size,
            max_text_length=eff_max_text_length,
            crop_strategy=(eff_crop_strategy or "e2p"),
            lm_name=(eff_lm_name or "Qwen/Qwen2.5-0.5B-Instruct"),
            num_workers=eff_num_workers,
            overlap_ratio=(eff_overlap_ratio if eff_overlap_ratio is not None else 0.5),
            image_size=eff_image_size,
            fov_deg=eff_fov_deg,
            image_mean=eff_image_mean,
            image_std=eff_image_std,
            anyres_patch_size=eff_anyres_patch_size,
            anyres_max_patches=eff_anyres_max_patches,
            normalize=eff_normalize,
            vision_name=eff_vision_name,
            system_msg=eff_system_msg,
            use_vision_processor=(bool(eff_use_vp) if eff_use_vp is not None else False),
            auto_max_text_length_cap=int(global_config.get("data", {}).get("auto_max_text_length_cap", 8192)) if isinstance(global_config, dict) else 8192,
            auto_max_text_length_floor=int(global_config.get("data", {}).get("auto_max_text_length_floor", 512)) if isinstance(global_config, dict) else None,
            auto_max_text_length_scan_limit=int(global_config.get("data", {}).get("auto_max_text_length_scan_limit", 1000)) if isinstance(global_config, dict) else None
        )

        # 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ìƒì„± (system_msg ì „ë‹¬)
        predictions, references, image_paths, input_texts = generate_predictions(
            model, test_dataloader, datamodule, device,
            max_new_tokens=int(eff_gen_max_new_tokens),
            temperature=float(eff_gen_temperature),
            top_p=float(eff_gen_top_p),
            top_k=int(eff_gen_top_k),
            repetition_penalty=float(eff_gen_repetition_penalty),
            length_penalty=float(eff_gen_length_penalty),
            min_new_tokens=int(eff_gen_min_new_tokens),
            system_msg=eff_system_msg,
            max_samples=max_samples_cli,
            log_samples=log_samples_flag,
            log_interval=log_interval_cli,
            log_max_samples=log_max_samples_cli
        )

        # 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ë¡œê¹…
        df = save_and_log_results(
            predictions,
            references,
            image_paths,
            input_texts,
            output_dir,
            timestamp,
            safe_prefix,
        )

        # 5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        if max_samples_cli is not None:
            logger.info(f"âš ï¸ ì œí•œëœ {len(df)}ê°œ ìƒ˜í”Œì— ëŒ€í•´ì„œë§Œ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•©ë‹ˆë‹¤.")
        metrics = calculate_evaluation_metrics(df, output_dir, timestamp, safe_prefix)

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print_final_results(metrics)

    except Exception as e:
        logger.error(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise

if __name__ == '__main__':
    main()
