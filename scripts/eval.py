# coding: utf-8
"""
PanoLLaVA Comprehensive Model Evaluation System
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ë‹¨ê³„ë³„ í‰ê°€ ì‹œìŠ¤í…œ:
1. ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ (ChatPanoTestDataset, VLMDataModule)
3. ë°°ì¹˜ë³„ í…ìŠ¤íŠ¸ ìƒì„± (generate)
4. ì˜ˆì¸¡/ì •ë‹µ í…ìŠ¤íŠ¸ ì €ì¥ ë° ë¡œê¹…
5. í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (BLEU, ROUGE, METEOR, SPICE, CIDEr, CLIP-S, RefCLIP-S)

ì‚¬ìš©ë²•:
    python eval.py --config config.yaml --csv-input data/quic360/test.csv
"""

import argparse
import torch
import json
import logging
import time
import traceback
import os
import sys
import re
from pathlib import Path
from tqdm import tqdm
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple

# Add src to Python path
script_dir = Path(__file__).parent
project_root = script_dir.parent
src_path = project_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from train import load_config_dict as _load_train_config_dict

# ë‚´ë¶€ ëª¨ë“ˆ
# Silence HF tokenizers fork/parallelism warnings and avoid deadlocks
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

from panovlm.dataset import VLMDataModule
from panovlm.processors.universal_text_formatter import UniversalTextFormatter

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


_STAGE_CANONICAL_MAP = {
    "vision": ("vision", "vision_pretraining", "vision_pretrain"),
    "resampler": ("resampler", "resampler_training"),
    "finetune": ("finetune", "instruction_tuning", "instruction_tune"),
    "generate": ("generate", "inference"),
}

_STAGE_VARIANT_LOOKUP = {
    variant: canonical
    for canonical, variants in _STAGE_CANONICAL_MAP.items()
    for variant in variants
}


def _infer_prefix_from_runs(runs_root: Path, crop_strategy: str, stage_names: List[str], resampler: str) -> Optional[str]:
    if not runs_root.exists() or not runs_root.is_dir():
        return None

    search_stage_names = stage_names or []
    patterns: List[Tuple[str, Optional[str]]] = []

    for stage_name in search_stage_names:
        patterns.append((f"*_{crop_strategy}_{stage_name}_{resampler}", stage_name))

    if not patterns:
        patterns.append((f"*_{crop_strategy}_*_{resampler}", None))

    for pattern, stage_hint in patterns:
        try:
            matches = sorted(
                runs_root.glob(pattern),
                key=lambda p: p.stat().st_mtime,
                reverse=True
            )
        except Exception:
            matches = sorted(runs_root.glob(pattern))
        if not matches:
            continue
        for match in matches:
            name = match.name
            if stage_hint:
                suffix = f"_{crop_strategy}_{stage_hint}_{resampler}"
                if not name.endswith(suffix):
                    continue
                prefix_candidate = name[:-len(suffix)]
                if prefix_candidate.endswith('_'):
                    prefix_candidate = prefix_candidate[:-1]
            else:
                if not name.endswith(f"_{resampler}"):
                    continue
                base = name[: -len(f"_{resampler}")]
                if base.endswith('_'):
                    base = base[:-1]
                token = f"_{crop_strategy}_"
                idx = base.rfind(token)
                if idx == -1:
                    continue
                prefix_candidate = base[:idx]
            if prefix_candidate:
                logger.info(f"ğŸ” Inferred prefix '{prefix_candidate}' from runs/{name}")
                return prefix_candidate

    return None


def _stage_variants(stage: Optional[str]) -> List[str]:
    """Return ordered unique stage variants covering historical aliases."""
    if stage is None:
        return []

    stage_key = str(stage).strip()
    canonical = _STAGE_VARIANT_LOOKUP.get(stage_key)
    if canonical:
        variants = _STAGE_CANONICAL_MAP[canonical]
        # Preserve order while removing duplicates
        seen = set()
        return [s for s in variants if not (s in seen or seen.add(s))]

    return [stage_key] if stage_key else []


def resolve_model_dir(config_or_path, stage: str = None, crop_strategy: str = None) -> str:
    """
    HF-style ëª¨ë¸ ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰ (PyTorch bin ê¸°ë°˜)
    - config_or_path: dict ë˜ëŠ” JSON íŒŒì¼ ê²½ë¡œ(str)
    - stage/crop_strategy: runs/<prefix>_<crop>_<stage>_<resampler>/hf_model íŒíŠ¸ êµ¬ì„±ì— ì‚¬ìš©
    """
    try:
        # config ë¡œë”© (dict ë˜ëŠ” íŒŒì¼ ê²½ë¡œ)
        if isinstance(config_or_path, (str, Path)):
            config = _load_train_config_dict(str(config_or_path))
        elif isinstance(config_or_path, dict):
            config = config_or_path
        else:
            raise TypeError(f"Unsupported config type: {type(config_or_path)}")

        resampler = config.get('models', {}).get('resampler_type') or config.get('models', {}).get('resampler', 'mlp')
        if stage is None:
            stage = config.get('training', {}).get('default_stage', 'finetune')

        if crop_strategy is None:
            crop_strategy = config.get('image_processing', {}).get('crop_strategy', 'e2p')

        stage_variants = _stage_variants(stage)

        prefix = config.get('training', {}).get('prefix')
        if not prefix:
            prefix = config.get('experiment', {}).get('name') if isinstance(config.get('experiment'), dict) else None
        if not prefix:
            prefix = config.get('experiment', {}).get('id') if isinstance(config.get('experiment'), dict) else None
        if not prefix:
            prefix = _infer_prefix_from_runs(
                Path(config.get('paths', {}).get('runs_dir', 'runs')),
                crop_strategy,
                stage_variants,
                resampler
            )
        if not prefix:
            raise KeyError("training.prefix is required in config.json")

        # ì¶”ê°€: pretrained_dir ì§€ì› ë° HF ë””ë ‰í† ë¦¬/ì²´í¬í¬ì¸íŠ¸ ìë™ íƒìƒ‰
        paths_cfg = config.get('paths', {}) if isinstance(config, dict) else {}
        pretrained_dir = paths_cfg.get('pretrained_dir')
        if pretrained_dir and Path(pretrained_dir).exists():
            p = Path(pretrained_dir)
            if p.is_file() and p.suffix == '.ckpt':
                logger.info(f"âœ… Using checkpoint from config: {pretrained_dir}")
            else:
                logger.info(f"âœ… Using pretrained_dir from config: {pretrained_dir}")
            return str(p)

        # runs ë””ë ‰í† ë¦¬ ë‚´ hf_model í´ë” ìë™ íƒìƒ‰
        runs_root = Path(paths_cfg.get('runs_dir', 'runs'))

        def try_from_run_dir(run_dir: Path, stage_hint: str) -> Optional[str]:
            if not run_dir.exists() or not run_dir.is_dir():
                return None

            hf_dir = run_dir / 'hf_model'
            if hf_dir.exists() and hf_dir.is_dir():
                logger.info(f"âœ… Using HF model dir (stage='{stage_hint}'): {str(hf_dir)}")
                return str(hf_dir)

            pano_dir = run_dir / 'panorama_model'
            if pano_dir.exists() and pano_dir.is_dir():
                logger.info(f"âœ… Using panorama_model dir (stage='{stage_hint}'): {str(pano_dir)}")
                return str(pano_dir)

            best_ckpt = run_dir / 'best.ckpt'
            if best_ckpt.exists():
                logger.info(f"âœ… Using best checkpoint (stage='{stage_hint}'): {str(best_ckpt)}")
                return str(best_ckpt)

            last_ckpt = run_dir / 'last.ckpt'
            if last_ckpt.exists():
                logger.info(f"âœ… Using last checkpoint (stage='{stage_hint}'): {str(last_ckpt)}")
                return str(last_ckpt)

            try:
                any_ckpts = sorted(run_dir.glob('*.ckpt'))
                if any_ckpts:
                    logger.info(f"âœ… Using checkpoint (stage='{stage_hint}'): {str(any_ckpts[0])}")
                    return str(any_ckpts[0])
            except Exception:
                pass

            return None

        checked_stages = set()
        for stage_name in stage_variants:
            checked_stages.add(stage_name)
            candidate_run_dir = runs_root / f"{prefix}_{crop_strategy}_{stage_name}_{resampler}"
            resolved = try_from_run_dir(candidate_run_dir, stage_name)
            if resolved:
                return resolved

        # wildcard fallback: scan any stage between prefix/crop/resampler
        pattern_prefix = f"{prefix}_{crop_strategy}_"
        pattern_suffix = f"_{resampler}"
        wildcard_pattern = f"{pattern_prefix}*{pattern_suffix}"
        for run_dir in sorted(runs_root.glob(wildcard_pattern)):
            stage_hint = run_dir.name[len(pattern_prefix):-len(pattern_suffix)] if len(pattern_suffix) > 0 else run_dir.name[len(pattern_prefix):]
            if stage_hint in checked_stages:
                continue
            resolved = try_from_run_dir(run_dir, stage_hint or "unknown")
            if resolved:
                return resolved

        raise FileNotFoundError("No pretrained model dir found. Set paths.pretrained_dir or pass --model-dir")

    except Exception as e:
        logger.error(f"Failed to resolve model dir: {e}")
        raise



def load_model_and_lora(
    model_dir: str,
    lora_weights_path: Optional[str],
    device: torch.device,
    config_path: Optional[str] = None,
    config_data: Optional[Dict[str, Any]] = None,
    **model_kwargs
):
    """
    1ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ì™€ LoRA ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ì—¬ ìƒì„±ìš© ëª¨ë¸ ì¤€ë¹„ (ì„¤ì • ì‹œìŠ¤í…œ í†µí•©)
    - ìƒˆë¡œìš´ PanoramaVLM ì¸í„°í˜ì´ìŠ¤ ìš°ì„  ì‹œë„
    - ì‹¤íŒ¨ ì‹œ VLMModule í´ë°± (ì´ë•Œ model_configë¥¼ ë°˜ë“œì‹œ ì „ë‹¬)
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ 1ë‹¨ê³„: ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ (ì„¤ì • ì‹œìŠ¤í…œ í†µí•©)")
    logger.info("=" * 60)

    # ë””ë°”ì´ìŠ¤ ë¬¸ìì—´
    device_str = str(device) if device != "auto" else "auto"

    # config ê°ì²´ ì¤€ë¹„ (ModelConfig ë˜ëŠ” dict)
    config_obj = None
    if config_data is not None:
        if isinstance(config_data, dict):
            config_obj = config_data
        else:
            logger.warning("config_data is not a dict; ignoring runtime config override")
    elif config_path:
        try:
            from panovlm.config.config_manager import ModelConfig
            try:
                config_obj = ModelConfig.load(config_path)
                logger.info(f"ğŸ“‹ ModelConfig ë¡œë“œ ì™„ë£Œ(from {config_path})")
            except Exception as e:
                logger.warning(f"ModelConfig.load ì‹¤íŒ¨, config dictë¡œ ëŒ€ì²´: {e}")
                config_obj = _load_train_config_dict(str(config_path))
        except Exception as e:
            logger.warning(f"panovlm.config.ModelConfig ì‚¬ìš© ë¶ˆê°€, config dictë¡œ ëŒ€ì²´: {e}")
            config_obj = _load_train_config_dict(str(config_path))

    # â”€â”€ PanoramaVLM (HF ë””ë ‰í† ë¦¬ ë˜ëŠ” .ckpt) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        from panovlm.models.model import PanoramaVLM

        # from_checkpoint/from_pretrained_dirì— config/model_config ì–´ëŠ ìª½ ì´ë¦„ì„ ì“°ëŠ”ì§€ ëª¨ë“ˆë³„ë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆì–´
        # ëª¨ë‘ ì•ˆì „í•˜ê²Œ ì „ë‹¬(ë°›ëŠ” ìª½ì—ì„œ ë¬´ì‹œí•´ë„ ë¬´í•´)
        extra_cfg = {}
        if config_obj is not None:
            extra_cfg["config"] = config_obj
            extra_cfg["model_config"] = config_obj

        mpath = Path(model_dir)
        if mpath.is_file() and mpath.suffix == ".ckpt":
            logger.info(f"ğŸ“¦ Loading from checkpoint: {str(mpath)}")
            model = PanoramaVLM.from_checkpoint(
                str(mpath),
                device=device_str,
                **extra_cfg,
                **{k: v for k, v in model_kwargs.items() if v is not None}
            )
        else:
            model = PanoramaVLM.from_pretrained_dir(
                str(mpath),
                device=device_str,
                **extra_cfg,
                **{k: v for k, v in model_kwargs.items() if v is not None}
            )

        # ì„¤ì • ì •ë³´ ë¡œê·¸
        if hasattr(model, "config") and model.config:
            logger.info("ğŸ“‹ Model Configuration ìš”ì•½:")
            for k in [
                "vision_name", "language_model_name", "latent_dimension",
                "image_size", "crop_strategy", "use_lora", "lora_r", "lora_alpha"
            ]:
                try:
                    val = getattr(model.config, k, None)
                except Exception:
                    val = None
                if val is not None:
                    logger.info(f"   - {k}: {val}")

        # ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„ ìœ„í•œ ë˜í¼
        class ModelWrapper:
            def __init__(self, panorama_model):
                self.model = panorama_model
                self._stage_key = "finetune"
            def eval(self):
                self.model.eval(); return self
            def to(self, dev):
                self.model = self.model.to(dev); return self

        wrapped_model = ModelWrapper(model).eval()
        logger.info(f"âœ“ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ - Device: {device}")
        return wrapped_model

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise

def prepare_test_dataset(
    csv_input: str,
    batch_size: int,
    max_text_length: str | int,
    crop_strategy: str,
    lm_name: str,
    num_workers: int = 0,
    overlap_ratio: float = 0.5,
    *,
    image_size: Tuple[int, int] | List[int] | None = None,
    fov_deg: float = 90.0,
    image_mean: Optional[List[float]] = None,
    image_std: Optional[List[float]] = None,
    anyres_patch_size: int = 336,
    anyres_max_patches: int = 12,
    normalize: bool = True,
    vision_name: Optional[str] = None,
    system_msg: Optional[str] = None,
    use_vision_processor: bool = True,
    auto_max_text_length_cap: Optional[int] = None,
    auto_max_text_length_floor: Optional[int] = None,
    auto_max_text_length_scan_limit: Optional[int] = None
) -> Tuple[VLMDataModule, Any]:
    """
    2ë‹¨ê³„: ChatPanoTestDatasetê³¼ VLMDataModuleì„ í™œìš©í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    - config.jsonì˜ image_processing/ training ë‚´ìš©ì„ ì¸ìí™”í•˜ì—¬ ë°˜ì˜
    """
    logger.info("=" * 60)
    logger.info("ğŸ“Š 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„")
    logger.info("=" * 60)

    logger.info(f"ğŸ“‚ CSV ì…ë ¥: {csv_input}")
    system_msg = system_msg or "You are an expert assistant specialized in analyzing panoramic images. Please provide detailed, accurate, and helpful responses about what you observe in the panoramic view shortly."

    # Normalize image_size
    _img_size = None
    if image_size is not None:
        try:
            if isinstance(image_size, (list, tuple)) and len(image_size) == 2:
                _img_size = (int(image_size[0]), int(image_size[1]))
        except Exception:
            _img_size = None

    datamodule = VLMDataModule(
        csv_train=csv_input,
        csv_val=csv_input,  # í‰ê°€ìš©ìœ¼ë¡œ ë™ì¼í•œ íŒŒì¼ ì‚¬ìš©
        batch_size=batch_size,
        num_workers=num_workers,
        tokenizer_name=lm_name,
        max_text_length=max_text_length,
        image_size=_img_size or (224, 224),
        crop_strategy=crop_strategy,
        eval_mode=True,
        system_msg=system_msg,
        overlap_ratio=overlap_ratio,
        fov_deg=fov_deg,
        image_mean=image_mean,
        image_std=image_std,
        anyres_patch_size=anyres_patch_size,
        anyres_max_patches=anyres_max_patches,
        normalize=normalize,
        vision_model_name=vision_name,
        use_vision_processor=use_vision_processor,
        auto_max_text_length_cap=int(auto_max_text_length_cap) if auto_max_text_length_cap is not None else 8192,
        auto_max_text_length_floor=int(auto_max_text_length_floor) if auto_max_text_length_floor is not None else None,
        auto_max_text_length_scan_limit=int(auto_max_text_length_scan_limit) if auto_max_text_length_scan_limit is not None else None
    )

    datamodule.setup()
    test_dataloader = datamodule.val_dataloader()

    logger.info(f"âœ“ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
    logger.info(f"   - ì´ ë°°ì¹˜ ìˆ˜: {len(test_dataloader)}")
    logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    logger.info(f"   - í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´ (requested): {max_text_length}")
    logger.info(f"   - í¬ë¡­ ì „ëµ: {crop_strategy}")
    logger.info(f"   - ê²¹ì¹¨ ë¹„ìœ¨: {overlap_ratio}")
    logger.info(f"   - ì›Œì»¤ ìˆ˜: {num_workers}")
    logger.info(f"   - Vision ëª¨ë¸: {vision_name}")
    logger.info(f"   - ì´ë¯¸ì§€ í¬ê¸°: {(_img_size or (224, 224))}")
    logger.info(f"   - fov_deg: {fov_deg}")
    logger.info(f"   - normalize: {normalize} | use_vision_processor: {use_vision_processor}")
    if image_mean is not None and image_std is not None:
        logger.info(f"   - image_mean/std: {image_mean} / {image_std}")
    logger.info(f"   - use_vision_processor: {use_vision_processor}")

    return datamodule, test_dataloader

def generate_predictions(
    model: Any,
    test_dataloader,
    datamodule: VLMDataModule,
    device: torch.device,
    *,
    max_new_tokens: int = 32,
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: int = 50,
    repetition_penalty: float = 1.1,
    length_penalty: float = 1.0,
    min_new_tokens: int = 5,
    system_msg: Optional[str] = None
) -> Tuple[List[str], List[str], List[str], List[str]]:
    """
    3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë°°ì¹˜ë³„ í…ìŠ¤íŠ¸ ìƒì„±
    - config.training.system_msg(ë˜ëŠ” system_messages.default) ë¥¼ UniversalTextFormatterì— ë°˜ì˜
    """
    logger.info("=" * 60)
    logger.info("ğŸ¤– 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ìƒì„± (UniversalTextFormatter í™œìš©)")
    logger.info("=" * 60)

    predictions, references, image_paths, input_texts = [], [], [], []

    tokenizer = datamodule.tokenizer
    sys_msg = system_msg or "You are an expert assistant specialized in analyzing panoramic images. Please provide detailed, accurate, and helpful responses about what you observe in the panoramic view shortly."
    text_formatter = UniversalTextFormatter(
        tokenizer,
        system_msg=sys_msg
    )

    logger.info(f"ğŸ¯ ìƒì„± íŒŒë¼ë¯¸í„° - Max tokens: {max_new_tokens}, Min tokens: {min_new_tokens}, Temperature: {temperature}")
    logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ í¬ë§·í„° - ëª¨ë¸: {text_formatter.model_family} ({'Instruct' if text_formatter.is_instruct else 'Base'})")

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(test_dataloader, desc="ìƒì„± ì¤‘")):
            try:
                pixel_values = batch["pixel_values"].to(device)
                input_ids = batch.get("input_ids")
                if input_ids is not None:
                    input_ids = input_ids.to(device)
                attention_mask = batch.get("attention_mask")
                if attention_mask is not None:
                    attention_mask = attention_mask.to(device)

                batch_size = pixel_values.shape[0]

                # ê°„ì†Œí™”ëœ ì •ë‹µÂ·ë©”íƒ€ ì¶”ì¶œ
                batch_references = []
                if "reference" in batch:
                    refs = batch["reference"]
                    batch_references = [str(r).strip() for r in (refs if isinstance(refs, list) else [refs]*batch_size)]
                else:
                    batch_references = [f"no_reference_{i}" for i in range(batch_size)]

                batch_image_paths = batch.get("image_path", [f"batch_{batch_idx}_sample_{i}" for i in range(batch_size)])
                batch_input_texts = batch.get("original_query", batch.get("input_text", [f"no_query_{i}" for i in range(batch_size)]))
                if not isinstance(batch_input_texts, list):
                    batch_input_texts = [batch_input_texts] * batch_size

                generation_config = text_formatter.get_generation_config()
                gen_kwargs = {
                    "pixel_values": pixel_values,
                    "input_ids": input_ids,
                    "attention_mask": attention_mask,
                    "max_new_tokens": max_new_tokens,
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": top_k,
                    "repetition_penalty": repetition_penalty,
                    "length_penalty": length_penalty,
                    "min_new_tokens": min_new_tokens,
                    "do_sample": True,
                    "pad_token_id": tokenizer.pad_token_id,
                    "eos_token_id": tokenizer.eos_token_id,
                }
                if hasattr(model, 'model') and hasattr(model.model, 'generation_config'):
                    if hasattr(model.model.generation_config, 'stop_strings'):
                        gen_kwargs["stop_strings"] = generation_config["stop_strings"][:3]

                if hasattr(model, 'model') and hasattr(model.model, 'generate'):
                    output = model.model.generate(**gen_kwargs)
                elif hasattr(model, 'generate'):
                    output = model.generate(**gen_kwargs)
                else:
                    raise AttributeError("ëª¨ë¸ì— generate ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")

                batch_predictions = []
                if isinstance(output, torch.Tensor):
                    for i in range(batch_size):
                        input_length = input_ids[i].shape[0] if input_ids is not None else 0
                        generated_tokens = output[i][input_length:]
                        raw_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                        clean_prediction = text_formatter.extract_assistant_response(raw_text)
                        batch_predictions.append(clean_prediction)
                elif isinstance(output, dict) and "text" in output:
                    for raw_text in output["text"]:
                        clean_prediction = text_formatter.extract_assistant_response(raw_text)
                        batch_predictions.append(clean_prediction)
                else:
                    logger.warning(f"Unexpected output format: {type(output)}")
                    batch_predictions = ["[ìƒì„± ì‹¤íŒ¨]"] * batch_size

                # í¬ê¸° ì •í•©
                if len(batch_predictions) != batch_size:
                    if len(batch_predictions) < batch_size:
                        batch_predictions.extend(["[í¬ê¸° ë¶€ì¡±]"] * (batch_size - len(batch_predictions)))
                    else:
                        batch_predictions = batch_predictions[:batch_size]

                # ì •ë¦¬
                cleaned_predictions = []
                for pred in batch_predictions:
                    cleaned_predictions.append(pred.strip().replace('\n\n', '\n') if pred and pred.strip() else "[ë¹ˆ ì‘ë‹µ]")

                # ë¡œê·¸ & ì¶•ì 
                logger.info(f"=== ë°°ì¹˜ {batch_idx} ê²°ê³¼ ë¡œê·¸ ===")
                for i, (pred, ref) in enumerate(zip(cleaned_predictions, batch_references)):
                    logger.info(f"  ìƒ˜í”Œ {len(predictions) + i}")
                    logger.info(f"    ì˜ˆì¸¡: '{pred}'")
                    logger.info(f"    ì •ë‹µ: '{ref}'")
                logger.info(f"==========================")

                predictions.extend(cleaned_predictions)
                references.extend(batch_references)
                image_paths.extend(batch_image_paths)
                input_texts.extend(batch_input_texts)

                if batch_idx % 10 == 0:
                    logger.info(f"ì§„í–‰: {batch_idx + 1}/{len(test_dataloader)} ë°°ì¹˜ ì™„ë£Œ ({len(predictions)} ìƒ˜í”Œ)")

            except Exception as e:
                logger.error(f"ë°°ì¹˜ {batch_idx} ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                bs = pixel_values.shape[0] if 'pixel_values' in locals() else 1
                predictions.extend([f"[ë°°ì¹˜ ì˜¤ë¥˜_{i}]" for i in range(bs)])
                references.extend(batch_references if 'batch_references' in locals() else [f"[ì •ë‹µ ì—†ìŒ_{i}]" for i in range(bs)])
                image_paths.extend(batch_image_paths if 'batch_image_paths' in locals() else [f"error_batch_{batch_idx}_sample_{i}" for i in range(bs)])
                input_texts.extend(batch_input_texts if 'batch_input_texts' in locals() else [f"error_input_{i}" for i in range(bs)])
                continue

    logger.info(f"âœ“ í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ! ì´ ìƒ˜í”Œ ìˆ˜: {len(predictions)}")
    return predictions, references, image_paths, input_texts



def save_and_log_results(
    predictions: List[str],
    references: List[str],
    image_paths: List[str],
    input_texts: List[str],
    output_dir: Path,
    timestamp: str,
    prefix: str,
) -> pd.DataFrame:
    """
    4ë‹¨ê³„: ìƒì„±ëœ ë‹µë³€ê³¼ ì •ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ê³  ë¡œê¹… (ê°œì„ ëœ ë¶„ì„ í¬í•¨)
    """
    logger.info("=" * 60)
    logger.info("ğŸ’¾ 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ë¶„ì„")
    logger.info("=" * 60)
    
    # ê°œì„ ëœ CSV ë°ì´í„° ì¤€ë¹„
    results_data = []
    for i, (pred, ref, img_path) in enumerate(zip(predictions, references, image_paths)):
        # ë¹ˆ ê°’ ì²˜ë¦¬ ë° ê¸°ë³¸ ì •ë¦¬
        pred_str = str(pred).strip() if pred is not None else ""
        ref_str = str(ref).strip() if ref is not None else ""
        img_path_str = str(img_path) if img_path is not None else ""
        
        # ì˜ˆì¸¡ê°’ í’ˆì§ˆ ë¶„ì„
        is_error = pred_str.startswith('[') and pred_str.endswith(']')
        is_empty = not pred_str or pred_str in ["", "[ë¹ˆ ì‘ë‹µ]"]
        
        # input_text ì¶”ì¶œ (ì¸ë±ìŠ¤ í™•ì¸ í›„ ì•ˆì „í•˜ê²Œ)
        input_text_str = ""
        if i < len(input_texts):
            input_text_str = str(input_texts[i]).strip() if input_texts[i] is not None else ""
        
        results_data.append({
            'sample_id': i,
            'image_path': img_path_str,
            'original_query': input_text_str,
            'prediction': pred_str,
            'reference': ref_str,
            'pred_length': len(pred_str.split()),
            'ref_length': len(ref_str.split()),
            'is_error': is_error,
            'is_empty': is_empty
        })
    
    # DataFrame ìƒì„± ë° ì €ì¥
    df = pd.DataFrame(results_data)
    safe_prefix = prefix if prefix else "model"
    csv_path = output_dir / f"{safe_prefix}_predictions_{timestamp}.csv"
    df.to_csv(csv_path, index=False, encoding='utf-8')
    
    # ê°œì„ ëœ ê²°ê³¼ í†µê³„ ë¶„ì„
    total_samples = len(df)
    error_count = df['is_error'].sum()
    empty_count = df['is_empty'].sum()
    valid_count = total_samples - error_count - empty_count
    
    # ê¸¸ì´ í†µê³„ (ìœ íš¨í•œ ì˜ˆì¸¡ê°’ë§Œ)
    valid_df = df[~df['is_error'] & ~df['is_empty']]
    if len(valid_df) > 0:
        avg_pred_length = valid_df['pred_length'].mean()
        avg_ref_length = valid_df['ref_length'].mean()
        pred_length_std = valid_df['pred_length'].std()
    else:
        avg_pred_length = avg_ref_length = pred_length_std = 0.0
    
    logger.info(f"ğŸ“Š ìƒì„± í’ˆì§ˆ ë¶„ì„:")
    logger.info(f"   - ì´ ìƒ˜í”Œ: {total_samples}")
    logger.info(f"   - ì„±ê³µì  ìƒì„±: {valid_count}ê°œ ({valid_count/total_samples*100:.1f}%)")
    logger.info(f"   - ìƒì„± ì˜¤ë¥˜: {error_count}ê°œ ({error_count/total_samples*100:.1f}%)")
    logger.info(f"   - ë¹ˆ ì‘ë‹µ: {empty_count}ê°œ ({empty_count/total_samples*100:.1f}%)")
    
    if valid_count > 0:
        logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„:")
        logger.info(f"   - í‰ê·  ì˜ˆì¸¡ ê¸¸ì´: {avg_pred_length:.1f} Â± {pred_length_std:.1f} ë‹¨ì–´")
        logger.info(f"   - í‰ê·  ì •ë‹µ ê¸¸ì´: {avg_ref_length:.1f} ë‹¨ì–´")
        logger.info(f"   - ê¸¸ì´ ë¹„ìœ¨ (ì˜ˆì¸¡/ì •ë‹µ): {avg_pred_length/avg_ref_length:.2f}")
    
    logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
    return df
    


def basic_cleanup(text: str) -> str:
    """
    Level 1: ê¸°ë³¸ ì •ë¦¬ - ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë§Œ ì œê±° (ì˜ë¯¸ ë³´ì¡´)

    - íŠ¹ìˆ˜ í† í° ì œê±° (<image>, <|im_start|> ë“±)
    - ì—­í•  íƒœê·¸ ì œê±° (ASSISTANT:, USER: ë“±)
    - í”„ë¡¬í”„íŠ¸ ëˆ„ìˆ˜ ì œê±°
    - ê³¼ë„í•œ ê³µë°± ì •ë¦¬

    ëŒ€ì†Œë¬¸ì, êµ¬ë‘ì ì€ ë³´ì¡´í•˜ì—¬ ì‹¤ì œ í’ˆì§ˆì„ ë°˜ì˜í•©ë‹ˆë‹¤.
    """
    if not text or pd.isna(text):
        return ""

    text = str(text)

    # 1. íŠ¹ìˆ˜ í† í° ì œê±°
    text = re.sub(r"<\|.*?\|>|<image>|</image>|<img>|</img>", " ", text, flags=re.I)
    text = re.sub(r"<vision_start>|<vision_end>|<image_pad>", " ", text, flags=re.I)

    # 2. ì—­í•  íƒœê·¸ ì œê±° (ë¬¸ì¥ ì‹œì‘ ë¶€ë¶„ì—ì„œ)
    text = re.sub(r"^(USER:|ASSISTANT:|Question:|Answer:)\s*", "", text, flags=re.I)

    # 3. ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+", " ", text).strip()

    return text


def calculate_evaluation_metrics(data_input, output_dir: Path, timestamp: str, prefix: str) -> Dict[str, float]:
    """
    5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (BLEU-4, METEOR, ROUGE-L, SPICE, CIDEr)

    Args:
        data_input: pandas DataFrame ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œ (str/Path)
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´
        prefix: ê²°ê³¼ íŒŒì¼ ì ‘ë‘ì–´

    Changes:
        - sacrebleu ì‚¬ìš© (í‘œì¤€ í† í°í™”, ì¬í˜„ ê°€ëŠ¥í•œ BLEU)
        - basic_cleanupìœ¼ë¡œ íŠ¹ìˆ˜ í† í°/ì—­í•  íƒœê·¸ ì œê±°
        - ëŒ€ì†Œë¬¸ì/êµ¬ë‘ì  ë³´ì¡´ (ì‹¤ì œ í’ˆì§ˆ ë°˜ì˜)
    """
    logger.info("=" * 60)
    logger.info("ğŸ“ˆ 5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°")
    logger.info("=" * 60)
    
    # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬: CSV íŒŒì¼ì´ë©´ DataFrameìœ¼ë¡œ ë³€í™˜
    if isinstance(data_input, (str, Path)):
        csv_path = Path(data_input)
        if not csv_path.exists():
            raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        
        logger.info(f"ğŸ“‚ CSV íŒŒì¼ ë¡œë“œ: {csv_path}")
        df = pd.read_csv(csv_path, encoding='utf-8')
        logger.info(f"âœ“ DataFrame ë³€í™˜ ì™„ë£Œ - ì´ {len(df)}ê°œ ìƒ˜í”Œ")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['prediction', 'reference']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"CSV íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}. í•„ìš”í•œ ì»¬ëŸ¼: {required_columns}")
        
        # ì˜µì…˜ ì»¬ëŸ¼ í™•ì¸ ë° ë¡œê·¸
        optional_columns = ['image_path']
        available_optional = [col for col in optional_columns if col in df.columns]
        logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: í•„ìˆ˜ {required_columns} + ì„ íƒ {available_optional}")
    
    elif isinstance(data_input, pd.DataFrame):
        df = data_input
        logger.info(f"âœ“ DataFrame ì…ë ¥ - ì´ {len(df)}ê°œ ìƒ˜í”Œ")
    else:
        raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {type(data_input)}. pandas DataFrame ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    # ìœ íš¨í•œ ìƒ˜í”Œë§Œ ì„ íƒ (ì˜ˆì¸¡ê³¼ ì •ë‹µê°€ ëª¨ë‘ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°)
    valid_df = df[(df['prediction'].str.strip() != '') & (df['reference'].str.strip() != '')]
    
    if len(valid_df) == 0:
        logger.error("âŒ ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    logger.info(f"ğŸ“Š í‰ê°€ ëŒ€ìƒ: {len(valid_df)}/{len(df)} ìƒ˜í”Œ")
    
    # ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (NaN ê°’ ì²˜ë¦¬)
    raw_predictions = [str(pred) if pred is not None and not pd.isna(pred) else "" for pred in valid_df['prediction'].tolist()]
    raw_references = [str(ref) if ref is not None and not pd.isna(ref) else "" for ref in valid_df['reference'].tolist()]

    # Level 1 ì •ë¦¬: íŠ¹ìˆ˜ í† í°, ì—­í•  íƒœê·¸ ì œê±°
    logger.info("ğŸ§¹ í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ (íŠ¹ìˆ˜ í† í°/ì—­í•  íƒœê·¸ ì œê±°)...")
    predictions = [basic_cleanup(pred) for pred in raw_predictions]
    references = [basic_cleanup(ref) for ref in raw_references]

    # "Assistant:" ë¶€ë¶„ ì²˜ë¦¬ (ì´ë¯¸ basic_cleanupì—ì„œ ì œê±°ë˜ì§€ë§Œ ì¶”ê°€ ì²´í¬)
    ref_texts_cleaned = []
    for ref in references:
        if "Assistant:" in ref:
            assistant_part = ref.split("Assistant:")[-1].strip()
            ref_texts_cleaned.append(assistant_part)
        else:
            ref_texts_cleaned.append(ref)
    references = ref_texts_cleaned

    # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
    valid_pairs = [(pred, ref) for pred, ref in zip(predictions, references) if pred.strip() and ref.strip()]

    if not valid_pairs:
        logger.error("âŒ ìœ íš¨í•œ ì˜ˆì¸¡-ì •ë‹µ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
        return {}

    predictions, references = zip(*valid_pairs)
    predictions = list(predictions)
    references = list(references)

    logger.info(f"ğŸ“Š ìµœì¢… í‰ê°€ ëŒ€ìƒ: {len(valid_pairs)} ìƒ˜í”Œ")
    logger.info(f"ğŸ“ ì˜ˆì‹œ - ì˜ˆì¸¡: '{predictions[0][:100]}...'")
    logger.info(f"ğŸ“ ì˜ˆì‹œ - ì •ë‹µ: '{references[0][:100]}...'")

    metrics = {}
    
    # 1. BLEU-4 ê³„ì‚° (sacrebleu ì‚¬ìš©)
    try:
        import sacrebleu

        # sacrebleuëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
        if len(predictions) == 0 or len(references) == 0:
            logger.warning("âš ï¸ BLEU-4: ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['bleu4'] = 0.0
        else:
            # sacrebleu ê³„ì‚° (í‘œì¤€ ì„¤ì •)
            bleu = sacrebleu.corpus_bleu(
                predictions,
                [references],           # ì°¸ì¡°ëŠ” ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸
                smooth_method="exp",    # í‘œì¤€ ìŠ¤ë¬´ë”©
                lowercase=False,        # ëŒ€ì†Œë¬¸ì ë³´ì¡´ (ì‹¤ì œ í’ˆì§ˆ ë°˜ì˜)
                tokenize="13a",         # Moses í† í¬ë‚˜ì´ì € (í•™ìˆ  í‘œì¤€)
                use_effective_order=True  # ì§§ì€ ë¬¸ì¥ ì•ˆì •í™”
            )
            metrics['bleu4'] = bleu.score / 100.0  # 0~1 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ (ê¸°ì¡´ í˜•ì‹ ìœ ì§€)
            logger.info(f"âœ“ BLEU-4 (sacrebleu): {metrics['bleu4']:.4f} (ì›ì ìˆ˜: {bleu.score:.2f}/100)")
            logger.info(f"  â†’ í† í°í™”: 13a (Moses), ìŠ¤ë¬´ë”©: exp, ëŒ€ì†Œë¬¸ì: ë³´ì¡´")
    except ImportError:
        logger.warning("âš ï¸ sacrebleuê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. NLTKë¡œ í´ë°±í•©ë‹ˆë‹¤.")
        logger.warning("   ê¶Œì¥: pip install sacrebleu")
        try:
            from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

            ref_tokens = [[ref.split()] for ref in references if ref.strip()]
            pred_tokens = [pred.split() for pred in predictions if pred.strip()]

            if len(ref_tokens) == 0 or len(pred_tokens) == 0:
                logger.warning("âš ï¸ BLEU-4: ìœ íš¨í•œ í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
                metrics['bleu4'] = 0.0
            else:
                smoothing = SmoothingFunction().method1
                metrics['bleu4'] = corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)
                logger.info(f"âœ“ BLEU-4 (NLTK í´ë°±): {metrics['bleu4']:.4f}")
        except Exception as e:
            logger.error(f"âŒ BLEU-4 ê³„ì‚° ì˜¤ë¥˜: {e}")
            metrics['bleu4'] = 0.0
    except Exception as e:
        logger.error(f"âŒ BLEU-4 ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['bleu4'] = 0.0
    
    # 2. METEOR ê³„ì‚°
    try:
        import nltk
        try:
            nltk.data.find('corpora/wordnet')
        except LookupError:
            logger.info("NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
            nltk.download('wordnet', quiet=True)
            nltk.download('punkt', quiet=True)

        from nltk.translate.meteor_score import meteor_score

        meteor_scores = []
        for ref, pred in zip(references, predictions):
            if ref.strip() and pred.strip():  # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                ref_tokens = ref.split()
                pred_tokens = pred.split()
                if len(ref_tokens) > 0 and len(pred_tokens) > 0:
                    score = meteor_score([ref_tokens], pred_tokens)
                    meteor_scores.append(score)

        if meteor_scores:
            metrics['meteor'] = float(np.mean(meteor_scores))
            logger.info(f"âœ“ METEOR: {metrics['meteor']:.4f}")
        else:
            logger.warning("âš ï¸ METEOR: ìœ íš¨í•œ ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['meteor'] = 0.0
    except Exception as e:
        logger.error(f"âŒ METEOR ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['meteor'] = 0.0

    # 3. ROUGE-L ê³„ì‚°
    try:
        from rouge_score import rouge_scorer
        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)

        rouge_scores = []
        for ref, pred in zip(references, predictions):
            if ref.strip() and pred.strip():  # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                scores = scorer.score(ref, pred)
                rouge_scores.append(scores['rougeL'].fmeasure)

        if rouge_scores:
            metrics['rougeL'] = float(np.mean(rouge_scores))
            logger.info(f"âœ“ ROUGE-L: {metrics['rougeL']:.4f}")
        else:
            logger.warning("âš ï¸ ROUGE-L: ìœ íš¨í•œ ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['rougeL'] = 0.0
    except Exception as e:
        logger.error(f"âŒ ROUGE-L ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['rougeL'] = 0.0
    
    # 4. SPICE ê³„ì‚° (ë” ì•ˆì „í•œ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬)
    try:
        from pycocoevalcap.spice.spice import Spice
        
        # SPICE ê³„ì‚°ì„ ë” ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        spice_scorer = Spice()

        # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
        valid_refs_for_spice = [ref for ref in references if ref.strip()]
        valid_preds_for_spice = [pred for pred in predictions if pred.strip()]
        
        if len(valid_refs_for_spice) == 0 or len(valid_preds_for_spice) == 0:
            logger.warning("âš ï¸ SPICE: ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['spice'] = 0.0
        else:
            gts = {str(i): [ref] for i, ref in enumerate(valid_refs_for_spice)}
            res = {str(i): [pred] for i, pred in enumerate(valid_preds_for_spice)}
            
            # ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ì´ìš©í•œ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ (ë” ì•ˆì „í•¨)
            import multiprocessing
            import queue
            
            def spice_calculate(gts, res, result_queue):
                try:
                    spice_score, _ = spice_scorer.compute_score(gts, res)
                    result_queue.put(('success', spice_score))
                except Exception as e:
                    result_queue.put(('error', str(e)))
            
            # í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•œ íƒ€ì„ì•„ì›ƒ
            result_queue = multiprocessing.Queue()
            process = multiprocessing.Process(target=spice_calculate, args=(gts, res, result_queue))
            process.start()
            process.join(timeout=60)  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
            
            if process.is_alive():
                process.terminate()
                process.join()
                raise TimeoutError("SPICE calculation timeout (60s)")
            
            # ê²°ê³¼ í™•ì¸
            try:
                result_type, result_value = result_queue.get_nowait()
                if result_type == 'success':
                    metrics['spice'] = float(result_value)
                    logger.info(f"âœ“ SPICE: {metrics['spice']:.4f}")
                else:
                    raise Exception(f"SPICE calculation failed: {result_value}")
            except queue.Empty:
                raise Exception("SPICE calculation returned no result")
            
    except (Exception, TimeoutError) as e:
        logger.warning(f"âš ï¸ SPICE ê³„ì‚° ì˜¤ë¥˜: {e}")
        # SPICE ëŒ€ì•ˆ: ê°„ë‹¨í•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
        try:
            logger.info("SPICE ëŒ€ì•ˆìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì‹œë„...")
            from sentence_transformers import SentenceTransformer
            model_st = SentenceTransformer('all-MiniLM-L6-v2')
            
            pred_embeddings = model_st.encode(predictions)
            ref_embeddings = model_st.encode(references)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = []
            for pred_emb, ref_emb in zip(pred_embeddings, ref_embeddings):
                sim = cosine_similarity([pred_emb], [ref_emb])[0][0]
                similarities.append(sim)
            
            metrics['spice'] = float(np.mean(similarities))
            logger.info(f"âœ“ SPICE (ëŒ€ì•ˆ-ì˜ë¯¸ìœ ì‚¬ë„): {metrics['spice']:.4f}")
        except Exception as fallback_e:
            logger.warning(f"âš ï¸ SPICE ëŒ€ì•ˆ ê³„ì‚°ë„ ì‹¤íŒ¨: {fallback_e}")
            metrics['spice'] = 0.0
    
    # 5. CIDEr ê³„ì‚°
    try:
        from pycocoevalcap.cider.cider import Cider
        cider_scorer = Cider()
        
        # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
        valid_refs_for_cider = [ref for ref in references if ref.strip()]
        valid_preds_for_cider = [pred for pred in predictions if pred.strip()]
        
        if len(valid_refs_for_cider) == 0 or len(valid_preds_for_cider) == 0:
            logger.warning("âš ï¸ CIDEr: ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            metrics['cider'] = 0.0
        else:
            gts = {str(i): [ref] for i, ref in enumerate(valid_refs_for_cider)}
            res = {str(i): [pred] for i, pred in enumerate(valid_preds_for_cider)}
            
            cider_score, _ = cider_scorer.compute_score(gts, res)
            metrics['cider'] = float(cider_score)
            logger.info(f"âœ“ CIDEr: {metrics['cider']:.4f}")
    except Exception as e:
        logger.warning(f"âš ï¸ CIDEr ê³„ì‚° ì˜¤ë¥˜: {e}")
        metrics['cider'] = 0.0
    
    # 6. CLIP Score ì¸¡ì • ì œê±°ë¨ (ì‚¬ìš©ì ìš”ì²­)
    logger.info("â„¹ï¸ CLIP Score ë° RefCLIP-S ì¸¡ì •ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ë©”íŠ¸ë¦­ ì €ì¥
    safe_prefix = prefix if prefix else "model"
    metrics_path = output_dir / f"{safe_prefix}_metrics_{timestamp}.json"
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ“ ë©”íŠ¸ë¦­ ì €ì¥: {metrics_path}")
    return metrics


def print_final_results(metrics: Dict[str, float]):
    """
    ìµœì¢… ê²°ê³¼ ì¶œë ¥
    """
    print("\n" + "=" * 80)
    print("ğŸ‰ PanoLLaVA ëª¨ë¸ í‰ê°€ ì™„ë£Œ")
    print("=" * 80)
    
    print("\nğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­ ê²°ê³¼:")
    print("-" * 40)
    
    if 'bleu4' in metrics:
        print(f"BLEU-4     (â†‘): {metrics['bleu4']:.4f}")
    if 'meteor' in metrics:
        print(f"METEOR     (â†‘): {metrics['meteor']:.4f}")
    if 'rougeL' in metrics:
        print(f"ROUGE-L    (â†‘): {metrics['rougeL']:.4f}")
    if 'spice' in metrics:
        print(f"SPICE      (â†‘): {metrics['spice']:.4f}")
    if 'cider' in metrics:
        print(f"CIDEr      (â†‘): {metrics['cider']:.4f}")
    # CLIP Score ì¶œë ¥ ì œê±°ë¨
    
    print("-" * 40)
    print("ğŸ’¡ (â†‘) í‘œì‹œëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤.")
    print("=" * 80)


def load_global_config(config_path: Optional[str] = None) -> Dict[str, Any]:
    """Load evaluation configuration using the shared training loader."""

    try:
        return _load_train_config_dict(config_path)
    except Exception as exc:
        logger.error(f"Failed to load configuration: {exc}")
        raise


def main():
    parser = argparse.ArgumentParser(description="PanoLLaVA ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ")
    # ì…ë ¥ ì¸ì: --config, --csv-input ë§Œ í—ˆìš©
    parser.add_argument('--config', help='Global config YAML ê²½ë¡œ (ë¯¸ì§€ì • ì‹œ PANOVLM_CONFIG or ./config.yaml ì‚¬ìš©)')
    parser.add_argument('--csv-input', dest='csv_input', default=None,
                        help='í‰ê°€ì— ì‚¬ìš©í•  CSV ê²½ë¡œ (ì˜ˆ: data/quic360/test.csv)')

    args = parser.parse_args()

    global_config = load_global_config(args.config)

    env_config = global_config.get("environment", {})
    model_config = global_config.get("models", {})
    data_config = global_config.get("data", {})
    training_config = global_config.get("training", {})
    image_cfg = global_config.get("image_processing", {})
    system_msgs = global_config.get("system_messages", {})

    # ë””ë°”ì´ìŠ¤ ì„¤ì •: í™˜ê²½ë³€ìˆ˜ ëŒ€ì‹  config ê¸°ë°˜ìœ¼ë¡œ GPU indexë¥¼ ì„ íƒ
    cuda_vis = env_config.get("cuda_visible_devices")
    if cuda_vis and torch.cuda.is_available():
        try:
            first_idx = int(str(cuda_vis).split(",")[0].strip())
            torch.cuda.set_device(first_idx)
            logger.info(f"Device: using GPU index {first_idx} (from config)")
        except Exception as e:
            logger.warning(f"Invalid cuda_visible_devices in config: {cuda_vis} ({e})")

    # CSV ì…ë ¥ ê²½ë¡œ: CLI ìš°ì„  -> config ìš°ì„ ìˆœìœ„ -> ê¸°ë³¸ê°’
    eff_csv_input = (
        args.csv_input
        or data_config.get("csv_test")
        or data_config.get("csv_val")
        or global_config.get("paths", {}).get("csv_val")
        or "data/quic360/test.csv"
    )

    # Model core
    eff_vision_name = model_config.get("vision_name")
    eff_lm_name = model_config.get("language_model_name") or model_config.get("lm_model")
    eff_resampler = model_config.get("resampler_type") or model_config.get("resampler")
    # Image processing
    eff_crop_strategy = image_cfg.get("crop_strategy", "e2p")
    eff_overlap_ratio = image_cfg.get("overlap_ratio", 0.5)
    eff_use_vp = image_cfg.get("use_vision_processor", True)
    eff_image_size = image_cfg.get("image_size", [224, 224])
    eff_fov_deg = image_cfg.get("fov_deg", 90.0)
    eff_image_mean = image_cfg.get("image_mean")
    eff_image_std = image_cfg.get("image_std")
    eff_anyres_patch_size = image_cfg.get("anyres_patch_size", 336)
    eff_anyres_max_patches = image_cfg.get("anyres_max_patches", 12)
    eff_normalize = image_cfg.get("normalize", True)
    # Tokenization
    eff_max_text_length = str(training_config.get("max_text_length", data_config.get("max_text_length", "auto")))
    eff_num_workers = training_config.get("num_workers", 16)
    eff_batch_size = (
        training_config.get("eval_batch_size")
        or training_config.get("batch_size")
        or training_config.get("finetune", {}).get("batch_size")
        or 16
    )
    eff_system_msg = training_config.get("system_msg", system_msgs.get("default", "You are a helpful assistant."))
    eff_output_dir = global_config.get("paths", {}).get("eval_dir", "results/eval_results")
    eff_prefix = training_config.get("prefix") or "model"
    safe_prefix = str(eff_prefix).strip() or "model"
    for ch in ["/", "\\", " "]:
        safe_prefix = safe_prefix.replace(ch, "_")
    # Generation
    gen_cfg = global_config.get("generation", {}) if isinstance(global_config, dict) else {}
    def _g(key, default):
        return gen_cfg.get(key, default) if isinstance(gen_cfg, dict) else default
    eff_gen_max_new_tokens = _g('max_new_tokens', 128)
    eff_gen_temperature = _g('temperature', 0.7)
    eff_gen_min_new_tokens = _g('min_new_tokens', 5)
    eff_gen_top_p = _g('top_p', 0.9)
    eff_gen_top_k = _g('top_k', 50)
    eff_gen_repetition_penalty = _g('repetition_penalty', 1.1)
    eff_gen_length_penalty = _g('length_penalty', 1.0)

    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìë™ í•´ê²° (args.configê°€ ì—†ìœ¼ë©´ ë¡œë“œëœ global_config ì‚¬ìš©)
    # stage strictly from config
    stage_from_cfg = training_config.get('default_stage', 'finetune')
    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìë™ í•´ê²°
    cfg_source = args.config if args.config else global_config
    model_dir = resolve_model_dir(cfg_source, stage_from_cfg, crop_strategy=eff_crop_strategy)

    # LoRA ê°€ì¤‘ì¹˜ ìë™ ì„¤ì • (config-only; no CLI override)
    lora_weights_path = None
    if model_dir:
        checkpoint_dir = Path(model_dir)
        checkpoint_dir = checkpoint_dir if checkpoint_dir.is_dir() else checkpoint_dir.parent
        potential_lora_path = checkpoint_dir / "lora_weights"
        if potential_lora_path.exists():
            lora_weights_path = str(potential_lora_path)
            logger.info(f"âœ… Auto-found LoRA weights: {lora_weights_path}")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path(eff_output_dir) / safe_prefix
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = time.strftime('%Y%m%d_%H%M%S')

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    try:
        # 1ë‹¨ê³„: ëª¨ë¸ ë° LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
        # Convert max_text_length for model only if numeric; otherwise omit (DataModule handles "auto")
        _mtl_val = None
        try:
            _mtl_val = int(eff_max_text_length)
        except Exception:
            _mtl_val = None
        # Use canonical config keys to avoid mismatches; include only when provided
        model_kwargs = {}
        if eff_vision_name:
            model_kwargs["vision_name"] = eff_vision_name
        if eff_lm_name:
            model_kwargs["language_model_name"] = eff_lm_name
        if eff_resampler:
            model_kwargs["resampler_type"] = eff_resampler
        if _mtl_val is not None:
            model_kwargs["max_text_length"] = _mtl_val
        model = load_model_and_lora(
            model_dir,
            lora_weights_path,
            device,
            config_path=args.config,  # ModelConfigë¥¼ ë³„ë„ë¡œ ì“°ëŠ” ê²½ìš°
            config_data=global_config if isinstance(global_config, dict) else None,
            **model_kwargs
        )

        # 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ (config ë°˜ì˜ ì¸ì ì¶”ê°€)
        datamodule, test_dataloader = prepare_test_dataset(
            csv_input=eff_csv_input,
            batch_size=eff_batch_size,
            max_text_length=eff_max_text_length,
            crop_strategy=(eff_crop_strategy or "e2p"),
            lm_name=(eff_lm_name or "Qwen/Qwen2.5-0.5B-Instruct"),
            num_workers=eff_num_workers,
            overlap_ratio=(eff_overlap_ratio if eff_overlap_ratio is not None else 0.5),
            image_size=eff_image_size,
            fov_deg=eff_fov_deg,
            image_mean=eff_image_mean,
            image_std=eff_image_std,
            anyres_patch_size=eff_anyres_patch_size,
            anyres_max_patches=eff_anyres_max_patches,
            normalize=eff_normalize,
            vision_name=eff_vision_name,
            system_msg=eff_system_msg,
            use_vision_processor=(bool(eff_use_vp) if eff_use_vp is not None else False),
            auto_max_text_length_cap=int(global_config.get("data", {}).get("auto_max_text_length_cap", 8192)) if isinstance(global_config, dict) else 8192,
            auto_max_text_length_floor=int(global_config.get("data", {}).get("auto_max_text_length_floor", 512)) if isinstance(global_config, dict) else None,
            auto_max_text_length_scan_limit=int(global_config.get("data", {}).get("auto_max_text_length_scan_limit", 1000)) if isinstance(global_config, dict) else None
        )

        # 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ìƒì„± (system_msg ì „ë‹¬)
        predictions, references, image_paths, input_texts = generate_predictions(
            model, test_dataloader, datamodule, device,
            max_new_tokens=int(eff_gen_max_new_tokens),
            temperature=float(eff_gen_temperature),
            top_p=float(eff_gen_top_p),
            top_k=int(eff_gen_top_k),
            repetition_penalty=float(eff_gen_repetition_penalty),
            length_penalty=float(eff_gen_length_penalty),
            min_new_tokens=int(eff_gen_min_new_tokens),
            system_msg=eff_system_msg
        )

        # 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ë¡œê¹…
        df = save_and_log_results(
            predictions,
            references,
            image_paths,
            input_texts,
            output_dir,
            timestamp,
            safe_prefix,
        )

        # 5ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = calculate_evaluation_metrics(df, output_dir, timestamp, safe_prefix)

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print_final_results(metrics)

    except Exception as e:
        logger.error(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise

if __name__ == '__main__':
    main()
