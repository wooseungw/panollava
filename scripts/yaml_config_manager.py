"""
YAML ê¸°ë°˜ ì„¤ì • ê´€ë¦¬ì
ê³µí†µ ì„¤ì •ê³¼ ìŠ¤í…Œì´ì§€ë³„ ì„¤ì •ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬
"""
import yaml
import json
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class StageConfig:
    """ë‹¨ì¼ ìŠ¤í…Œì´ì§€ ì„¤ì •"""
    name: str
    epochs: int
    learning_rate: float
    batch_size: int
    accumulate_grad_batches: int
    losses: List[Dict[str, Any]]
    data: Dict[str, Any]
    image_processing: Dict[str, Any]
    model_config: Dict[str, Any]

class ConfigManager:
    """YAML ì„¤ì • ê´€ë¦¬ì"""
    
    def __init__(self, config_path: Union[str, Path]):
        self.config_path = Path(config_path)
        self.config: Optional[Dict[str, Any]] = None
        
    def load_config(self) -> Dict[str, Any]:
        """YAML ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self._validate_config()
        self._merge_common_settings()
        return self.config
    
    def _validate_config(self):
        """ì„¤ì • ê²€ì¦"""
        required_sections = ['experiment', 'model', 'training']
        for section in required_sections:
            if section not in self.config:
                raise ValueError(f"Required section '{section}' not found in config")
        
        # ìŠ¤í…Œì´ì§€ ìˆœì„œì™€ ì‹¤ì œ ìŠ¤í…Œì´ì§€ ì •ì˜ í™•ì¸
        stage_order = self.config['training'].get('stage_order', [])
        stages = self.config['training'].get('stages', {})
        
        for stage_name in stage_order:
            if stage_name not in stages:
                raise ValueError(f"Stage '{stage_name}' defined in stage_order but not in stages")
    
    def _merge_common_settings(self):
        """ê³µí†µ ì„¤ì •ì„ ê° ìŠ¤í…Œì´ì§€ì— ë³‘í•©"""
        common_model = self.config.get('model', {})
        stages = self.config['training']['stages']
        
        for stage_name, stage_config in stages.items():
            # ëª¨ë¸ ì„¤ì • ë³‘í•©
            if 'model_config' in stage_config:
                # LoRA ì„¤ì • ë³‘í•©
                if 'lora' in stage_config['model_config']:
                    stage_lora = stage_config['model_config']['lora']
                    if isinstance(stage_lora, dict):
                        # ìŠ¤í…Œì´ì§€ë³„ LoRA ì„¤ì •ì´ dictì¸ ê²½ìš° ê³µí†µ ì„¤ì •ê³¼ ë³‘í•©
                        common_lora = common_model.get('lora', {})
                        merged_lora = {**common_lora, **stage_lora}
                        stage_config['model_config']['lora'] = merged_lora
                    elif stage_lora.get('enabled', False):
                        # enabled: trueë§Œ ìˆëŠ” ê²½ìš° ê³µí†µ ì„¤ì • ì‚¬ìš©
                        stage_config['model_config']['lora'] = common_model.get('lora', {})
                        stage_config['model_config']['lora']['enabled'] = True
    
    def get_stage_config(self, stage_name: str) -> StageConfig:
        """íŠ¹ì • ìŠ¤í…Œì´ì§€ ì„¤ì • ë°˜í™˜"""
        if not self.config:
            raise ValueError("Config must be loaded first")
        
        stages = self.config['training']['stages']
        if stage_name not in stages:
            raise ValueError(f"Stage '{stage_name}' not found")
        
        stage_data = stages[stage_name]
        
        return StageConfig(
            name=stage_name,
            epochs=stage_data['epochs'],
            learning_rate=stage_data['learning_rate'],
            batch_size=stage_data['batch_size'],
            accumulate_grad_batches=stage_data.get('accumulate_grad_batches', 1),
            losses=stage_data['losses'],
            data=stage_data['data'],
            image_processing=stage_data['image_processing'],
            model_config=stage_data['model_config']
        )
    
    def get_stage_order(self) -> List[str]:
        """ìŠ¤í…Œì´ì§€ ì‹¤í–‰ ìˆœì„œ ë°˜í™˜"""
        return self.config['training'].get('stage_order', [])
    
    def get_model_config(self) -> Dict[str, Any]:
        """ê³µí†µ ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        return self.config.get('model', {})
    
    def get_global_config(self) -> Dict[str, Any]:
        """ì „ì—­ ì„¤ì • ë°˜í™˜"""
        return self.config['training'].get('global', {})
    
    def get_experiment_info(self) -> Dict[str, Any]:
        """ì‹¤í—˜ ì •ë³´ ë°˜í™˜"""
        return self.config.get('experiment', {})
    
    @classmethod
    def convert_from_json(cls, json_path: Union[str, Path], output_path: Optional[Union[str, Path]] = None):
        """ê¸°ì¡´ JSON ì„¤ì •ì„ YAMLë¡œ ë³€í™˜"""
        json_path = Path(json_path)
        
        with open(json_path, 'r', encoding='utf-8') as f:
            json_config = json.load(f)
        
        # JSON -> YAML ë³€í™˜ ë¡œì§
        yaml_config = cls._convert_json_structure(json_config)
        
        if output_path is None:
            output_path = json_path.with_suffix('.yaml')
        
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(yaml_config, f, default_flow_style=False, indent=2, sort_keys=False)
        
        logger.info(f"Converted {json_path} -> {output_path}")
        return output_path
    
    @staticmethod
    def _convert_json_structure(json_config: Dict[str, Any]) -> Dict[str, Any]:
        """JSON êµ¬ì¡°ë¥¼ YAML êµ¬ì¡°ë¡œ ë³€í™˜"""
        models = json_config.get('models', {})
        training = json_config.get('training', {})
        
        # ê¸°ë³¸ YAML êµ¬ì¡° ìƒì„±
        yaml_config = {
            'experiment': {
                'name': training.get('prefix', 'converted_experiment'),
                'description': 'Converted from JSON config',
                'tags': ['converted'],
                'version': '1.0'
            },
            'model': {
                'vision': {
                    'name': models.get('vision_name', 'google/siglip-base-patch16-224')
                },
                'language': {
                    'name': models.get('language_model_name', 'meta-llama/Llama-3.2-1B')
                },
                'resampler': {
                    'type': models.get('resampler_type', 'mlp'),
                    'latent_dimension': models.get('latent_dimension', 768)
                },
                'lora': json_config.get('lora', {})
            },
            'data': {
                'default_dataset': {
                    'train_csv': json_config.get('paths', {}).get('csv_train', 'data/quic360/train.csv'),
                    'val_csv': json_config.get('paths', {}).get('csv_val', 'data/quic360/valid.csv'),
                    'max_text_length': training.get('max_text_length', 'auto'),
                    'auto_max_text_length_cap': json_config.get('data', {}).get('auto_max_text_length_cap', 512)
                }
            },
            'image_processing': json_config.get('image_processing', {}),
            'training': {
                'global': {
                    'num_workers': training.get('num_workers', 16),
                    'wandb_project': training.get('wandb_project', 'panollava-training'),
                    'system_msg': training.get('system_msg', ''),
                    'empty_cache_each_step': training.get('empty_cache_each_step', 1)
                },
                'stage_order': training.get('stages', ['vision', 'resampler', 'finetune']),
                'stages': {}
            },
            'evaluation': json_config.get('generation', {}),
            'environment': json_config.get('environment', {}),
            'hardware': {
                'precision': '16-mixed'
            },
            'deepspeed': training.get('deepspeed', {'enabled': False})
        }
        
        # ìŠ¤í…Œì´ì§€ ì„¤ì • ë³€í™˜
        stage_configs = training.get('stage_configs', {})
        for stage_name, stage_config in stage_configs.items():
            yaml_stage = {
                'epochs': stage_config.get('epochs', 1),
                'learning_rate': stage_config.get('lr', 1e-5),
                'batch_size': stage_config.get('batch_size', 1),
                'accumulate_grad_batches': stage_config.get('accumulate_grad_batches', 1),
                'losses': [],
                'data': stage_config.get('data', yaml_config['data']['default_dataset']),
                'image_processing': stage_config.get('image_processing', yaml_config['image_processing']),
                'model_config': {
                    'freeze_language_model': True,  # ê¸°ë³¸ê°’
                    'freeze_vision_encoder': False,
                    'use_vicreg_head': stage_config.get('vicreg_loss_weight', 0.0) > 0
                }
            }
            
            # Loss ì„¤ì • ë³€í™˜
            if stage_config.get('vicreg_loss_weight', 0.0) > 0:
                yaml_stage['losses'].append({
                    'type': 'vicreg',
                    'weight': stage_config['vicreg_loss_weight'],
                    'params': {
                        'similarity_weight': stage_config.get('vicreg_similarity_weight', 25.0),
                        'variance_weight': stage_config.get('vicreg_variance_weight', 25.0),
                        'covariance_weight': stage_config.get('vicreg_covariance_weight', 1.0)
                    }
                })
            else:
                yaml_stage['losses'].append({
                    'type': 'cross_entropy',
                    'weight': 1.0
                })
            
            yaml_config['training']['stages'][stage_name] = yaml_stage
        
        return yaml_config
    
    def preview_config(self):
        """ì„¤ì • ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥"""
        if not self.config:
            raise ValueError("Config must be loaded first")
        
        exp_info = self.get_experiment_info()
        print(f"\nğŸ§ª ì‹¤í—˜: {exp_info.get('name', 'Unknown')}")
        print(f"ğŸ“ ì„¤ëª…: {exp_info.get('description', 'No description')}")
        
        model_config = self.get_model_config()
        print(f"\nğŸ¤– ëª¨ë¸ ì„¤ì •:")
        print(f"   Vision: {model_config['vision']['name']}")
        print(f"   Language: {model_config['language']['name']}")
        print(f"   Resampler: {model_config['resampler']['type']}")
        
        print(f"\nğŸ¯ í›ˆë ¨ ìŠ¤í…Œì´ì§€:")
        for i, stage_name in enumerate(self.get_stage_order(), 1):
            stage = self.get_stage_config(stage_name)
            print(f"   {i}. {stage.name}")
            print(f"      - ì—í¬í¬: {stage.epochs}")
            print(f"      - í•™ìŠµë¥ : {stage.learning_rate}")
            print(f"      - ë°°ì¹˜ í¬ê¸°: {stage.batch_size}")
            print(f"      - ë¡œìŠ¤: {[loss['type'] for loss in stage.losses]}")
            
            # VICReg ì‚¬ìš© ì—¬ë¶€
            use_vicreg = stage.model_config.get('use_vicreg_head', False)
            print(f"      - VICReg Head: {'âœ…' if use_vicreg else 'âŒ'}")
            
            # LoRA ì‚¬ìš© ì—¬ë¶€
            lora_config = stage.model_config.get('lora', {})
            use_lora = lora_config.get('enabled', False) if isinstance(lora_config, dict) else False
            print(f"      - LoRA: {'âœ…' if use_lora else 'âŒ'}")
